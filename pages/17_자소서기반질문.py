# pages/17_ìì†Œì„œê¸°ë°˜ì§ˆë¬¸.py
# ìì†Œì„œ ê¸°ë°˜ ë©´ì ‘ ì§ˆë¬¸ ìƒì„± í˜ì´ì§€

import re
import time
import hashlib
import os
import sys
from typing import List, Dict, Any, Optional, Tuple

from logging_config import get_logger
logger = get_logger(__name__)

import streamlit as st

# í˜ì´ì§€ ì„¤ì • (ë°˜ë“œì‹œ ì²« ë²ˆì§¸ Streamlit ëª…ë ¹)
st.set_page_config(page_title="ìì†Œì„œ ê¸°ë°˜ ì§ˆë¬¸", page_icon="ğŸ“‹", layout="wide")




# êµ¬ê¸€ ë²ˆì—­ ë°©ì§€
st.markdown('<meta name="google" content="notranslate"><style>html{translate:no;}</style>', unsafe_allow_html=True)
st.markdown('<div translate="no" class="notranslate">', unsafe_allow_html=True)

# ìƒìœ„ ë””ë ‰í† ë¦¬ import ê²½ë¡œ ì¶”ê°€
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# ë‚´ë¶€ ëª¨ë“ˆ import
from config import (
    AIRLINES, AIRLINE_VALUES, VALUES_DEFAULT,
    _canonical_airline_name, _raw_airline_key, airline_profile,
    SOFT_TIMEOUT_SEC,
    FSC_VALUE_DATA, LCC_VALUE_DATA, HSC_VALUE_DATA,
    VALUE_Q_TEMPLATES_FSC, VALUE_Q_TEMPLATES_LCC,
    # ë²„ì „ë³„ VALUE_Q í…œí”Œë¦¿
    VALUE_Q_TEMPLATES_FSC_SOFT, VALUE_Q_TEMPLATES_FSC_SHARP,
    VALUE_Q_TEMPLATES_LCC_SOFT, VALUE_Q_TEMPLATES_LCC_SHARP,
    Q1_FIXED, Q1_POOL, Q5_POOL, Q5_FORBIDDEN_WORDS,
    # í•­ê³µì‚¬ ìœ í˜•ë³„ ì§ˆë¬¸ í’€
    Q1_POOL_FSC, Q1_POOL_LCC, Q1_POOL_HSC, Q1_POOL_COMMON,
    Q5_POOL_FSC, Q5_POOL_LCC, Q5_POOL_HSC, Q5_POOL_COMMON,
    # ë²„ì „ë³„ ë§íˆ¬ ì°¨ì´ (SHARP=ë‚ ì¹´ë¡œì›€, SOFT=ë¶€ë“œëŸ¬ì›€)
    Q5_POOL_COMMON_SHARP, Q5_POOL_COMMON_SOFT,
    Q5_POOL_FSC_SHARP, Q5_POOL_FSC_SOFT,
    Q5_POOL_LCC_SHARP, Q5_POOL_LCC_SOFT,
    # Q1 ë²„ì „ë³„ ë§íˆ¬
    Q1_POOL_COMMON_SOFT, Q1_POOL_COMMON_SHARP,
    Q1_POOL_FSC_SOFT, Q1_POOL_FSC_SHARP,
    Q1_POOL_LCC_SOFT, Q1_POOL_LCC_SHARP,
    Q1_POOL_HSC_SOFT, Q1_POOL_HSC_SHARP,
    # ë²„ì „ë³„ í’€ ì„ íƒ í•¨ìˆ˜
    get_q1_pool_by_airline, get_q5_pool_by_airline, get_value_q_templates,
    # í†µí•©/ì¸ìˆ˜í•©ë³‘ ê´€ë ¨ ì§ˆë¬¸
    INTEGRATED_LCC_Q_TEMPLATES, INTEGRATED_LCC_AIRLINES,
    # ìƒˆ ê³µê²© í¬ì¸íŠ¸ â†’ ì§ˆë¬¸ ë²ˆì—­ í…œí”Œë¦¿
    Q2_ATTACK_TEMPLATES, Q2_ATTACK_TEMPLATES_SOFT,
    Q2_RISK_TEMPLATES, Q2_RISK_TEMPLATES_SOFT,
    Q2_ALTERNATIVE_TEMPLATES, Q2_ALTERNATIVE_TEMPLATES_SOFT,
    # ì¶”ìƒì  ë¬¸ì¥ìš© í…œí”Œë¦¿
    ABSTRACT_SENTENCE_PATTERNS,
    Q2_ABSTRACT_TEMPLATES, Q2_ABSTRACT_TEMPLATES_SOFT,
    # ê¿ˆ/ì†Œë§ ë¬¸ì¥ìš© í…œí”Œë¦¿
    Q2_DREAM_TEMPLATES, Q2_DREAM_TEMPLATES_SOFT,
    # Q3 ê¼¬ë¦¬ì§ˆë¬¸ 4ê°€ì§€ ìœ í˜•
    Q3_CONDITION_CHANGE, Q3_PRIORITY_CONFLICT,
    Q3_LIMIT_RECOGNITION, Q3_REPEATABILITY,
    # Q3 ê¿ˆ/ëª©í‘œ ì „ìš© í…œí”Œë¦¿
    Q3_DREAM_CONDITION, Q3_DREAM_PRIORITY,
    Q3_DREAM_LIMIT, Q3_DREAM_REPEAT,
    # ìš”ê¸ˆì œ ë° ì´ë ¥ì„œ ì„¤ì •
    PLAN_CONFIG, DEFAULT_PLAN,
    RESUME_MAJOR_OPTIONS, RESUME_EXPERIENCE_OPTIONS, RESUME_GAP_OPTIONS,
    # ë©´ì ‘ íŒ ë° 2026 ì±„ìš© íŠ¸ë Œë“œ
    AIRLINE_PREFERRED_TYPE, ENGLISH_INTERVIEW_AIRLINES,
    INTERVIEW_TIPS, FSC_VS_LCC_INTERVIEW,
    COMMON_INTERVIEW_MISTAKES, CREW_ESSENTIAL_QUALITIES,
    KOREAN_AIR_INTERVIEW_INFO, HIRING_TRENDS_2026,
    # ë©´ì ‘ê´€ í†¤ ê·œì¹™ ë° í•œêµ­ì–´ ì§ˆë¬¸ ìƒì„± ê·œì¹™
    INTERVIEWER_TONE_RULES, KOREAN_QUESTION_RULES,
    TONE_CONVERSION_RULES, ABSOLUTE_PROHIBITIONS,
    # Q2 ê³µê²© êµ¬ì¡° ë° Few-shot ì˜ˆì‹œ
    Q2_ATTACK_STRUCTURES, Q2_FORBIDDEN_PATTERNS, Q2_FEWSHOT_EXAMPLES,
)
from text_utils import (
    normalize_ws, stable_int_hash, split_sentences, split_essay_items,
    extract_evidence_sentences, extract_risk_keywords_kor,
    fmt_anchor_text, _strip_ellipsis_tokens, _trim_no_ellipsis,
    _sanity_kor_endings, _auto_fix_particles_kor, _fix_particles_after_format,
    _sanitize_question_strict, _dedup_keep_order, build_basis_text,
)
from analysis import (
    _pick_anchor_by_rule, _srcai_analyze_text,
    _classify_prompt_type_kor, _extract_topic_keywords_short,
    _pick_situation_snippet, _pick_two_basis_by_type,
)
from llm_utils import (
    # CLOVA ë‹¨ë… ëª¨ë“œ: OpenAI ê´€ë ¨ í•¨ìˆ˜ ì œê±°
    _llm_type_to_internal, _llm_extract_for_slot, generate_q3_from_answer,
    generate_resume_questions,
    # í”„ë¦¬ë¯¸ì—„ ë¶„ì„ í•¨ìˆ˜
    premium_analyze_resume, get_premium_q2_question, get_premium_q3_question,
    # ê°„ë‹¨í•œ Q2/Q3 ì§ì ‘ ìƒì„± í•¨ìˆ˜
    generate_simple_q2, generate_simple_q3,
)
# CLOVA ë‹¨ë…: ë‚ ì¹´ë¡œìš´ ì§ˆë¬¸ íŒŒì´í”„ë¼ì¸ (ë¹ ë¥¸ ë²„ì „ë§Œ ì‚¬ìš©)
from sharp_question_pipeline import analyze_resume_fast, _validate_and_fix_question_format
# FLYREADY 2íšŒ í˜¸ì¶œ ì—”ì§„ v2.0 (ì‹ ê·œ)
from flyready_clova_engine import FlyreadyClovaEngine, analyze_resume_with_flyready
from extraction_verifier import is_complete_sentence
from feedback_analyzer import analyze_answer
# FLYREADY ì§ˆë¬¸ ë°ì´í„° ëª¨ë“ˆ (ê³µí†µì§ˆë¬¸ 100ê°œ, í•­ê³µì‚¬ë³„ 660ê°œ, ëŒë°œì§ˆë¬¸ 100ê°œ)
from flyready_question_data import (
    get_common_questions, get_random_common_questions,
    get_airline_question_list, get_random_airline_questions,
    get_all_surprise_questions, get_random_surprise_questions,
    get_followup_patterns, get_random_followup,
    get_airline_type, get_mixed_questions,
)


# ----------------------------
# ë¹„ë°€ë²ˆí˜¸ ë³´í˜¸ (í…ŒìŠ¤í„° 5ëª…ë§Œ ì ‘ê·¼ ê°€ëŠ¥)
# ----------------------------


# ----------------------------
# ìºì‹œ ê°•ì œ í´ë¦¬ì–´ (ì½”ë“œ ìˆ˜ì • ì‹œ ì´ì „ ìºì‹œ ë¬´íš¨í™”)
# ----------------------------
_CODE_VERSION = "v26_pipeline_v2_gate_validation_20260120"
if st.session_state.get("_code_version") != _CODE_VERSION:
    # ëª¨ë“  LLM ê´€ë ¨ ìºì‹œ í´ë¦¬ì–´
    if "_llm_extract_box" in st.session_state:
        del st.session_state["_llm_extract_box"]
    if "_item_analysis_box" in st.session_state:
        del st.session_state["_item_analysis_box"]
    st.session_state["_code_version"] = _CODE_VERSION


# ----------------------------
# SRCAI ì ìš© í•¨ìˆ˜ (session_state ì ‘ê·¼ í•„ìš”)
# ----------------------------

def _srcai_apply_to_qa_sets(qa_sets: List[Dict[str, str]]) -> None:
    if "ps_srcai_sentences" not in st.session_state:
        st.session_state.ps_srcai_sentences = []
    if "ps_srcai_roles" not in st.session_state:
        st.session_state.ps_srcai_roles = []
    if "ps_srcai_selected_actions" not in st.session_state:
        st.session_state.ps_srcai_selected_actions = []
    if "ps_srcai_selected_results" not in st.session_state:
        st.session_state.ps_srcai_selected_results = []
    if "ps_srcai_selected_questionables" not in st.session_state:
        st.session_state.ps_srcai_selected_questionables = []

    per_sentences: List[List[str]] = []
    per_roles: List[List[str]] = []
    per_actions: List[List[str]] = []
    per_results: List[List[str]] = []
    per_questionables: List[List[str]] = []

    if not qa_sets:
        st.session_state.ps_srcai_sentences = per_sentences
        st.session_state.ps_srcai_roles = per_roles
        st.session_state.ps_srcai_selected_actions = per_actions
        st.session_state.ps_srcai_selected_results = per_results
        st.session_state.ps_srcai_selected_questionables = per_questionables
        return

    for qa in qa_sets:
        ans_raw = qa.get("answer", "") or ""
        analyzed = _srcai_analyze_text(ans_raw)
        per_sentences.append(analyzed["sentences"])
        per_roles.append(analyzed["roles"])
        per_actions.append(analyzed["selected_action_sentences"])
        per_results.append(analyzed["selected_result_sentences"])
        per_questionables.append(analyzed["selected_questionable_sentences"])

    st.session_state.ps_srcai_sentences = per_sentences
    st.session_state.ps_srcai_roles = per_roles
    st.session_state.ps_srcai_selected_actions = per_actions
    st.session_state.ps_srcai_selected_results = per_results
    st.session_state.ps_srcai_selected_questionables = per_questionables


# ----------------------------
# Basis ì„ íƒ ë¡œì§ (session_state ì ‘ê·¼ í•„ìš”)
# ----------------------------

def _is_valid_basis_text(text: str) -> bool:
    """basis í…ìŠ¤íŠ¸ê°€ ìœ íš¨í•œì§€ ê²€ì¦ (ë¶ˆì™„ì „í•œ ë¬¸ì¥ í•„í„°ë§)"""
    if not text or len(text.strip()) < 10:
        return False
    text = text.strip()

    # ë¶ˆì™„ì „í•œ ë¬¸ì¥ íŒ¨í„´ (ê´€í˜•í˜• ì–´ë¯¸ë¡œ ëë‚¨ - ëª…ì‚¬ê°€ ì™€ì•¼ í•¨)
    incomplete_endings = [
        "í–¥í•œ", "ìœ„í•œ", "í†µí•œ", "ëŒ€í•œ", "ê´€í•œ",
        "í•˜ëŠ”", "ë˜ëŠ”", "ê°™ì€", "ë‹¤ë¥¸", "ìˆëŠ”", "ì—†ëŠ”",
        "ë¼ëŠ”", "ì´ë¼ëŠ”", "ë¼ê³  í•˜ëŠ”",
        "ê³¼ì˜", "ì™€ì˜", "ì—ì„œì˜", "ìœ¼ë¡œì˜", "ë¡œì˜",
        "ì²˜ëŸ¼", "ê°™ì´", "ëŒ€ë¡œ",
    ]
    for ending in incomplete_endings:
        if text.endswith(ending):
            return False

    # ì—°ê²°ì–´ë¯¸ë¡œ ëë‚˜ëŠ” ë¬¸ì¥
    connecting_endings = [
        "í•˜ê³ ", "í•˜ë©°", "í•˜ë©´ì„œ", "í•˜ì—¬", "í•´ì„œ",
        "ë˜ê³ ", "ë˜ë©°", "ìœ¼ë©°", "ë©°", "ë©´ì„œ",
        "ì§€ë§Œ", "ëŠ”ë°", "ë‹ˆê¹Œ", "ë¯€ë¡œ", "ì–´ì„œ", "ì•„ì„œ",
        "ë ¤ê³ ", "ìœ¼ë ¤ê³ ", "ê³ ì", "ë„ë¡",
    ]
    for ending in connecting_endings:
        if text.endswith(ending):
            return False

    # ì¡°ì‚¬ë¡œë§Œ ëë‚˜ëŠ” ì§§ì€ ë¬¸ì¥
    if re.search(r"[ì„ë¥¼ì´ê°€ì€ëŠ”ì™€ê³¼]$", text) and len(text) < 25:
        return False

    return True


def _pick_basis_from_llm_or_fallback(
    idx0: int,
    qtype_internal: str,
    answer_raw: str,
    llm_item: Optional[Dict[str, Any]],
) -> tuple:
    if isinstance(llm_item, dict):
        acts = llm_item.get("action_sentences", [])
        ress = llm_item.get("result_sentences", [])
        if isinstance(acts, list) and isinstance(ress, list):
            # ìœ íš¨í•œ ë¬¸ì¥ë§Œ í•„í„°ë§ (ë¶ˆì™„ì „í•œ ë¬¸ì¥ ì œì™¸)
            valid_acts = [a for a in acts if isinstance(a, str) and _is_valid_basis_text(a)]
            valid_ress = [r for r in ress if isinstance(r, str) and _is_valid_basis_text(r)]

            A_raw = valid_acts[0] if valid_acts else ""
            B_raw = valid_ress[0] if valid_ress else (valid_acts[1] if len(valid_acts) > 1 else "")
            if A_raw or B_raw:
                A = {"text": _trim_no_ellipsis(_strip_ellipsis_tokens(A_raw), 120) if A_raw else "", "kind": "action"}
                B = {"text": _trim_no_ellipsis(_strip_ellipsis_tokens(B_raw), 120) if B_raw else "", "kind": "result"}
                if A["text"] and B["text"]:
                    return A, B
                fbA, fbB = _pick_two_basis_by_type(qtype=qtype_internal, answer=answer_raw, version_seed=idx0 + 1)
                if not A["text"]:
                    A["text"] = fbA.get("text", "")
                    A["kind"] = fbA.get("kind", "action")
                if not B["text"]:
                    B["text"] = fbB.get("text", "")
                    B["kind"] = fbB.get("kind", "role")
                return A, B

    try:
        per_actions = st.session_state.get("ps_srcai_selected_actions", [])
        per_results = st.session_state.get("ps_srcai_selected_results", [])
        act_cands = per_actions[idx0] if isinstance(per_actions, list) and idx0 < len(per_actions) else []
        res_cands = per_results[idx0] if isinstance(per_results, list) and idx0 < len(per_results) else []
        if isinstance(act_cands, list) or isinstance(res_cands, list):
            # ìœ íš¨í•œ ë¬¸ì¥ë§Œ í•„í„°ë§ (ë¶ˆì™„ì „í•œ ë¬¸ì¥ ì œì™¸)
            valid_act_cands = [a for a in act_cands if isinstance(a, str) and _is_valid_basis_text(a)] if isinstance(act_cands, list) else []
            valid_res_cands = [r for r in res_cands if isinstance(r, str) and _is_valid_basis_text(r)] if isinstance(res_cands, list) else []

            A_raw = valid_act_cands[0] if valid_act_cands else ""
            B_raw = (valid_res_cands[0] if valid_res_cands else "") or (
                valid_act_cands[1] if len(valid_act_cands) > 1 else ""
            )
            if A_raw or B_raw:
                A = {"text": _trim_no_ellipsis(_strip_ellipsis_tokens(A_raw), 120) if A_raw else "", "kind": "action"}
                B = {"text": _trim_no_ellipsis(_strip_ellipsis_tokens(B_raw), 120) if B_raw else "", "kind": "result"}
                if A["text"] or B["text"]:
                    if not A["text"] or not B["text"]:
                        fbA, fbB = _pick_two_basis_by_type(qtype=qtype_internal, answer=answer_raw, version_seed=idx0 + 1)
                        if not A["text"]:
                            A["text"] = fbA.get("text", "")
                            A["kind"] = fbA.get("kind", "action")
                        if not B["text"]:
                            B["text"] = fbB.get("text", "")
                            B["kind"] = fbB.get("kind", "role")
                    return A, B
    except Exception:
        pass

    return _pick_two_basis_by_type(qtype=qtype_internal, answer=answer_raw, version_seed=idx0 + 1)


# ----------------------------
# ë¬¸í•­ ë¶„ì„ ë¡œì§
# ----------------------------

def _analyze_qa_sets(qa_sets: List[Dict[str, str]], llm_data: Optional[Dict[str, Any]] = None) -> List[Dict[str, Any]]:
    out: List[Dict[str, Any]] = []
    if not qa_sets:
        return out

    llm_items = None
    if isinstance(llm_data, dict):
        li = llm_data.get("items", None)
        if isinstance(li, list):
            llm_items = li

    per_actions = st.session_state.get("ps_srcai_selected_actions", [])
    per_results = st.session_state.get("ps_srcai_selected_results", [])

    for i, qa in enumerate(qa_sets, start=1):
        p_raw = qa.get("prompt", "") or ""
        a_raw = qa.get("answer", "") or ""

        p = normalize_ws(p_raw)
        a = normalize_ws(a_raw)

        qtype_internal = _classify_prompt_type_kor(p)
        llm_item = None
        if isinstance(llm_items, list) and (i - 1) < len(llm_items):
            cand = llm_items[i - 1]
            if isinstance(cand, dict):
                llm_item = cand
                # question_type í•„ë“œ ì‚¬ìš© (ê¸°ì¡´ type í•„ë“œë„ í´ë°±ìœ¼ë¡œ ì§€ì›)
                llm_qtype = cand.get("question_type", "") or cand.get("type", "")
                qtype_internal = _llm_type_to_internal(llm_qtype, qtype_internal)

        keywords = _extract_topic_keywords_short(a, top_k=10)
        situation = _pick_situation_snippet(a, qtype=qtype_internal)

        basisA, basisB = _pick_basis_from_llm_or_fallback(
            idx0=i - 1,
            qtype_internal=qtype_internal,
            answer_raw=a_raw,
            llm_item=llm_item,
        )

        action_sents: List[str] = []
        result_sents: List[str] = []

        if isinstance(llm_item, dict):
            acts = llm_item.get("action_sentences", [])
            ress = llm_item.get("result_sentences", [])
            if isinstance(acts, list):
                action_sents.extend([s for s in acts if isinstance(s, str) and s])
            if isinstance(ress, list):
                result_sents.extend([s for s in ress if isinstance(s, str) and s])

        try:
            if isinstance(per_actions, list) and (i - 1) < len(per_actions):
                aa = per_actions[i - 1]
                if isinstance(aa, list):
                    action_sents.extend([s for s in aa if isinstance(s, str) and s])
            if isinstance(per_results, list) and (i - 1) < len(per_results):
                rr = per_results[i - 1]
                if isinstance(rr, list):
                    result_sents.extend([s for s in rr if isinstance(s, str) and s])
        except Exception:
            pass

        out.append({
            "index": i,
            "prompt": p,
            "answer": a,
            "qtype": qtype_internal,
            "keywords": keywords,
            "situation": situation,
            "basisA": basisA,
            "basisB": basisB,
            "action_sents": _dedup_keep_order(action_sents),
            "result_sents": _dedup_keep_order(result_sents),
        })
    return out


# ìºì‹œ ë¬´íš¨í™” ë²„ì „ - ì´ ê°’ì„ ë°”ê¾¸ë©´ ëª¨ë“  ì„¸ì…˜ ìºì‹œê°€ ë¬´íš¨í™”ë¨
_CACHE_VERSION = "v3_title_filter_fix"

def _get_or_build_item_analysis_cache(qa_sets: List[Dict[str, str]], essay_hash: str, force_rebuild: bool = False) -> Dict[str, Any]:
    """CLOVA ë‹¨ë… ëª¨ë“œ: ë¡œì»¬ ë¶„ì„ë§Œ ìˆ˜í–‰ (OpenAI ì˜ì¡´ì„± ì œê±°)"""
    box = st.session_state.get("_item_analysis_box", {})
    key = f"{_CACHE_VERSION}|{essay_hash}"

    # ìºì‹œëœ ê²°ê³¼ í™•ì¸ (force_rebuildê°€ ì•„ë‹Œ ê²½ìš°ì—ë§Œ)
    if not force_rebuild and isinstance(box, dict) and key in box:
        return box[key]

    # CLOVA ë‹¨ë… ëª¨ë“œ: ë¡œì»¬ ë¶„ì„ë§Œ ìˆ˜í–‰
    analyzed = _analyze_qa_sets(qa_sets, llm_data=None)

    agg_text = "\n".join([x.get("answer", "") for x in analyzed if x.get("answer", "")]).strip()
    agg_evidence = extract_evidence_sentences(agg_text, max_sents=12) if agg_text else []
    agg_risk = extract_risk_keywords_kor(agg_text, top_k=14) if agg_text else []
    agg_keywords = []
    seen_kw = set()
    for it in analyzed:
        for kw in it.get("keywords", [])[:10]:
            if kw and kw not in seen_kw:
                seen_kw.add(kw)
                agg_keywords.append(kw)
            if len(agg_keywords) >= 14:
                break
        if len(agg_keywords) >= 14:
            break

    built = {
        "items": analyzed,
        "agg_text": agg_text,
        "agg_evidence": agg_evidence,
        "agg_risk": agg_risk,
        "agg_keywords": agg_keywords,
    }
    if not isinstance(box, dict):
        box = {}
    box[key] = built
    st.session_state["_item_analysis_box"] = box
    return built


# ----------------------------
# ì§ˆë¬¸ ìƒì„± í—¬í¼ í•¨ìˆ˜
# ----------------------------

def _slot_q1_common(base: int, version: int, atype: str = "LCC") -> str:
    """Q1: FLYREADY ê³µí†µì§ˆë¬¸ 100ê°œ + ê¸°ì¡´ í’€ì—ì„œ ë²„ì „ë³„ ì„ íƒ

    ë²„ì „ì— ë”°ë¼ í†¤ ì„ íƒ: í™€ìˆ˜(1,3,5)=ë‚ ì¹´ë¡œìš´, ì§ìˆ˜(2,4,6)=ë¶€ë“œëŸ¬ìš´
    FLYREADY ë°ì´í„°: 01_ê³µí†µì§ˆë¬¸_100.json (10ê°œ ì¹´í…Œê³ ë¦¬)
    """
    # ë²„ì „ì— ë”°ë¼ í†¤ ì„ íƒ
    is_soft_version = (version % 2 == 0)

    # FLYREADY ê³µí†µì§ˆë¬¸ 100ê°œ ë¡œë“œ
    flyready_common = get_common_questions()
    flyready_questions = [q.get("question", "") for q in flyready_common if q.get("question")]

    # í•­ê³µì‚¬ ìœ í˜•ì— ë§ëŠ” ê¸°ì¡´ ì§ˆë¬¸ í’€ ì„ íƒ (ë²„ì „ë³„ í†¤ ì ìš©)
    if atype == "FSC":
        if is_soft_version:
            legacy_pool = Q1_POOL_COMMON_SOFT + Q1_POOL_FSC_SOFT
        else:
            legacy_pool = Q1_POOL_COMMON_SHARP + Q1_POOL_FSC_SHARP
    elif atype == "HSC":
        if is_soft_version:
            legacy_pool = Q1_POOL_COMMON_SOFT + Q1_POOL_HSC_SOFT
        else:
            legacy_pool = Q1_POOL_COMMON_SHARP + Q1_POOL_HSC_SHARP
    else:  # LCC
        if is_soft_version:
            legacy_pool = Q1_POOL_COMMON_SOFT + Q1_POOL_LCC_SOFT
        else:
            legacy_pool = Q1_POOL_COMMON_SHARP + Q1_POOL_LCC_SHARP

    # FLYREADY ì§ˆë¬¸ + ê¸°ì¡´ í’€ í•©ì¹˜ê¸° (FLYREADY ìš°ì„ )
    pool = flyready_questions + legacy_pool

    if not pool:
        return "ìŠ¹ë¬´ì›ìœ¼ë¡œì„œ ì•ˆì „ê³¼ ì„œë¹„ìŠ¤ê°€ ë™ì‹œì— ìš”êµ¬ë˜ëŠ” ìƒí™©ì—ì„œ ë¬´ì—‡ì„ ë¨¼ì € ì„ íƒí•˜ê³  ì–´ë–¤ í–‰ë™ìœ¼ë¡œ ë§ˆë¬´ë¦¬í•˜ê² ìŠµë‹ˆê¹Œ?"

    # ë²„ì „ì— ë”°ë¼ ë‹¤ë¥¸ ì§ˆë¬¸ ì„ íƒ (ë§¤ë²ˆ ë‹¤ë¥¸ ì§ˆë¬¸ ë‚˜ì˜¤ë„ë¡)
    idx = (base + version * 7 + 1) % len(pool)
    # Q1ë„ í’€ì—ì„œ ê°€ì ¸ì˜¨ ì§ˆë¬¸ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš© (ë³€í˜• ê¸ˆì§€)
    return normalize_ws(pool[idx])


def _extract_topic_from_claim(claim: str, max_len: int = 25) -> str:
    """claimì—ì„œ í•µì‹¬ ì£¼ì œ ë¶€ë¶„ë§Œ ì¶”ì¶œ (ë™ì‚¬/ì–´ë¯¸ ì œê±°)

    ì˜ˆ: "ìŠ¹ë¬´ì›ì˜ ë”°ëœ»í•œ ë¯¸ì†Œì™€ ë°°ë ¤ì— ê°ëª…ë°›ì•˜ìŠµë‹ˆë‹¤" â†’ "ìŠ¹ë¬´ì›ì˜ ë”°ëœ»í•œ ë¯¸ì†Œì™€ ë°°ë ¤"
    """
    if not claim or len(claim.strip()) < 5:
        return ""

    text = claim.strip()

    # ë™ì‚¬ ì–´ë¯¸ íŒ¨í„´ ì œê±° (ë’¤ì—ì„œë¶€í„°)
    verb_endings = [
        "ì…ë‹ˆë‹¤", "ìŠµë‹ˆë‹¤", "ì˜€ìŠµë‹ˆë‹¤", "ì—ˆìŠµë‹ˆë‹¤", "í–ˆìŠµë‹ˆë‹¤",
        "í•©ë‹ˆë‹¤", "ë©ë‹ˆë‹¤", "ìˆìŠµë‹ˆë‹¤", "ì—†ìŠµë‹ˆë‹¤", "ê² ìŠµë‹ˆë‹¤",
        "ã…‚ë‹ˆë‹¤", "ìŠµë‹ˆë‹¤", "ë‹ˆë‹¤",
        "í–ˆë‹¤", "ì˜€ë‹¤", "ì—ˆë‹¤", "ì´ë‹¤", "í–ˆì–´ìš”", "í–ˆì£ ",
        "ì— ê°ëª…ë°›ì•˜", "ì„ ëŠê¼ˆ", "ë¥¼ ëŠê¼ˆ", "ì— ê°ë™ë°›ì•˜",
        "ì„ ê¹¨ë‹¬ì•˜", "ë¥¼ ê¹¨ë‹¬ì•˜", "ì„ ë°°ì› ", "ë¥¼ ë°°ì› ",
        "ê³  ìƒê°í–ˆ", "ê³  ëŠê¼ˆ", "ê²Œ ë˜ì—ˆ", "ê²Œ ë",
    ]

    for ending in verb_endings:
        if text.endswith(ending):
            text = text[:-len(ending)].strip()
            break

    # ì¡°ì‚¬ë¡œ ëë‚˜ë©´ ìœ ì§€, ì•„ë‹ˆë©´ ì •ë¦¬
    # "~ì—", "~ë¥¼", "~ì„", "~ì™€", "~ê³¼" ë“±ìœ¼ë¡œ ëë‚˜ë©´ ì œê±°
    trailing_particles = ["ì—", "ë¥¼", "ì„", "ì™€", "ê³¼", "ì´", "ê°€", "ì€", "ëŠ”", "ë¡œ", "ìœ¼ë¡œ"]
    for p in trailing_particles:
        if text.endswith(p) and len(text) > len(p) + 2:
            text = text[:-len(p)].strip()
            break

    # ìµœëŒ€ ê¸¸ì´ ì œí•œ
    if len(text) > max_len:
        # ìì—°ìŠ¤ëŸ¬ìš´ ëŠê¹€ ì°¾ê¸° (ì¡°ì‚¬/ê³µë°± ìœ„ì¹˜)
        for i in range(max_len, 10, -1):
            if text[i-1] in "ì„ë¥¼ì´ê°€ì—ì„œì™€ê³¼ ":
                text = text[:i].strip()
                break
        else:
            text = text[:max_len]

    # ë„ˆë¬´ ì§§ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´
    if len(text) < 5:
        return ""

    return text


def _extract_topic_from_answer(raw_answer: str, qtype: str = "ê²½í—˜ ìš”êµ¬í˜•") -> str:
    """ìì†Œì„œ ì›ë¬¸ì—ì„œ í•µì‹¬ ì£¼ì œ í‚¤ì›Œë“œ ì¶”ì¶œ (ì–´ë–¤ ìì†Œì„œë“  ì‘ë™)

    qtypeì— ë”°ë¼ ë‹¤ë¥¸ í‚¤ì›Œë“œ ìš°ì„ ìˆœìœ„ ì ìš©:
    - ë™ê¸°Â·ì •ì²´ì„±í˜•: ìŠ¹ë¬´ì›, í•­ê³µì‚¬, ì„œë¹„ìŠ¤, ê¿ˆ, ëª©í‘œ ê´€ë ¨
    - ê²½í—˜ ìš”êµ¬í˜•: ê²½í—˜, í™œë™, í”„ë¡œì íŠ¸, ë¬¸ì œí•´ê²° ê´€ë ¨
    - ê°€ì¹˜Â·íƒœë„í˜•: ê°€ì¹˜, ë°°ë ¤, ì†Œí†µ, í˜‘ë ¥ ê´€ë ¨
    """
    if not raw_answer or len(raw_answer.strip()) < 10:
        return ""

    text = raw_answer.strip()

    # qtypeë³„ í•µì‹¬ í‚¤ì›Œë“œ íŒ¨í„´
    TOPIC_PATTERNS = {
        "ë™ê¸°Â·ì •ì²´ì„±í˜•": [
            (r"ìŠ¹ë¬´ì›[ì˜\s]*(ë”°ëœ»í•œ\s*)?(ë¯¸ì†Œ|ë°°ë ¤|ì„œë¹„ìŠ¤|ëª¨ìŠµ)", "ìŠ¹ë¬´ì›ì˜ {0}"),
            (r"(ë”°ëœ»í•œ|ì„¬ì„¸í•œ|ì¹œì ˆí•œ)\s*(ë¯¸ì†Œ|ë°°ë ¤|ì„œë¹„ìŠ¤)", "{0} {1}"),
            (r"(í•­ê³µì‚¬|ë¹„í–‰ê¸°|ê¸°ë‚´)[ì—ì„œì˜\s]*(ê²½í—˜|ì„œë¹„ìŠ¤|ë¶„ìœ„ê¸°)", "{0} {1}"),
            (r"(ê¿ˆ|ëª©í‘œ|ë¹„ì „)[ì„ë¥¼ì´ê°€]\s*(ê°–|í’ˆ|í‚¤)", "{0}"),
            (r"ìŠ¹ë¬´ì›[ì´ê°€]\s*ë˜ê³ \s*ì‹¶", "ìŠ¹ë¬´ì›ì´ ë˜ê³  ì‹¶ì€ ë§ˆìŒ"),
        ],
        "ê²½í—˜ ìš”êµ¬í˜•": [
            (r"(íŒ€|ë™ì•„ë¦¬|í”„ë¡œì íŠ¸|ëŒ€íšŒ|í™œë™)[ì—ì„œ\s]*(í˜‘ë ¥|í˜‘ë™|ì°¸ì—¬|ê²½í—˜)", "{0}ì—ì„œì˜ {1}"),
            (r"(ë¬¸ì œ|ê°ˆë“±|ì–´ë ¤ì›€)[ì„ë¥¼]\s*(í•´ê²°|ê·¹ë³µ|ëŒ€ì²˜)", "{0} {1} ê²½í—˜"),
            (r"(ë¦¬ë”|íŒ€ì¥|ëŒ€í‘œ)[ë¡œì„œ\s]*(ì—­í• |ê²½í—˜|í™œë™)", "{0}ë¡œì„œì˜ {1}"),
            (r"(ê³ ê°|ì†ë‹˜|ì‚¬ëŒ)[ì„ë¥¼ì—ê²Œ\s]*(ì„œë¹„ìŠ¤|ì‘ëŒ€|ë„ì›€)", "{0} {1} ê²½í—˜"),
            (r"(ì•„ë¥´ë°”ì´íŠ¸|ì¸í„´|ê·¼ë¬´)[ì—ì„œ\s]*(ê²½í—˜|ë°°ìš´)", "{0} ê²½í—˜"),
        ],
        "ê°€ì¹˜Â·íƒœë„ ì •í•©ì„±í˜•": [
            (r"(ë°°ë ¤|ì†Œí†µ|í˜‘ë ¥|íŒ€ì›Œí¬|ì±…ì„ê°)[ì˜\s]*(ì¤‘ìš”|ê°€ì¹˜|ì˜ë¯¸)", "{0}ì˜ ì¤‘ìš”ì„±"),
            (r"(ê³ ê°|ìŠ¹ê°|ì‚¬ëŒ)[ì„ë¥¼]\s*(ë°°ë ¤|ì¡´ì¤‘|ì´í•´)", "{0} {1}"),
            (r"(ì„œë¹„ìŠ¤|ì•ˆì „|ê·œì •)[ì˜\s]*(ê°€ì¹˜|ì¤‘ìš”|ì˜ë¯¸)", "{0}ì˜ ê°€ì¹˜"),
            (r"(ì •ì§|ì„±ì‹¤|ì±…ì„)[í•˜ê²Œ\s]*(í–‰ë™|ì‹¤ì²œ|ë…¸ë ¥)", "{0}í•œ íƒœë„"),
        ],
    }

    # qtypeì— ë§ëŠ” íŒ¨í„´ ìš°ì„  ì‹œë„
    patterns = TOPIC_PATTERNS.get(qtype, [])
    for pattern, template in patterns:
        match = re.search(pattern, text)
        if match:
            groups = match.groups()
            try:
                topic = template.format(*[g for g in groups if g])
                if len(topic) >= 5 and len(topic) <= 25:
                    return topic
            except Exception as e:
                logger.debug(f'Exception occurred: {e}')
                pass

    # ì¼ë°˜ íŒ¨í„´ (qtype ë¬´ê´€)
    general_patterns = [
        (r"(ìŠ¹ë¬´ì›|ì„œë¹„ìŠ¤|ê³ ê°|íŒ€ì›Œí¬|í˜‘ë ¥|ë°°ë ¤|ì†Œí†µ)[ì˜ì—\s]+([\wê°€-í£]+)", "{0}ì˜ {1}"),
        (r"(ë”°ëœ»í•œ|ì„¬ì„¸í•œ|ì¹œì ˆí•œ|ì ê·¹ì ì¸)\s*([\wê°€-í£]+)", "{0} {1}"),
    ]

    for pattern, template in general_patterns:
        match = re.search(pattern, text)
        if match:
            groups = match.groups()
            try:
                topic = template.format(*[g for g in groups if g])
                if len(topic) >= 5 and len(topic) <= 25:
                    return topic
            except Exception as e:
                logger.debug(f'Exception occurred: {e}')
                pass

    return ""


def _is_abstract_sentence(point: str) -> bool:
    """ì¶”ìƒì /ë¹„ìœ ì  ë¬¸ì¥ì¸ì§€ ê°ì§€"""
    point_lower = point.lower()
    for pattern in ABSTRACT_SENTENCE_PATTERNS:
        if pattern in point_lower:
            return True
    return False


def _is_dream_sentence(point: str) -> bool:
    """ê¿ˆ/ì†Œë§/ëª©í‘œ ë¬¸ì¥ì¸ì§€ ê°ì§€ (ê²½í—˜ì´ ì•„ë‹Œ ë¬¸ì¥)"""
    dream_patterns = [
        "ë˜ê³  ì‹¶", "ì‹¶ë‹¤ëŠ” ê¿ˆ", "ê¿ˆì„ í’ˆ", "ê¿ˆì„ ê°–", "ê¿ˆì´ ìƒ",
        "ë˜ê² ë‹¤ëŠ”", "ë˜ê³ ì", "ì‹¶ìŠµë‹ˆë‹¤", "ì‹¶ì—ˆìŠµë‹ˆë‹¤",
        "ëª©í‘œë¥¼ ê°–", "ëª©í‘œê°€ ìƒ", "ë¹„ì „ì„",
        "ë‹¤ì§", "ê²°ì‹¬", "ê°ì˜¤",
    ]
    point_lower = point.lower()
    for pattern in dream_patterns:
        if pattern in point_lower:
            return True
    return False


def _is_context_dependent_sentence(point: str) -> bool:
    """ë§¥ë½ ì—†ì´ ì´í•´ ë¶ˆê°€ëŠ¥í•œ ë¬¸ì¥ì¸ì§€ ê°ì§€

    ì´ëŸ° ë¬¸ì¥ì€ ê³µê²© í¬ì¸íŠ¸ë¡œ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ:
    - íŠ¹ì • ì‚¬ê±´/ì‚¬íƒœ ì–¸ê¸‰ (ëŒ€í•œí•­ê³µ ì‚¬íƒœ, OO í”„ë¡œì íŠ¸)
    - ë¹„ìœ /ì€ìœ ê°€ ë§¥ë½ ì—†ì´ ì´í•´ ë¶ˆê°€ (ì‚´ë³´ë‹¤ ë¼ˆë¥¼...)
    - ì§€ì‹œì–´ë§Œ ìˆëŠ” ë¬¸ì¥ (ì´ë²ˆì˜, ê·¸ë•Œì˜, ê·¸ê²ƒì€...)
    - ë„ˆë¬´ ì§§ê±°ë‚˜ ì˜ë¯¸ íŒŒì•… ë¶ˆê°€
    - ë¶ˆì™„ì „í•œ ë¬¸ì¥ (ë™ì‚¬ ì—†ì´ ëë‚¨, ì—°ê²°ì–´ë¯¸ë¡œ ëë‚¨)

    ìê¸°ì†Œê°œì„œë¥¼ ëª¨ë¥´ëŠ” ì‚¬ëŒë„ ì§ˆë¬¸ë§Œ ë³´ê³  ëŒ€ë‹µí•  ìˆ˜ ìˆì–´ì•¼ í•¨.
    """
    point = point.strip()

    # ë„ˆë¬´ ì§§ì€ ë¬¸ì¥ì€ ì œì™¸ (ë§¥ë½ ì—†ì´ ì´í•´ ë¶ˆê°€)
    if len(point) < 15:
        return True

    # ë¶ˆì™„ì „í•œ ë¬¸ì¥ ê°ì§€ (ë™ì‚¬ ì—†ì´ ëë‚˜ëŠ” íŒ¨í„´)
    incomplete_endings = [
        # ê´€í˜•í˜• ì–´ë¯¸ (ëª…ì‚¬ê°€ ë’¤ì— ì™€ì•¼ í•¨)
        "ì„ í–¥í•œ", "ë¥¼ í–¥í•œ", "ì— í–¥í•œ", "í–¥í•œ",
        "ì„ ìœ„í•œ", "ë¥¼ ìœ„í•œ", "ì— ìœ„í•œ", "ìœ„í•œ",
        "ê³¼ í•¨ê»˜", "ì™€ í•¨ê»˜",
        "ì„ í†µí•œ", "ë¥¼ í†µí•œ", "í†µí•œ",
        "ì— ëŒ€í•œ", "ì—ì„œì˜", "ìœ¼ë¡œì˜", "ë¡œì˜",
        "ê³¼ì˜", "ì™€ì˜", "ì—ê²Œì˜",
        "ì´ë¼ëŠ”", "ë¼ëŠ”", "ë¼ê³  í•˜ëŠ”",
        # ê´€í˜•ì‚¬í˜• ì „ì„±ì–´ë¯¸
        "í•˜ëŠ”", "ë˜ëŠ”", "ì¸", "ì ì¸", "ìŠ¤ëŸ¬ìš´",
        "ê°™ì€", "ë‹¤ë¥¸", "ìƒˆë¡œìš´", "ë†’ì€", "ë‚®ì€",
        # ë¶€ì‚¬í˜•ìœ¼ë¡œ ëë‚˜ëŠ” ë¶ˆì™„ì „ ë¬¸ì¥
        "ì²˜ëŸ¼", "ê°™ì´", "ëŒ€ë¡œ",
        # ì ‘ì†ì¡°ì‚¬/ë³´ì¡°ì‚¬ë¡œ ëë‚˜ëŠ” ë¬¸ì¥
        "ê·¸ë¦¬ê³ ", "ê·¸ëŸ¬ë‚˜", "í•˜ì§€ë§Œ", "ë”°ë¼ì„œ",
    ]
    for ending in incomplete_endings:
        if point.endswith(ending):
            return True

    # ì—°ê²°ì–´ë¯¸ë¡œ ëë‚˜ëŠ” ë¶ˆì™„ì „í•œ ë¬¸ì¥
    connecting_endings = [
        "í•˜ê³ ", "í•˜ë©°", "í•˜ë©´ì„œ", "í•˜ì—¬", "í•´ì„œ",
        "ë˜ê³ ", "ë˜ë©°", "ë˜ë©´ì„œ",
        "ìœ¼ë©°", "ë©°", "ë©´ì„œ",
        "ì§€ë§Œ", "ëŠ”ë°", "ã„´ë°",
        "ìœ¼ë©´", "ë©´", "ë‹ˆê¹Œ", "ë¯€ë¡œ", "ì–´ì„œ", "ì•„ì„œ",
        "ë ¤ê³ ", "ìœ¼ë ¤ê³ ", "ê³ ì", "ë„ë¡",
    ]
    for ending in connecting_endings:
        if point.endswith(ending):
            return True

    # ì¡°ì‚¬ë¡œë§Œ ëë‚˜ëŠ” ë¶ˆì™„ì „í•œ ë¬¸ì¥ (ì£¼ì–´/ëª©ì ì–´ë§Œ ìˆê³  ë™ì‚¬ ì—†ìŒ)
    if re.search(r"[ì„ë¥¼ì´ê°€ì€ëŠ”ë„ë§Œ]$", point) and len(point) < 25:
        return True

    # "~ì™€/ê³¼" ë¡œ ëë‚˜ë©´ ë¶ˆì™„ì „ (ë³‘ë ¬ êµ¬ì¡°ê°€ ëŠê¹€)
    if re.search(r"[ì™€ê³¼]$", point):
        return True

    context_dependent_patterns = [
        # íŠ¹ì • ì‚¬ê±´/ì‚¬íƒœ (ê³ ìœ ëª…ì‚¬ ì´ë²¤íŠ¸)
        "ì‚¬íƒœ", "ì‚¬ê±´", "í”„ë¡œì íŠ¸", "ë‹¹ì‹œ", "ê·¸ë•Œ", "ê·¸ ë‹¹ì‹œ",
        "OO", "XX", "â—‹â—‹", "Ã—Ã—",  # ìµëª…í™”ëœ í‘œí˜„

        # ë¹„ìœ /ì€ìœ  (ë§¥ë½ ì—†ì´ ì´í•´ ë¶ˆê°€)
        "ì‚´ë³´ë‹¤", "ë¼ˆë¥¼", "í”¼ë¥¼", "ëˆˆë¬¼ì„", "í”¼ì™€ ë•€",
        "ì‚´ê³¼ ë¼ˆ", "í”¼ì™€ ëˆˆë¬¼", "ì•„í””ì´ í•¨ê»˜",

        # ì§€ì‹œì–´/ëŒ€ëª…ì‚¬ ì¤‘ì‹¬ ë¬¸ì¥
        "ì´ë²ˆì˜", "ì´ë²ˆì—", "ê·¸ë•Œì˜", "ê·¸ê²ƒì€", "ì´ê²ƒì€",
        "ê·¸ ìˆœê°„", "ê·¸ë‚ ", "ê·¸ ì¼", "ì´ ì¼",

        # ì¶”ìƒì /ì² í•™ì  í‘œí˜„ (êµ¬ì²´ì„± ë¶€ì¡±)
        "ì—­ì‚¬ë¥¼", "ë°œì „ê³¼ ì—­ì‚¬", "ìƒˆë¡œìš´ ì‹œì‘ê³¼ ë",
        "ì¸ìƒì˜ ì „í™˜ì ", "ì‚¶ì˜ ì˜ë¯¸",

        # ë¬¸ë§¥ ì˜ì¡´ì  ë¹„êµ í‘œí˜„
        "ê·¸ë³´ë‹¤", "ì´ë³´ë‹¤", "ì €ë³´ë‹¤",
        "ì „ë³´ë‹¤", "ì˜ˆì „ë³´ë‹¤", "ì´ì „ë³´ë‹¤",

        # íŠ¹ì • ì§ìœ„/ê´€ê³„ ì–¸ê¸‰ (êµ¬ì²´ì  ìƒí™© ëª¨ë¦„)
        "â—‹â—‹ë‹˜", "â–³â–³ë‹˜", "ã…‡ã…‡ë‹˜",
    ]
    point_lower = point.lower()
    for pattern in context_dependent_patterns:
        if pattern in point_lower:
            return True

    # ë¬¸ì¥ì´ ì§€ì‹œì–´ë¡œ ì‹œì‘í•˜ë©´ ë§¥ë½ ì˜ì¡´
    if point.startswith(("ê·¸", "ì´", "ì €")) and len(point) < 30:
        return True

    # ì™„ì „í•œ ë¬¸ì¥ì¸ì§€ ê²€ì‚¬ (ë™ì‚¬/í˜•ìš©ì‚¬ ì–´ë¯¸ê°€ ìˆì–´ì•¼ í•¨)
    complete_endings = [
        "ë‹¤", "ìš”", "ì£ ", "ë‹ˆë‹¤", "ìŠµë‹ˆë‹¤", "ì…ë‹ˆë‹¤",
        "í–ˆë‹¤", "í–ˆìŠµë‹ˆë‹¤", "í–ˆì–´ìš”", "ëë‹¤", "ëìŠµë‹ˆë‹¤",
        "ìˆë‹¤", "ìˆìŠµë‹ˆë‹¤", "ì—†ë‹¤", "ì—†ìŠµë‹ˆë‹¤",
        "ì´ë‹¤", "ì´ì—ˆë‹¤", "ì˜€ë‹¤",
    ]
    has_complete_ending = any(point.endswith(e) for e in complete_endings)

    # ì™„ì „í•œ ì–´ë¯¸ê°€ ì—†ì–´ë„ ì˜ë¯¸ìˆëŠ” ëª…ì‚¬êµ¬ëŠ” í—ˆìš© (ì˜ˆ: "í•¨ê»˜ë¼ì„œ ê·¹ë³µí•  ìˆ˜ ìˆì—ˆë‹¤")
    meaningful_patterns = [
        r"í•  ìˆ˜ ìˆ",
        r"ìˆ˜ ìˆë‹¤",
        r"ê·¹ë³µ", "í•´ê²°", "ì´ê²¨ëƒˆ", "ì„±ê³µ",
        r"í–ˆìŠµë‹ˆë‹¤", r"í–ˆë‹¤", r"í–ˆì–´ìš”",
    ]
    has_meaningful = any(re.search(p, point) for p in meaningful_patterns)

    if not has_complete_ending and not has_meaningful and len(point) < 40:
        return True

    return False


def _get_sentence_topic(point: str) -> str:
    """ë¬¸ì¥ì˜ ì£¼ì œë¥¼ ì¶”ì¶œ (ê³ ê°, íŒ€, ê°œì¸ ë“±)"""
    point_lower = point.lower()

    if any(kw in point_lower for kw in ["ê³ ê°", "ì†ë‹˜", "ìŠ¹ê°", "íƒ‘ìŠ¹ê°", "ì„œë¹„ìŠ¤"]):
        return "customer"
    if any(kw in point_lower for kw in ["íŒ€", "ë™ë£Œ", "í˜‘ë ¥", "í˜‘ë™", "í•¨ê»˜", "ìš°ë¦¬"]):
        return "team"
    if any(kw in point_lower for kw in ["ìƒì‚¬", "ì‚¬ë¬´ì¥", "ì„ ë°°", "ê¸°ì¥"]):
        return "senior"
    if any(kw in point_lower for kw in ["ì•ˆì „", "ê·œì •", "ì ˆì°¨", "ë§¤ë‰´ì–¼"]):
        return "safety"
    if any(kw in point_lower for kw in ["ì„±ì¥", "ë°œì „", "ë…¸ë ¥", "ë„ì „"]):
        return "growth"

    return "general"


def _extract_short_point(point: str) -> str:
    """ê¸´ ë¬¸ì¥ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ/í‘œí˜„ë§Œ ì¶”ì¶œ (ì˜ë¯¸ìˆëŠ” ì™„ì „í•œ êµ¬ë¬¸)"""
    # ë¶ˆì™„ì „í•œ ëª…ì‚¬ ë íŒ¨í„´ (ì œëª©/ì¡°ê° í˜•íƒœ)
    bad_endings = re.compile(
        r'(ì¥ë©´|ëª¨ìŠµ|ì‚¬ëŒ|ë§ˆìŒ|ìƒê°|ìì„¸|íƒœë„|ìˆœê°„|ê²½í—˜|ëŠ¥ë ¥|ì—­ëŸ‰|ê°€ì¹˜|'
        r'ì´ìœ |ê³„ê¸°|ë™ê¸°|ëª©í‘œ|ê¿ˆ|ë¹„ì „|ì—´ì •|ë…¸ë ¥|ì„±ì¥|ë³€í™”|ë„ì „|ì‹œì‘ì |ê¸°ì |ì¶œë°œì )$'
    )

    def is_valid_short(text: str) -> bool:
        """short_pointê°€ ìœ íš¨í•œì§€ ê²€ì¦"""
        text = text.strip()
        if len(text) < 6:
            return False
        # ëª…ì‚¬ë¡œ ëë‚˜ëŠ” ë¶ˆì™„ì „ ì¡°ê° ê±°ë¶€
        if bad_endings.search(text):
            return False
        return True

    # ë”°ì˜´í‘œ ì•ˆì˜ ë‚´ìš© ì¶”ì¶œ (10ì ì´ìƒì´ì–´ì•¼ ì˜ë¯¸ìˆìŒ)
    quote_match = re.search(r"['\"]([^'\"]{10,40})['\"]", point)
    if quote_match:
        result = quote_match.group(1)
        if is_valid_short(result):
            return result

    # ì˜ë¯¸ìˆëŠ” í•µì‹¬ í‘œí˜„ íŒ¨í„´ (ì™„ì „í•œ êµ¬ë¬¸ìœ¼ë¡œ ì¶”ì¶œ)
    meaningful_patterns = [
        # êµ¬ì²´ì ì¸ í‘œí˜„ë“¤ (ì™„ì „í•œ í˜•íƒœë¡œ)
        r"(ë”°ëœ»í•˜ê²Œ ë§ì´í•˜ëŠ”)",
        r"(ë”°ëœ»í•˜ê²Œ ë§ì´í•˜)",
        r"(ì²« ì—¬ì •ì„ ë”°ëœ»í•˜ê²Œ ë§ì´)",
        r"(ëˆ„êµ°ê°€ì˜ ì²« ì—¬ì •ì„ ë”°ëœ»í•˜ê²Œ)",
        # ê¿ˆ/ì†Œë§ ê´€ë ¨
        r"(ë˜ê³  ì‹¶ë‹¤ëŠ” ê¿ˆì„ í’ˆê²Œ)",
        r"(ìŠ¹ë¬´ì›ì´ ë˜ê³  ì‹¶ë‹¤)",
        r"(ê¿ˆì„ í’ˆê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤)",
        r"(ê¿ˆì„ í’ˆê²Œ ë˜ì—ˆ)",
        # ë™ì‚¬êµ¬ íŒ¨í„´ (ì™„ì „í•œ í˜•íƒœ)
        r"([ê°€-í£]+í•˜ê²Œ [ê°€-í£]+í•˜ëŠ”)",
        r"([ê°€-í£]+ì„ [ê°€-í£]+í•˜ëŠ”)",
    ]
    for pattern in meaningful_patterns:
        match = re.search(pattern, point)
        if match:
            result = match.group(1)
            if 6 <= len(result) <= 30 and is_valid_short(result):
                return result

    # ë™ì‚¬ ì–´ë¯¸ë¡œ ëë‚˜ëŠ” êµ¬ë¬¸ ì¶”ì¶œ (ë” ìì—°ìŠ¤ëŸ¬ìš´ ëŠê¹€)
    # "~í•˜ëŠ”", "~í•˜ë‹¤", "~ë˜ì—ˆ" ë“±ìœ¼ë¡œ ëë‚˜ëŠ” ë¶€ë¶„ ì°¾ê¸°
    verb_ending_match = re.search(r"([ê°€-í£]{2,8}(?:í•˜ëŠ”|í•˜ë‹¤|ë˜ëŠ”|ë˜ë‹¤|ë˜ì—ˆ|ì˜€ë‹¤|í–ˆë‹¤|ì‹¶ë‹¤|ì‹¶ì€))", point)
    if verb_ending_match:
        result = verb_ending_match.group(1)
        if 6 <= len(result) <= 20:
            return result

    # í´ë°±: ë¬¸ì¥ì´ ë„ˆë¬´ ê¸¸ë©´ ë™ì‚¬ ì–´ë¯¸ì—ì„œ ìë¥´ê¸° (ëª…ì‚¬ ë ê¸ˆì§€)
    if len(point) > 35:
        # ë™ì‚¬ ì–´ë¯¸ê°€ ìˆëŠ” ìœ„ì¹˜ì—ì„œ ìë¥´ê¸°
        verb_endings = ["ìŠµë‹ˆë‹¤", "í•©ë‹ˆë‹¤", "í–ˆìŠµë‹ˆë‹¤", "ë©ë‹ˆë‹¤", "ì…ë‹ˆë‹¤", "í•˜ëŠ”", "ë˜ëŠ”"]
        for ve in verb_endings:
            idx = point.find(ve, 15, 50)
            if idx > 0:
                return point[:idx + len(ve)]

    # ìµœì¢… í´ë°±: pointê°€ ë„ˆë¬´ ê¸¸ë©´ ì•ë¶€ë¶„ë§Œ + "..." í˜•íƒœë¡œ (ì¡°ê° ë°©ì§€)
    if len(point) > 30:
        # ì´ ê²½ìš° ì „ì²´ ë¬¸ì¥ì„ ì‚¬ìš©í•˜ë„ë¡ point ë°˜í™˜
        return point[:25] + "..."

    return point


def _extract_who_from_context(point: str, raw_answer: str = "") -> str:
    """ë¬¸ë§¥ì—ì„œ ìƒëŒ€ë°©ì´ ëˆ„êµ¬ì¸ì§€ ì¶”ì¶œ (ê³ ê°, ë™ë£Œ, ìŠ¹ê° ë“±)"""
    combined = (point + " " + raw_answer).lower()

    if any(kw in combined for kw in ["ê³ ê°", "ì†ë‹˜", "ìŠ¹ê°", "íƒ‘ìŠ¹ê°", "ì—¬í–‰ê°"]):
        return "ê³ ê°"
    if any(kw in combined for kw in ["ë™ë£Œ", "íŒ€ì›", "ì„ ë°°", "í›„ë°°", "í¬ë£¨", "ìŠ¹ë¬´ì›"]):
        return "ë™ë£Œ"
    if any(kw in combined for kw in ["ìƒì‚¬", "íŒ€ì¥", "ì‚¬ë¬´ì¥", "ê¸°ì¥"]):
        return "ìƒì‚¬"
    if any(kw in combined for kw in ["ê°€ì¡±", "ë¶€ëª¨", "ì¹œêµ¬"]):
        return "ì£¼ë³€ ì‚¬ëŒë“¤"

    # í´ë°±: ìŠ¹ë¬´ì› ë§¥ë½ì—ì„œëŠ” ì£¼ë¡œ ê³ ê°
    return "ìƒëŒ€ë°©"


def _extract_judgment_from_point(point: str) -> str:
    """pointì—ì„œ êµ¬ì²´ì  íŒë‹¨/í–‰ë™ì„ ì¶”ì¶œ (ì˜ˆ: 'ì„œë¡œ ì˜ì§€í•˜ê² ë‹¤')"""
    # í‚¤ì›Œë“œ ê¸°ë°˜ íŒë‹¨ ì¶”ì¶œ
    judgment_map = [
        (["í•¨ê»˜", "ê°™ì´", "í˜‘ë ¥"], "í•¨ê»˜ í•´ê²°í•˜ê² ë‹¤"),
        (["ê·¹ë³µ", "ì´ê²¨", "í•´ê²°"], "ê·¹ë³µí•˜ê² ë‹¤"),
        (["ì†Œí†µ", "ëŒ€í™”"], "ì†Œí†µí•˜ê² ë‹¤"),
        (["ì˜ì§€", "ì§€ì§€", "ë„ì›€"], "ì„œë¡œ ì˜ì§€í•˜ê² ë‹¤"),
        (["ì„±ì¥", "ë°œì „", "ì„±ê³µ"], "ì„±ì¥í•˜ê² ë‹¤"),
        (["ë…¸ë ¥", "ì—´ì‹¬", "ìµœì„ "], "ë…¸ë ¥í•˜ê² ë‹¤"),
        (["ë„ì „", "ì‹œë„"], "ë„ì „í•˜ê² ë‹¤"),
        (["ë°°ë ¤", "ì¹œì ˆ", "ë¯¸ì†Œ"], "ë°°ë ¤í•˜ê² ë‹¤"),
        (["ë¯¿ìŒ", "ì‹ ë¢°"], "ì‹ ë¢°í•˜ê² ë‹¤"),
        (["í¬ê¸°", "ë²„í‹°"], "í¬ê¸°í•˜ì§€ ì•Šê² ë‹¤"),
        (["ê¿ˆ", "ëª©í‘œ"], "ê¿ˆì„ ì´ë£¨ê² ë‹¤"),
        (["ê°ì‚¬", "ê°ë™"], "ê°ì‚¬í•˜ê² ë‹¤"),
    ]

    point_lower = point.lower()
    for keywords, judgment in judgment_map:
        for kw in keywords:
            if kw in point_lower:
                return judgment

    # í´ë°±: pointì—ì„œ ë™ì‚¬ ì¶”ì¶œ ì‹œë„
    # "~í–ˆë‹¤", "~ê² ë‹¤" ë“±ì˜ íŒ¨í„´ì—ì„œ ì¶”ì¶œ
    match = re.search(r"([ê°€-í£]+(?:í–ˆ|ê² |í•˜ê² |í•  ìˆ˜ ìˆ))", point)
    if match:
        return match.group(1)

    return "ê°™ì€ ì„ íƒì„ í•˜ê² ë‹¤"


def _extract_premise_from_point(point: str) -> Tuple[str, str]:
    """ì´ìƒì  í‘œí˜„ì—ì„œ ìˆ¨ê²¨ì§„ ì „ì œì™€ ì „ì œê°€ ê¹¨ì§„ ìƒí™©ì„ ì¶”ì¶œ

    ë¬¸ì¥ ì£¼ì œë¥¼ ë¨¼ì € íŒŒì•…í•˜ì—¬ ì£¼ì œì— ë§ëŠ” ì „ì œë§Œ ì„ íƒí•¨.
    ì˜ˆ: ê³ ê° ê´€ë ¨ ë¬¸ì¥ì´ ì•„ë‹ˆë©´ "ê³ ê°ì´ ëƒ‰ë‹´í•œ" ì „ì œ ì‚¬ìš© ì•ˆí•¨.
    """
    # ë¨¼ì € ë¬¸ì¥ ì£¼ì œ íŒŒì•…
    sentence_topic = _get_sentence_topic(point)

    # í‚¤ì›Œë“œ ê¸°ë°˜ ì „ì œ ë§¤í•‘ - ì£¼ì œë³„ë¡œ ë¶„ë¥˜
    # (keywords, premise, premise_broken, allowed_topics)
    # allowed_topicsê°€ Noneì´ë©´ ëª¨ë“  ì£¼ì œì— ì‚¬ìš© ê°€ëŠ¥
    premise_map = [
        # â˜… 1ìˆœìœ„: ê¿ˆ/ì†Œë§/ëª©í‘œ ê´€ë ¨ (ëª¨ë“  ì£¼ì œì— í—ˆìš©)
        (["ë˜ê³  ì‹¶", "ì‹¶ë‹¤ëŠ” ê¿ˆ", "ê¿ˆì„ í’ˆ", "ê¿ˆì„ ê°–"], "ê·¸ ê¿ˆì„ ì´ë£° ê¸°íšŒê°€ ìˆë‹¤ëŠ” ê²ƒ", "í˜„ì‹¤ì ì¸ ì œì•½ìœ¼ë¡œ ê·¸ ê¿ˆì„ ì´ë£¨ê¸° ì–´ë ¤ìš´ ìƒí™©ì´ë¼ë©´", None),
        (["ê¿ˆ", "ëª©í‘œ", "ë¹„ì „"], "ê·¸ ê¿ˆì„ í–¥í•´ ë‚˜ì•„ê°ˆ ìˆ˜ ìˆë‹¤ëŠ” ê²ƒ", "í˜„ì‹¤ì˜ ë²½ì— ë¶€ë”ªí˜€ ê¿ˆì´ í”ë“¤ë¦¬ëŠ” ìƒí™©ì´ë¼ë©´", None),

        # â˜… 2ìˆœìœ„: êµ¬ì²´ì  ìƒí™©/í–‰ë™ (ì£¼ì œë³„ í•„í„°ë§)
        (["í•¨ê»˜", "ê°™ì´", "í˜‘ë ¥", "í˜‘ë™", "ê³µë™ì²´"], "ì£¼ë³€ ì‚¬ëŒë“¤ì´ í˜‘ì¡°ì ì´ë¼ëŠ” ê²ƒ", "ì£¼ë³€ ì‚¬ëŒë“¤ì´ ë¹„í˜‘ì¡°ì ì¸ ìƒí™©ì´ë¼ë©´", ["team", "general"]),
        (["ê·¹ë³µ", "ì´ê²¨", "í•´ê²°", "ë„˜"], "ë¬¸ì œê°€ í•´ê²°ë  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒ", "ì•„ë¬´ë¦¬ ë…¸ë ¥í•´ë„ í•´ê²°ë˜ì§€ ì•ŠëŠ” ìƒí™©ì´ë¼ë©´", None),
        (["ì†Œí†µ", "ëŒ€í™”", "ì»¤ë®¤ë‹ˆì¼€ì´ì…˜", "ì´ì•¼ê¸°"], "ìƒëŒ€ë°©ì´ ëŒ€í™”ì— ì‘í•œë‹¤ëŠ” ê²ƒ", "ìƒëŒ€ë°©ì´ ëŒ€í™” ìì²´ë¥¼ ê±°ë¶€í•˜ëŠ” ìƒí™©ì´ë¼ë©´", None),
        (["íŒ€ì›Œí¬", "íŒ€", "ìš°ë¦¬"], "íŒ€ì›ë“¤ì´ ê°™ì€ ë°©í–¥ì„ ë³´ê³  ìˆë‹¤ëŠ” ê²ƒ", "íŒ€ì›ë“¤ ê°„ ëª©í‘œê°€ ì¶©ëŒí•˜ëŠ” ìƒí™©ì´ë¼ë©´", ["team", "general"]),
        (["ê³ ê°", "ì†ë‹˜", "ìŠ¹ê°", "íƒ‘ìŠ¹ê°"], "ê³ ê°ì´ í•©ë¦¬ì ì´ë¼ëŠ” ê²ƒ", "ê³ ê°ì´ ë¬´ë¦¬í•œ ìš”êµ¬ë¥¼ ê³„ì†í•˜ëŠ” ìƒí™©ì´ë¼ë©´", ["customer"]),
        (["ì„œë¹„ìŠ¤"], "ì„œë¹„ìŠ¤ê°€ í™˜ì˜ë°›ëŠ”ë‹¤ëŠ” ê²ƒ", "ì„œë¹„ìŠ¤ì— ëŒ€í•´ ë¬´ê´€ì‹¬í•˜ê±°ë‚˜ ë¶ˆë§Œì¡±í•˜ëŠ” ìƒí™©ì´ë¼ë©´", ["customer", "general"]),
        (["ì‹ ë¢°", "ë¯¿ìŒ", "ë¯¿"], "ì„œë¡œ ì‹ ë¢°í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒ", "ìƒëŒ€ë°©ì´ ì•½ì†ì„ ì–´ê¸°ê±°ë‚˜ ë°°ì‹ í•˜ëŠ” ìƒí™©ì´ë¼ë©´", None),
        (["ë„ì „", "ìƒˆë¡œìš´", "ì‹œë„"], "ìƒˆë¡œìš´ ì‹œë„ê°€ í™˜ì˜ë°›ëŠ”ë‹¤ëŠ” ê²ƒ", "ë³€í™”ë¥¼ ê±°ë¶€í•˜ëŠ” ë³´ìˆ˜ì ì¸ í™˜ê²½ì´ë¼ë©´", None),
        (["ì˜ì§€", "ì§€ì§€", "ì‘ì›", "ë„ì›€"], "ëˆ„êµ°ê°€ ì˜†ì—ì„œ ë„ì™€ì¤€ë‹¤ëŠ” ê²ƒ", "ì•„ë¬´ë„ ë„ì™€ì£¼ì§€ ì•Šê³  í˜¼ìì¸ ìƒí™©ì´ë¼ë©´", None),
        (["ì™¸ë¡œ", "íƒ€ì§€", "ë‚¯ì„ ", "í˜¼ì"], "í•¨ê»˜í•  ì‚¬ëŒì´ ìˆë‹¤ëŠ” ê²ƒ", "ì™„ì „íˆ í˜¼ìì´ê³  ì˜ì§€í•  ì‚¬ëŒì´ ì—†ëŠ” ìƒí™©ì´ë¼ë©´", None),
        (["ì²«", "ì²˜ìŒ", "ì‹œì‘"], "ì²« ê²½í—˜ì´ ê¸ì •ì ì´ë¼ëŠ” ê²ƒ", "ì²« ê²½í—˜ì´ ë§¤ìš° ë¶€ì •ì ì¸ ìƒí™©ì´ë¼ë©´", None),

        # â˜… 3ìˆœìœ„: ì¼ë°˜ì /ì¶”ìƒì  í‚¤ì›Œë“œ (ì£¼ì œë³„ í•„í„°ë§)
        (["ì›ƒ", "ì¦ê±°", "ê¸ì •", "ë°"], "ë¶„ìœ„ê¸°ê°€ ì¢‹ë‹¤ëŠ” ê²ƒ", "ë¶„ìœ„ê¸°ê°€ í—˜ì•…í•˜ê±°ë‚˜ ê°ˆë“±ì´ ì‹¬í•œ ìƒí™©ì´ë¼ë©´", None),
        # "ë¯¸ì†Œ", "ì¹œì ˆ" ë“±ì€ ê³ ê° ì£¼ì œì¼ ë•Œë§Œ "ê³ ê°ì´ ëƒ‰ë‹´í•œ" ì‚¬ìš©
        (["ë¯¸ì†Œ", "ì¹œì ˆ", "ë°°ë ¤", "ë”°ëœ»", "ë°˜ê²¨", "ë§ì´"], "ê·¸ëŸ° íƒœë„ê°€ í™˜ì˜ë°›ëŠ”ë‹¤ëŠ” ê²ƒ", "ìƒëŒ€ë°©ì´ ê·¸ëŸ° íƒœë„ì— ëƒ‰ë‹´í•˜ê±°ë‚˜ ë¬´ê´€ì‹¬í•œ ìƒí™©ì´ë¼ë©´", ["general", "team", "growth"]),
        (["ë¯¸ì†Œ", "ì¹œì ˆ", "ë°°ë ¤", "ë”°ëœ»", "ë°˜ê²¨", "ë§ì´"], "ê³ ê°ì´ ê·¸ëŸ° ì„œë¹„ìŠ¤ì— ë§Œì¡±í•œë‹¤ëŠ” ê²ƒ", "ê³ ê°ì´ ê·¸ëŸ° ì„œë¹„ìŠ¤ì— ëƒ‰ë‹´í•˜ê±°ë‚˜ ë¬´ê´€ì‹¬í•œ ìƒí™©ì´ë¼ë©´", ["customer"]),
        (["ì„±ì¥", "ë°œì „", "ì„±ê³µ", "ì´ë£¨"], "ë…¸ë ¥í•˜ë©´ ì„±ê³¼ê°€ ë‚˜ì˜¨ë‹¤ëŠ” ê²ƒ", "ë…¸ë ¥í•´ë„ ì„±ê³¼ê°€ ì „í˜€ ë‚˜ì˜¤ì§€ ì•ŠëŠ” ìƒí™©ì´ë¼ë©´", None),
        (["ìµœì„ ", "ë…¸ë ¥", "ì—´ì‹¬"], "ë…¸ë ¥ì´ ì¸ì •ë°›ëŠ”ë‹¤ëŠ” ê²ƒ", "ì•„ë¬´ë¦¬ ì—´ì‹¬íˆ í•´ë„ ì¸ì •ë°›ì§€ ëª»í•˜ëŠ” ìƒí™©ì´ë¼ë©´", None),
        (["ì•ˆì •", "í‰í™”", "ì•ˆì „"], "í™˜ê²½ì´ ì•ˆì •ì ì´ë¼ëŠ” ê²ƒ", "í™˜ê²½ì´ ë¶ˆì•ˆì •í•˜ê³  ì˜ˆì¸¡ ë¶ˆê°€ëŠ¥í•œ ìƒí™©ì´ë¼ë©´", ["safety", "general"]),
        (["ê°ë™", "ê°ì‚¬", "ê³ ë§ˆ"], "ìƒëŒ€ë°©ë„ ê°ì‚¬í•¨ì„ ëŠë‚€ë‹¤ëŠ” ê²ƒ", "ë‚´ ë…¸ë ¥ì´ ë‹¹ì—°í•˜ê²Œ ì—¬ê²¨ì§€ëŠ” ìƒí™©ì´ë¼ë©´", None),
        (["ê¸°ì–µ", "ìŠì§€", "ë§ˆìŒ"], "ê·¸ ê¸°ì–µì´ í˜ì´ ëœë‹¤ëŠ” ê²ƒ", "ê·¸ ê¸°ì–µì´ ì˜¤íˆë ¤ ë¶€ë‹´ì´ ë˜ëŠ” ìƒí™©ì´ë¼ë©´", None),
        (["ì°¨ë³„", "ê³µì •", "í‰ë“±", "ëª¨ë‘ë¥¼"], "ëª¨ë“  ì‚¬ëŒì„ ë™ë“±í•˜ê²Œ ëŒ€í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒ", "í˜„ì‹¤ì ìœ¼ë¡œ ì°¨ë³„ ì—†ì´ ëŒ€í•˜ê¸° ì–´ë ¤ìš´ ìƒí™©ì´ë¼ë©´", None),
    ]

    point_lower = point.lower()
    for keywords, premise, premise_broken, allowed_topics in premise_map:
        # ì£¼ì œ í•„í„°ë§: allowed_topicsê°€ Noneì´ë©´ ëª¨ë“  ì£¼ì œ í—ˆìš©
        if allowed_topics is not None and sentence_topic not in allowed_topics:
            continue
        for kw in keywords:
            if kw in point_lower:
                return premise, premise_broken

    # í´ë°±: ì›ë¬¸ì—ì„œ í•µì‹¬ ë‹¨ì–´ë¥¼ ì¶”ì¶œí•´ì„œ ì „ì œ ìƒì„±
    # "~í–ˆë‹¤", "~ì˜€ë‹¤" ë“±ì˜ í‘œí˜„ì—ì„œ ìƒí™© ì¶”ë¡ 
    if "ìˆ˜ ìˆ" in point_lower or "í•  ìˆ˜" in point_lower:
        return "ê·¸ê²ƒì´ ê°€ëŠ¥í•˜ë‹¤ëŠ” ê²ƒ", "ê·¸ê²ƒì´ ë¶ˆê°€ëŠ¥í•œ ìƒí™©ì´ë¼ë©´"
    if "ë˜ì—ˆ" in point_lower or "ë" in point_lower:
        return "ê¸ì •ì ì¸ ê²°ê³¼ê°€ ë‚˜ì˜¨ë‹¤ëŠ” ê²ƒ", "ê²°ê³¼ê°€ ë¶€ì •ì ì¸ ìƒí™©ì´ë¼ë©´"

    # ìµœì¢… í´ë°±: ì›ë¬¸ ê¸°ë°˜ ìì—°ìŠ¤ëŸ¬ìš´ ì „ì œ
    short_point = point[:20] + "..." if len(point) > 20 else point
    return f"'{short_point}'ê°€ ê°€ëŠ¥í•˜ë‹¤ëŠ” ê²ƒ", "ê·¸ê²ƒì´ ë¶ˆê°€ëŠ¥í•œ ìƒí™©ì´ë¼ë©´"


def _apply_airline_tone(question: str, is_fsc: bool, is_soft: bool) -> str:
    """FSC/LCC í†¤ ê·œì¹™ì„ ì§ˆë¬¸ì— ì ìš© (config.py ìƒìˆ˜ í™œìš©)

    INTERVIEWER_TONE_RULESì—ì„œ ì •ì˜ëœ ê·œì¹™ ì ìš©:
    - FSC: ë§ìˆ˜ ì ìŒ, ê°ì • ì ˆì œ, ì§§ê³  ë‹¨ì •, ê²€ì¦/íŒë‹¨/ê¸°ì¤€ ìœ„ì£¼
    - LCC: êµ¬ì–´ì²´, í˜„ì¥ ìƒí™© ì—°ìƒ, íŒë‹¨â†’í–‰ë™â†’ê²°ê³¼ ë¹ ë¥´ê²Œ ìš”êµ¬

    KOREAN_QUESTION_RULES["forbidden_expressions"]ì—ì„œ ê¸ˆì§€ í‘œí˜„ ì œê±°
    TONE_CONVERSION_RULESì—ì„œ FSC/LCC + SOFT/SHARP ì¡°í•©ë³„ ë³€í™˜ ì ìš©
    """
    if not question:
        return question

    q = question.strip()

    # ê¸ˆì§€ í‘œí˜„ ì œê±° (KOREAN_QUESTION_RULES í™œìš©)
    forbidden_expressions = KOREAN_QUESTION_RULES.get("forbidden_expressions", [])
    for expr in forbidden_expressions:
        if expr == "ì„¤ëª…í•´ì£¼ì„¸ìš”":
            q = q.replace(expr, "ë§ì”€í•´ì£¼ì„¸ìš”")
        else:
            q = q.replace(expr, "")

    # í†¤ ë³€í™˜ í‚¤ ê²°ì •
    if is_fsc:
        tone_key = "FSC_SOFT" if is_soft else "FSC_SHARP"
        # FSC: ë¶ˆí•„ìš”í•œ ì™„ì¶© í‘œí˜„ ì œê±°
        fsc_trim = [
            ("í˜¹ì‹œ ", ""),
            ("ì•„ë§ˆ ", ""),
            ("ê·¸ëŸ°ë°ìš”, ", ""),
            ("ê·¸ëŸ¬ë©´ìš”, ", ""),
            ("ê·¸ëŸ¼ìš”, ", ""),
        ]
        for old, new in fsc_trim:
            q = q.replace(old, new)
    else:
        tone_key = "LCC_SOFT" if is_soft else "LCC_SHARP"

    # TONE_CONVERSION_RULESì—ì„œ ë³€í™˜ ê·œì¹™ ì ìš©
    tone_rule = TONE_CONVERSION_RULES.get(tone_key, {})
    conversions = tone_rule.get("conversions", [])
    for old, new in conversions:
        q = q.replace(old, new)

    # ì¤‘ë³µ ê³µë°± ì œê±°
    while "  " in q:
        q = q.replace("  ", " ")

    return q.strip()


def _slot_q2_deep(
    base: int,
    version: int,
    qtype: str,
    situation: str,
    llm_item: Optional[Dict[str, Any]],
    basis: str,
    raw_answer: str = "",
    atype: str = "LCC",
) -> Tuple[str, str, str]:
    """Q2: ê²€ì¦í˜• ì‹¬ì¸µ ì§ˆë¬¸ (ìƒˆ í”„ë¡¬í”„íŠ¸ ê¸°ë°˜ - deep_questions ìš°ì„  ì‚¬ìš©)

    FSC/LCC í†¤ ê·œì¹™:
    - FSC: ë§ìˆ˜ ì ìŒ, ê°ì • ì ˆì œ, ì§§ê³  ë‹¨ì •í•œ ë¬¸ì¥, ê²€ì¦/íŒë‹¨/ê¸°ì¤€ ìœ„ì£¼
    - LCC: êµ¬ì–´ì²´, í˜„ì¥ ìƒí™© ì—°ìƒ, íŒë‹¨â†’í–‰ë™â†’ê²°ê³¼ ë¹ ë¥´ê²Œ ìš”êµ¬
    """

    # ë²„ì „ì— ë”°ë¼ í†¤ ì„ íƒ: í™€ìˆ˜(1,3,5...)=ë‚ ì¹´ë¡œìš´, ì§ìˆ˜(2,4,6...)=ë¶€ë“œëŸ¬ìš´
    is_soft_version = (version % 2 == 0)
    is_fsc = (atype == "FSC")

    # ========================================
    # ìƒˆ ë°©ì‹: deep_questions ì§ì ‘ ì‚¬ìš© (ìš°ì„ )
    # ========================================
    deep_questions = _llm_extract_for_slot(llm_item, "deep_questions", [])

    # ë””ë²„ê·¸ ë¡œê·¸
    logger.debug(f"[DEBUG Q2] llm_item is None: {llm_item is None}")
    if llm_item:
        logger.debug(f"[DEBUG Q2] llm_item keys: {list(llm_item.keys()) if isinstance(llm_item, dict) else 'not a dict'}")
        logger.debug(f"[DEBUG Q2] deep_questions count: {len(deep_questions) if deep_questions else 0}")

    if deep_questions and isinstance(deep_questions, list) and len(deep_questions) > 0:
        # ë²„ì „ì— ë”°ë¼ ë‹¤ë¥¸ ì§ˆë¬¸ ì„ íƒ
        q_idx = (base + version) % len(deep_questions)
        selected_q = deep_questions[q_idx]

        if isinstance(selected_q, dict):
            q2_raw = selected_q.get("question", "")
            source_sentence = selected_q.get("source_sentence", "")

            # ì§ˆë¬¸ í˜•ì‹ ê²€ì¦ (ê¸ˆì§€ íŒ¨í„´ í•„í„°ë§)
            q2_text = _validate_and_fix_question_format(q2_raw) if q2_raw else None

            # ì§ˆë¬¸ì´ ìœ íš¨í•œì§€ í™•ì¸ (ê²€ì¦ í†µê³¼ + ìµœì†Œ ê¸¸ì´)
            if q2_text and len(q2_text) >= 10:
                logger.debug(f"[DEBUG Q2] Using deep_questions! Q2: {q2_text[:50]}...")

                # FSC/LCC í†¤ ê·œì¹™ ì ìš©
                q2_text = _apply_airline_tone(q2_text, is_fsc, is_soft_version)

                # Q3ìš©ìœ¼ë¡œ expected_answers ì €ì¥
                expected_answers = selected_q.get("expected_answers", [])
                if expected_answers:
                    st.session_state["_q2_expected_answers"] = expected_answers

                return q2_text, source_sentence, source_sentence
            else:
                logger.debug(f"[DEBUG Q2] deep_questions rejected (invalid format): {q2_raw[:50] if q2_raw else 'empty'}...")

    # ========================================
    # í´ë°±: ê¸°ì¡´ ë°©ì‹ (over_idealized_points ë“±)
    # ========================================
    logger.debug("[DEBUG Q2] Falling back to old method...")

    # 1ë‹¨ê³„: LLMì—ì„œ ê³µê²© í¬ì¸íŠ¸ ì¶”ì¶œ
    over_idealized = _llm_extract_for_slot(llm_item, "over_idealized_points", [])
    risk_points = _llm_extract_for_slot(llm_item, "risk_points", [])
    rejected_alts = _llm_extract_for_slot(llm_item, "rejected_alternatives", [])
    claim = _llm_extract_for_slot(llm_item, "claim", "")

    # ìœ íš¨í•œ í¬ì¸íŠ¸ í•„í„°ë§ (ë§¥ë½ ì˜ì¡´ì  ë¬¸ì¥ + ë¶ˆì™„ì „í•œ ì¡°ê° ì œì™¸)
    valid_idealized = [
        p for p in (over_idealized or [])
        if isinstance(p, str) and p.strip()
        and not _is_context_dependent_sentence(p)
        and is_complete_sentence(p)  # ë¶ˆì™„ì „í•œ ì¡°ê° í•„í„°ë§
    ]
    valid_risks = [
        r for r in (risk_points or [])
        if isinstance(r, str) and r.strip()
        and not _is_context_dependent_sentence(r)
        and is_complete_sentence(r)  # ë¶ˆì™„ì „í•œ ì¡°ê° í•„í„°ë§
    ]
    valid_alts = [
        a for a in (rejected_alts or [])
        if isinstance(a, str) and a.strip()
        and not _is_context_dependent_sentence(a)
        and is_complete_sentence(a)  # ë¶ˆì™„ì „í•œ ì¡°ê° í•„í„°ë§
    ]

    # ê³µê²© í¬ì¸íŠ¸ ì„ íƒ - ë²„ì „ë³„ë¡œ ë‹¤ë¥¸ íƒ€ì… ìš°ì„ ìˆœìœ„ ì ìš©í•˜ì—¬ ë‹¤ì–‘ì„± í™•ë³´
    # ë²„ì „ 1, 4: ì´ìƒì  í‘œí˜„ ê³µê²© (over_idealized)
    # ë²„ì „ 2, 5: ì·¨ì•½ì  ê³µê²© (risk_points)
    # ë²„ì „ 3: ëŒ€ì•ˆ ê³µê²© (rejected_alternatives)
    attack_point = ""
    attack_type = "idealized"

    version_mod = version % 3  # 0, 1, 2ë¡œ ìˆœí™˜

    if version_mod == 1:  # ë²„ì „ 1, 4, 7... â†’ ì´ìƒì  í‘œí˜„ ìš°ì„ 
        priority_order = [
            ("idealized", valid_idealized),
            ("risk", valid_risks),
            ("alternative", valid_alts),
        ]
    elif version_mod == 2:  # ë²„ì „ 2, 5, 8... â†’ ì·¨ì•½ì  ìš°ì„ 
        priority_order = [
            ("risk", valid_risks),
            ("idealized", valid_idealized),
            ("alternative", valid_alts),
        ]
    else:  # ë²„ì „ 3, 6, 9... â†’ ëŒ€ì•ˆ ìš°ì„ 
        priority_order = [
            ("alternative", valid_alts),
            ("idealized", valid_idealized),
            ("risk", valid_risks),
        ]

    # ìš°ì„ ìˆœìœ„ì— ë”°ë¼ ê³µê²© í¬ì¸íŠ¸ ì„ íƒ
    for atype, points in priority_order:
        if points:
            # ë²„ì „ê³¼ íƒ€ì…ì— ë”°ë¼ ë‹¤ë¥¸ ì¸ë±ìŠ¤ ì„ íƒ (ë” ë‹¤ì–‘í•˜ê²Œ)
            idx = (base + version * 7 + hash(atype) % 5) % len(points)
            attack_point = normalize_ws(points[idx])
            attack_type = atype
            break

    # 2ë‹¨ê³„: í´ë°± - LLM ì‹¤íŒ¨ ì‹œ ìì†Œì„œì—ì„œ ì´ìƒì  í‘œí˜„ ì§ì ‘ ì¶”ì¶œ
    if not attack_point and raw_answer:
        # í‚¤ì›Œë“œ ê¸°ë°˜ìœ¼ë¡œ ìì†Œì„œ ë¬¸ì¥ ê²€ìƒ‰ (ì™„ì „í•œ ë¬¸ì¥ë§Œ)
        sentences = split_sentences(raw_answer)
        idealistic_keywords = ["í•¨ê»˜", "ê·¹ë³µ", "ì†Œí†µ", "íŒ€ì›Œí¬", "í˜‘ë ¥", "í•´ê²°", "ì„±ì¥", "ë…¸ë ¥", "ë°°ë ¤", "ë”°ëœ»"]
        for sentence in sentences:
            sentence = sentence.strip()
            if len(sentence) < 15:
                continue
            # ì´ìƒì  í‚¤ì›Œë“œ í¬í•¨ ì—¬ë¶€ ì²´í¬
            has_keyword = any(kw in sentence for kw in idealistic_keywords)
            if has_keyword and is_complete_sentence(sentence):
                attack_point = normalize_ws(sentence)[:100]
                attack_type = "idealized"
                break

    # 3ë‹¨ê³„: ìµœì¢… í´ë°± - ìì†Œì„œì—ì„œ ì•„ë¬´ ë¬¸ì¥ì´ë‚˜ ì¶”ì¶œ (ì¼ë°˜ ë¬¸ì¥ ê¸ˆì§€!)
    if not attack_point and raw_answer:
        # ìì†Œì„œë¥¼ ë¬¸ì¥ìœ¼ë¡œ ë¶„ë¦¬
        sentences = split_sentences(raw_answer)
        valid_sentences = [
            s for s in sentences
            if isinstance(s, str) and len(s.strip()) >= 15 and is_complete_sentence(s.strip())
        ]
        if valid_sentences:
            # ë²„ì „ì— ë”°ë¼ ë‹¤ë¥¸ ë¬¸ì¥ ì„ íƒ
            idx = (base + version * 17) % len(valid_sentences)
            attack_point = normalize_ws(valid_sentences[idx])[:100]
            attack_type = "idealized"

    # 4ë‹¨ê³„: ì •ë§ ì•„ë¬´ê²ƒë„ ì—†ìœ¼ë©´ claim ì‚¬ìš© (ì™„ì „í•œ ë¬¸ì¥ì¸ ê²½ìš°ë§Œ)
    if not attack_point and claim and is_complete_sentence(claim):
        attack_point = claim
        attack_type = "idealized"

    # 4.5ë‹¨ê³„: attack_point ìµœì¢… ê²€ì¦ - ìœ íš¨í•˜ì§€ ì•Šìœ¼ë©´ ê¸°ë³¸ ì§ˆë¬¸ ì‚¬ìš©
    if not attack_point or not is_complete_sentence(attack_point):
        # ìœ íš¨í•œ attack_pointê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ ì§ˆë¬¸ ë°˜í™˜
        default_questions = [
            "ìì†Œì„œì— ì ì–´ì£¼ì‹  ê²½í—˜ ì¤‘ ê°€ì¥ ê¸°ì–µì— ë‚¨ëŠ” ê²Œ ìˆìœ¼ì‹ ê°€ìš”?",
            "ë³¸ì¸ì´ ìƒê°í•˜ëŠ” ê°€ì¥ í° ê°•ì ì€ ë¬´ì—‡ì¸ê°€ìš”?",
            "ì´ ê²½í—˜ì„ í†µí•´ ì–»ì€ ê°€ì¥ í° êµí›ˆì´ ìˆë‹¤ë©´ìš”?",
            "ìì†Œì„œì— ì ì–´ì£¼ì‹  ë‚´ìš© ì¤‘ ê°€ì¥ ìì‹ ìˆê²Œ ë§ì”€í•˜ì‹¤ ìˆ˜ ìˆëŠ” ë¶€ë¶„ì€ìš”?",
        ]
        q2_text = default_questions[(base + version) % len(default_questions)]
        return q2_text, claim, ""

    # 5ë‹¨ê³„: ê³µê²© íƒ€ì…ì— ë”°ë¥¸ í…œí”Œë¦¿ ì„ íƒ
    if attack_type == "idealized":
        # ë¬¸ì¥ ìœ í˜• í™•ì¸ (ìš°ì„ ìˆœìœ„: ê¿ˆ/ì†Œë§ > ì¶”ìƒì /ë¹„ìœ  > ì¼ë°˜ ê²½í—˜)
        is_dream = _is_dream_sentence(attack_point)
        is_abstract = _is_abstract_sentence(attack_point)

        # ì „ì œ ì¶”ì¶œ (ëª¨ë“  ì¼€ì´ìŠ¤ì—ì„œ í•„ìš”)
        premise, premise_broken = _extract_premise_from_point(attack_point)

        if is_dream:
            # ê¿ˆ/ì†Œë§ ë¬¸ì¥: "ê²½í—˜"ì´ ì•„ë‹Œ ì ì ˆí•œ ì§ˆë¬¸ ë°©ì‹
            if is_soft_version:
                templates = Q2_DREAM_TEMPLATES_SOFT
            else:
                templates = Q2_DREAM_TEMPLATES
            tpl_idx = (base + version * 19) % len(templates)
            q2_raw = templates[tpl_idx].format(
                point=attack_point,
                premise=premise,
                premise_broken=premise_broken
            )
        elif is_abstract:
            # ì¶”ìƒì  ë¬¸ì¥: "ì™œ ê·¸ë ‡ê²Œ í‘œí˜„í•˜ì…¨ë‚˜ìš”?" ìŠ¤íƒ€ì¼
            # í•µì‹¬ë§Œ ì¶”ì¶œí•˜ì—¬ ì§§ê²Œ
            short_point = _extract_short_point(attack_point)

            if is_soft_version:
                templates = Q2_ABSTRACT_TEMPLATES_SOFT
            else:
                templates = Q2_ABSTRACT_TEMPLATES
            tpl_idx = (base + version * 19) % len(templates)
            q2_raw = templates[tpl_idx].format(
                point=attack_point,
                short_point=short_point
            )
        else:
            # ì¼ë°˜ ì´ìƒì  í‘œí˜„ (ê²½í—˜ ê¸°ë°˜): ì „ì œ ê³µê²© ìŠ¤íƒ€ì¼
            # êµ¬ì²´ì  íŒë‹¨ ì¶”ì¶œ
            judgment = _extract_judgment_from_point(attack_point)
            # ìƒëŒ€ë°©ì´ ëˆ„êµ¬ì¸ì§€ ì¶”ì¶œ
            who = _extract_who_from_context(attack_point, raw_answer)

            if is_soft_version:
                templates = Q2_ATTACK_TEMPLATES_SOFT
            else:
                templates = Q2_ATTACK_TEMPLATES
            tpl_idx = (base + version * 19) % len(templates)
            q2_raw = templates[tpl_idx].format(
                point=attack_point,
                premise=premise,
                premise_broken=premise_broken,
                judgment=judgment,
                who=who
            )

    elif attack_type == "risk":
        if is_soft_version:
            templates = Q2_RISK_TEMPLATES_SOFT
        else:
            templates = Q2_RISK_TEMPLATES
        tpl_idx = (base + version * 19) % len(templates)
        q2_raw = templates[tpl_idx].format(risk=attack_point)

    else:  # alternative
        if is_soft_version:
            templates = Q2_ALTERNATIVE_TEMPLATES_SOFT
        else:
            templates = Q2_ALTERNATIVE_TEMPLATES
        tpl_idx = (base + version * 19) % len(templates)
        q2_raw = templates[tpl_idx].format(alt=attack_point)

    # ì¡°ì‚¬(ì„/ë¥¼, ì´/ê°€ ë“±) ë³´ì • í›„ ì •ë¦¬
    q2_text = _sanitize_question_strict(_fix_particles_after_format(q2_raw))

    # FSC/LCC í†¤ ê·œì¹™ ì ìš©
    q2_text = _apply_airline_tone(q2_text, is_fsc, is_soft_version)

    # Q3ìš©ìœ¼ë¡œ claimê³¼ attack_point ë°˜í™˜
    return q2_text, claim, attack_point


def _generate_predicted_answer(attack_point: str, is_soft: bool) -> str:
    """Q2ì— ëŒ€í•œ ì˜ˆìƒ ë‹µë³€ì„ ìƒì„± (ê°•ìš” X, ì¶”ì¸¡ë§Œ)"""
    # í‚¤ì›Œë“œ ê¸°ë°˜ ì˜ˆìƒ ë‹µë³€ ë§¤í•‘ (ë” ë§ì€ í‚¤ì›Œë“œ)
    prediction_map = [
        (["í•¨ê»˜", "ê°™ì´", "í˜‘ë ¥", "í˜‘ë™"], "ê·¸ë˜ë„ í•¨ê»˜ í•´ê²°í•˜ë ¤ ë…¸ë ¥í•˜ê² ë‹¤"),
        (["ê·¹ë³µ", "ì´ê²¨", "í•´ê²°", "ë„˜"], "ë‹¤ë¥¸ ë°©ë²•ì„ ì°¾ì•„ë³´ê² ë‹¤"),
        (["ì†Œí†µ", "ëŒ€í™”", "ì»¤ë®¤ë‹ˆì¼€ì´ì…˜"], "ë” ì ê·¹ì ìœ¼ë¡œ ì†Œí†µí•˜ê² ë‹¤"),
        (["íŒ€ì›Œí¬", "íŒ€", "ê³µë™ì²´"], "íŒ€ì„ ìœ„í•´ ì–‘ë³´í•˜ê² ë‹¤"),
        (["ë¯¸ì†Œ", "ì¹œì ˆ", "ë°°ë ¤", "ë”°ëœ»", "ë°˜ê²¨"], "ê·¸ë˜ë„ ì¹œì ˆì„ ìœ ì§€í•˜ê² ë‹¤"),
        (["ì„±ì¥", "ë°œì „", "ì„±ê³µ"], "í¬ê¸°í•˜ì§€ ì•Šê² ë‹¤"),
        (["ë„ì „", "ìƒˆë¡œìš´", "ì‹œë„"], "ì‹¤íŒ¨í•´ë„ ë°°ì›€ì´ ìˆë‹¤"),
        (["ìµœì„ ", "ë…¸ë ¥", "ì—´ì‹¬"], "ë” ì—´ì‹¬íˆ í•˜ê² ë‹¤"),
        (["ì˜ì§€", "ì§€ì§€", "ì‘ì›", "ë„ì›€"], "ìŠ¤ìŠ¤ë¡œ ê·¹ë³µí•˜ë ¤ ë…¸ë ¥í•˜ê² ë‹¤"),
        (["ì™¸ë¡œ", "íƒ€ì§€", "ë‚¯ì„ "], "í˜¼ìì„œë„ ë²„í…¨ë³´ê² ë‹¤"),
        (["ê¿ˆ", "ëª©í‘œ", "ë˜ê³  ì‹¶"], "ê¿ˆì„ í¬ê¸°í•˜ì§€ ì•Šê² ë‹¤"),
        (["ì²«", "ì²˜ìŒ", "ì‹œì‘"], "ë‹¤ì‹œ ì‹œì‘í•˜ê² ë‹¤"),
        (["ê°ë™", "ê°ì‚¬", "ê³ ë§ˆ"], "ê°ì‚¬í•˜ëŠ” ë§ˆìŒì„ ìƒì§€ ì•Šê² ë‹¤"),
        (["ê¸°ì–µ", "ìŠì§€", "ë§ˆìŒ"], "ê·¸ ê¸°ì–µì„ ì†Œì¤‘íˆ ê°„ì§í•˜ê² ë‹¤"),
        (["ì‹ ë¢°", "ë¯¿ìŒ"], "ì‹ ë¢°ë¥¼ íšŒë³µí•˜ë ¤ ë…¸ë ¥í•˜ê² ë‹¤"),
        (["ì•ˆì •", "í‰í™”", "ì•ˆì „"], "ì¹¨ì°©í•˜ê²Œ ëŒ€ì‘í•˜ê² ë‹¤"),
        (["ê³ ê°", "ì„œë¹„ìŠ¤", "ì†ë‹˜"], "ê³ ê° ì…ì¥ì—ì„œ ìƒê°í•˜ê² ë‹¤"),
    ]

    point_lower = attack_point.lower() if attack_point else ""
    for keywords, prediction in prediction_map:
        for kw in keywords:
            if kw in point_lower:
                return prediction

    return "ìƒí™©ì— ë§ê²Œ ëŒ€ì²˜í•˜ê² ë‹¤"


def _slot_q3_followup(
    base: int,
    version: int,
    qtype: str,
    llm_item: Optional[Dict[str, Any]],
    claim: str,
    attack_point: str,
    atype: str = "LCC",
) -> str:
    """Q3: ê¼¬ë¦¬ ì§ˆë¬¸ (ìƒˆ í”„ë¡¬í”„íŠ¸ ê¸°ë°˜ - expected_answers ìš°ì„  ì‚¬ìš©)

    í•µì‹¬ ì›ì¹™:
    - ë‹µë³€ ê°•ìš” X, ì‚¬ê³  í™•ì¥ O
    - Q2ì˜ ì—°ì¥ì„  (ìƒˆë¡œìš´ ì£¼ì œ ë„ì… ê¸ˆì§€)
    - "ë§Œì•½", "ê°€ëŠ¥í• ê¹Œìš”" ê°™ì€ ì—´ë¦° í‘œí˜„ ì‚¬ìš©

    FSC/LCC í†¤ ê·œì¹™ë„ ì ìš©
    """
    is_soft_version = (version % 2 == 0)
    is_fsc = (atype == "FSC")

    # ========================================
    # 1ìˆœìœ„: FLYREADY ì—”ì§„ ê¼¬ë¦¬ì§ˆë¬¸ ì‚¬ìš© (ì‹ ê·œ)
    # ========================================
    flyready_followup = st.session_state.get("_flyready_followup", None)

    if flyready_followup and len(flyready_followup) >= 5:
        logger.debug(f"[DEBUG Q3] Using FLYREADY followup! Q3: {flyready_followup[:50]}...")
        # FSC/LCC í†¤ ê·œì¹™ ì ìš©
        q3_text = _apply_airline_tone(flyready_followup, is_fsc, is_soft_version)
        # ì‚¬ìš© í›„ ì´ˆê¸°í™” (ë‹¤ìŒ ë²„ì „ì—ì„œ ë‹¤ë¥¸ ì§ˆë¬¸ ì„ íƒë˜ë„ë¡)
        st.session_state["_flyready_followup"] = None
        return q3_text

    # ========================================
    # 2ìˆœìœ„: expected_answersì—ì„œ ê¼¬ë¦¬ì§ˆë¬¸ ì‚¬ìš© (ê¸°ì¡´)
    # ========================================
    expected_answers = st.session_state.get("_q2_expected_answers", None)

    if expected_answers and isinstance(expected_answers, list) and len(expected_answers) > 0:
        # ë²„ì „ì— ë”°ë¼ ë‹¤ë¥¸ ì˜ˆìƒë‹µë³€/ê¼¬ë¦¬ì§ˆë¬¸ ì„ íƒ
        ans_idx = (base + version) % len(expected_answers)
        selected_ans = expected_answers[ans_idx]

        if isinstance(selected_ans, dict):
            followup = selected_ans.get("followup", "")

            if followup and len(followup) >= 5:
                logger.debug(f"[DEBUG Q3] Using expected_answers! Q3: {followup[:50]}...")
                # FSC/LCC í†¤ ê·œì¹™ ì ìš©
                q3_text = _apply_airline_tone(followup, is_fsc, is_soft_version)
                return q3_text

    # ========================================
    # 3ìˆœìœ„: í´ë°± - ê¸°ì¡´ í…œí”Œë¦¿ ë°©ì‹
    # ========================================
    logger.debug("[DEBUG Q3] Falling back to template method...")

    # ì „ì œ ì •ë³´ ì¶”ì¶œ (Q2ì™€ ë™ì¼í•œ ë§¥ë½ ìœ ì§€)
    premise, premise_broken = _extract_premise_from_point(attack_point)

    # Q2ê°€ ê¿ˆ/ëª©í‘œ ë¬¸ì¥ì¸ì§€ í™•ì¸
    is_dream = _is_dream_sentence(attack_point)

    # 4ê°€ì§€ Q3 ìœ í˜•ì„ ê²°ì •ë¡ ì ìœ¼ë¡œ ì„ íƒ (version ê¸°ë°˜)
    # 0: ì¡°ê±´ ë³€í™”í˜•, 1: ìš°ì„ ìˆœìœ„ ì¶©ëŒí˜•, 2: í•œê³„ ì¸ì‹í˜•, 3: ì¬í˜„ì„± ê²€ì¦í˜•
    q3_type = (base + version) % 4

    if is_dream:
        # ê¿ˆ/ëª©í‘œ ë¬¸ì¥ìš© Q3 í…œí”Œë¦¿ ì‚¬ìš©
        if q3_type == 0:
            templates = Q3_DREAM_CONDITION
            tpl_idx = (base + version * 11) % len(templates)
            q3_text = templates[tpl_idx].format(premise_broken=premise_broken)
        elif q3_type == 1:
            templates = Q3_DREAM_PRIORITY
            tpl_idx = (base + version * 13) % len(templates)
            q3_text = templates[tpl_idx]
        elif q3_type == 2:
            templates = Q3_DREAM_LIMIT
            tpl_idx = (base + version * 17) % len(templates)
            q3_text = templates[tpl_idx]
        else:
            templates = Q3_DREAM_REPEAT
            tpl_idx = (base + version * 19) % len(templates)
            q3_text = templates[tpl_idx]
    else:
        # ì¼ë°˜ ê²½í—˜ ë¬¸ì¥ìš© Q3 í…œí”Œë¦¿ ì‚¬ìš©
        if q3_type == 0:
            # â‘  ì¡°ê±´ ë³€í™”í˜•: "ë§Œì•½ â—‹â—‹ ì¡°ê±´ì´ ë‹¬ë¼ì§„ë‹¤ë©´?"
            templates = Q3_CONDITION_CHANGE
            tpl_idx = (base + version * 11) % len(templates)
            q3_text = templates[tpl_idx].format(premise_broken=premise_broken)
        elif q3_type == 1:
            # â‘¡ ìš°ì„ ìˆœìœ„ ì¶©ëŒí˜•: "ë‘˜ ë‹¤ ì¤‘ìš”í•  ë•Œ ë¬´ì—‡ì„ ë¨¼ì € ì„ íƒí•˜ëŠ”ê°€"
            templates = Q3_PRIORITY_CONFLICT
            tpl_idx = (base + version * 13) % len(templates)
            q3_text = templates[tpl_idx]
        elif q3_type == 2:
            # â‘¢ í•œê³„ ì¸ì‹í˜•: "ê·¸ ì„ íƒì˜ ì•½ì ì€ ë¬´ì—‡ì¸ê°€"
            templates = Q3_LIMIT_RECOGNITION
            tpl_idx = (base + version * 17) % len(templates)
            q3_text = templates[tpl_idx]
        else:
            # â‘£ ì¬í˜„ì„± ê²€ì¦í˜•: "ë‹¤ìŒì—ë„ ê°€ëŠ¥í•œê°€"
            templates = Q3_REPEATABILITY
            tpl_idx = (base + version * 19) % len(templates)
            q3_text = templates[tpl_idx]

    # ì¡°ì‚¬(ì„/ë¥¼, ì´/ê°€ ë“±) ë³´ì • í›„ ì •ë¦¬
    q3_text = _sanitize_question_strict(_fix_particles_after_format(q3_text))

    # FSC/LCC í†¤ ê·œì¹™ ì ìš©
    q3_text = _apply_airline_tone(q3_text, is_fsc, is_soft_version)

    return q3_text


def _slot_q5_surprise(base: int, version: int, atype: str = "LCC") -> str:
    """Q5: FLYREADY ëŒë°œì§ˆë¬¸ 100ê°œ + ê¸°ì¡´ í’€ì—ì„œ ì„ íƒ (ê²½í—˜í˜• ê¸ˆì§€)

    ë²„ì „ì— ë”°ë¼ í†¤ ì„ íƒ: í™€ìˆ˜(1,3,5)=ë‚ ì¹´ë¡œìš´, ì§ìˆ˜(2,4,6)=ë¶€ë“œëŸ¬ìš´
    FLYREADY ë°ì´í„°: 05_ëŒë°œí™•ì¥ì§ˆë¬¸_100.json (7ê°œ ì¹´í…Œê³ ë¦¬)
    """
    # ë²„ì „ì— ë”°ë¼ í†¤ ì„ íƒ
    is_soft_version = (version % 2 == 0)

    # FLYREADY ëŒë°œì§ˆë¬¸ 100ê°œ ë¡œë“œ
    flyready_surprise = get_all_surprise_questions()

    # í•­ê³µì‚¬ ìœ í˜•ì— ë§ëŠ” ê¸°ì¡´ ì§ˆë¬¸ í’€ ì„ íƒ (ë²„ì „ë³„ í†¤ ì ìš©)
    if atype == "FSC":
        if is_soft_version:
            legacy_pool = Q5_POOL_COMMON_SOFT + Q5_POOL_FSC_SOFT
        else:
            legacy_pool = Q5_POOL_COMMON_SHARP + Q5_POOL_FSC_SHARP
    elif atype == "HSC":
        # HSCëŠ” ì•„ì§ SHARP/SOFT ë¶„ë¦¬ ì•ˆ ë¨ - ê¸°ì¡´ í’€ ì‚¬ìš©í•˜ë˜ ë²„ì „ ë°˜ì˜
        if is_soft_version:
            legacy_pool = Q5_POOL_COMMON_SOFT + Q5_POOL_HSC
        else:
            legacy_pool = Q5_POOL_COMMON_SHARP + Q5_POOL_HSC
    else:  # LCC
        if is_soft_version:
            legacy_pool = Q5_POOL_COMMON_SOFT + Q5_POOL_LCC_SOFT
        else:
            legacy_pool = Q5_POOL_COMMON_SHARP + Q5_POOL_LCC_SHARP

    # FLYREADY ì§ˆë¬¸ + ê¸°ì¡´ í’€ í•©ì¹˜ê¸° (FLYREADY ìš°ì„ )
    pool = flyready_surprise + legacy_pool

    if not pool:
        return "ì••ë°•ì´ í° ìƒí™©ì—ì„œ ì„¤ëª…ì„ ì§§ê²Œ ì •ë¦¬í•´ì•¼ í•  ë•Œ, ì–´ë–¤ ìˆœì„œë¡œ ë§í•˜ê³  ì–´ë–¤ í–‰ë™ë¶€í„° ì‹¤í–‰í•˜ê² ìŠµë‹ˆê¹Œ?"

    # ë²„ì „ì— ë”°ë¼ ë‹¤ë¥¸ ì§ˆë¬¸ ì„ íƒ (ë§¤ë²ˆ ë‹¤ë¥¸ ì§ˆë¬¸ ë‚˜ì˜¤ë„ë¡)
    idx = (base + version * 17 + 5 * 101) % len(pool)
    q5_raw = pool[idx]

    for forbidden in Q5_FORBIDDEN_WORDS:
        if forbidden in q5_raw:
            for fallback_idx in range(len(pool)):
                alt_idx = (idx + fallback_idx + 1) % len(pool)
                alt = pool[alt_idx]
                if not any(f in alt for f in Q5_FORBIDDEN_WORDS):
                    q5_raw = alt
                    break
            break

    # Q5ëŠ” í’€ì—ì„œ ê°€ì ¸ì˜¨ ì§ˆë¬¸ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš© (ë³€í˜• ê¸ˆì§€)
    return normalize_ws(q5_raw)


def _pick_det_idx(base: int, version: int, slot_seed: int, n: int) -> int:
    if n <= 0:
        return 0
    return (base + version * 17 + slot_seed * 101) % n


def _version_angle(version: int) -> int:
    if version <= 0:
        return 1
    return ((version - 1) % 5) + 1


def _pick_item_index(base: int, version: int, n_items: int) -> int:
    if n_items <= 0:
        return 0
    return (base + version * 7) % n_items


def _pick_basis_ab(base: int, version: int) -> int:
    return (base + version * 11) % 2


def _normalize_airline_name_in_text(text: str, airline: str) -> str:
    t = text
    raw = _raw_airline_key(airline)
    if raw == "ì œì£¼":
        t = t.replace("ì œì£¼í•­ê³µ", "ì œì£¼í•­ê³µ").replace("ì œì£¼", "ì œì£¼í•­ê³µ")
    elif raw == "ì•„ì‹œì•„ë‚˜":
        t = t.replace("ì•„ì‹œì•„ë‚˜í•­ê³µ", "ì•„ì‹œì•„ë‚˜í•­ê³µ").replace("ì•„ì‹œì•„ë‚˜", "ì•„ì‹œì•„ë‚˜í•­ê³µ")
    elif raw == "í‹°ì›¨ì´":
        t = t.replace("í‹°ì›¨ì´í•­ê³µ", "í‹°ì›¨ì´í•­ê³µ").replace("í‹°ì›¨ì´", "í‹°ì›¨ì´í•­ê³µ")
    elif raw == "ì´ìŠ¤íƒ€":
        t = t.replace("ì´ìŠ¤íƒ€í•­ê³µ", "ì´ìŠ¤íƒ€í•­ê³µ").replace("ì´ìŠ¤íƒ€", "ì´ìŠ¤íƒ€í•­ê³µ")
    return t


def _fallback_questions_fixed_slots_item(
    base: int,
    version: int,
    airline: str,
    atype: str,
    item: Dict[str, Any]
) -> Dict[str, Dict[str, str]]:
    """í´ë°± ì§ˆë¬¸ ìƒì„± (ê³µê²© í¬ì¸íŠ¸ ê¸°ë°˜ - ìƒˆ ë°©ì‹)"""
    logger.debug("[DEBUG FALLBACK] _fallback_questions_fixed_slots_item í˜¸ì¶œë¨!")
    qtype = item.get("qtype", "ê²½í—˜ ìš”êµ¬í˜•")
    situation = item.get("situation", "í˜„ì¥ì—ì„œ ë³€ìˆ˜ê°€ ë°œìƒí•œ ìƒí™©")
    raw_answer = item.get("answer", "")
    basisA = (item.get("basisA", {}) or {}).get("text", "")
    basisB = (item.get("basisB", {}) or {}).get("text", "")
    pick_ab = _pick_basis_ab(base, version)
    basis = basisA if pick_ab == 0 else basisB
    if not basis:
        basis = raw_answer[:80] if raw_answer else "ìê¸°ì†Œê°œì„œ ë‹µë³€ì—ì„œ ì–¸ê¸‰í•œ ë‚´ìš©"

    # Q1: ê³µí†µ ì§ˆë¬¸
    q1 = _slot_q1_common(base, version, atype)
    is_soft_version = (version % 2 == 0)
    qtype = item.get("qtype", "ê²½í—˜ ìš”êµ¬í˜•")
    situation = item.get("situation", "")
    raw_answer = item.get("answer", "")

    # Q2: FLYREADY ìŠ¤íƒ€ì¼ ë‚ ì¹´ë¡œìš´ ì§ˆë¬¸ (í´ë°±ìš©)
    # ê·œì¹™: "~ì…ë‹ˆê¹Œ?", "~ì—†ìŠµë‹ˆê¹Œ?", "~ë­¡ë‹ˆê¹Œ?" ì–´ë¯¸ë§Œ ì‚¬ìš©
    # ì£¼ì˜: "ê·¸ ê²°ì •", "ê·¸ ê²½í—˜" ê°™ì€ ë§‰ì—°í•œ í‘œí˜„ ê¸ˆì§€ - êµ¬ì²´ì  ë§¥ë½ í•„ìˆ˜
    Q2_FLYREADY_FALLBACK = [
        # ì—­ë°©í–¥ ê³µê²© (ê°€ì¹˜ì„ ì–¸ â†’ ì†í•´ ê²€ì¦)
        "íŒ€ì›Œí¬ê°€ ì¤‘ìš”í•˜ë‹¤ê³  í•˜ì…¨ëŠ”ë°, íŒ€ì›Œí¬ ë•Œë¬¸ì— ì†í•´ ë³¸ ì ì€ ì—†ìŠµë‹ˆê¹Œ?",
        "ë°°ë ¤ê°€ ì¤‘ìš”í•˜ë‹¤ê³  í•˜ì…¨ëŠ”ë°, ë°°ë ¤ ë•Œë¬¸ì— ë³¸ì¸ì´ ì†í•´ ë³¸ ì ì€ ì—†ìŠµë‹ˆê¹Œ?",
        "ì†Œí†µì„ ê°•ì¡°í•˜ì…¨ëŠ”ë°, ì†Œí†µì´ ì•ˆ í†µí•œ ì ì€ ì—†ì—ˆìŠµë‹ˆê¹Œ?",
        # ìˆ˜í˜œì ê²€ì¦ (ë„ì›€ë°›ìŒ â†’ ê¸°ì—¬ ê²€ì¦)
        "íŒ€ì›ë“¤ ë•ë¶„ì— ì„±ê³¼ë¥¼ ëƒˆë‹¤ê³  í•˜ì…¨ëŠ”ë°, ë³¸ì¸ì´ ê¸°ì—¬í•œ ê±´ êµ¬ì²´ì ìœ¼ë¡œ ë­¡ë‹ˆê¹Œ?",
        "ì„ ë°° ë„ì›€ìœ¼ë¡œ ì„±ì¥í–ˆë‹¤ê³  í•˜ì…¨ëŠ”ë°, ë³¸ì¸ í˜¼ì í•´ë‚¸ ë¶€ë¶„ì€ ë­¡ë‹ˆê¹Œ?",
        "í˜‘ë ¥ìœ¼ë¡œ í•´ê²°í–ˆë‹¤ê³  í•˜ì…¨ëŠ”ë°, ë³¸ì¸ ì—­í• ì€ ì •í™•íˆ ëª‡ í¼ì„¼íŠ¸ì…ë‹ˆê¹Œ?",
        # ì„±ê³¼ ê²€ì¦ (ì¶”ìƒì  ì„±ê³¼ â†’ ìˆ«ì ìš”êµ¬)
        "ì„±ê³¼ë¥¼ ëƒˆë‹¤ê³  í•˜ì…¨ëŠ”ë°, ìˆ«ìë¡œ ë§í•˜ë©´ êµ¬ì²´ì ìœ¼ë¡œ ì–¼ë§ˆì…ë‹ˆê¹Œ?",
        "ì„±ê³µí–ˆë‹¤ê³  í•˜ì…¨ëŠ”ë°, ì„±ê³µì´ë¼ê³  íŒë‹¨í•œ ê¸°ì¤€ì´ ë­¡ë‹ˆê¹Œ?",
        "ì¢‹ì€ ê²°ê³¼ë¥¼ ì–»ì—ˆë‹¤ê³  í•˜ì…¨ëŠ”ë°, ì‹¤íŒ¨í•œ ë¶€ë¶„ì€ ì—†ì—ˆìŠµë‹ˆê¹Œ?",
        # ì„ íƒ ê²€ì¦ (ì£¼ì²´ì„± í™•ì¸)
        "ìœ í•™ ê²°ì •ì— ëŒ€í•œ í›„íšŒëŠ” ì—†ì—ˆìŠµë‹ˆê¹Œ?",
        "ì§„ë¡œ ì„ íƒí•  ë•Œ ë‹¤ë¥¸ ì„ íƒì§€ëŠ” ê³ ë ¤í•˜ì§€ ì•Šì•˜ìŠµë‹ˆê¹Œ?",
        "ìŠ¹ë¬´ì›ì´ë¼ëŠ” ì§ì—… ì„ íƒì„ í›„íšŒí•œ ì ì€ ì—†ìŠµë‹ˆê¹Œ?",
        # ì§ë¬´ì—°ê²° ì§ˆë¬¸ (CLOVA ë¶„ì„ ì‹¤íŒ¨ ì‹œ ì‚¬ìš©)
        "ë§ì”€í•˜ì‹  ê²½í—˜ì´ ì‹¤ì œ ë¹„í–‰ ì¤‘ ëŒë°œìƒí™©ì—ì„œ ì–´ë–¤ ì‹ìœ¼ë¡œ ì ìš©ë  ìˆ˜ ìˆìŠµë‹ˆê¹Œ?",
        "ê¸°ë‚´ì—ì„œ ë¹„ìŠ·í•œ ìƒí™©ì´ ë°œìƒí•˜ë©´ ê°™ì€ ë°©ì‹ìœ¼ë¡œ í•´ê²°í•  ìˆ˜ ìˆìŠµë‹ˆê¹Œ?",
        "ìŠ¹ë¬´ì› íŒ€ì›Œí¬ ìƒí™©ì—ì„œ ë³¸ì¸ì˜ ì—­í• ì€ ì£¼ë¡œ ë¬´ì—‡ì´ ë˜ê² ìŠµë‹ˆê¹Œ?",
        "ì•ˆì „ê³¼ ì„œë¹„ìŠ¤ê°€ ì¶©ëŒí•˜ëŠ” ìƒí™©ì—ì„œ ìš°ì„ ìˆœìœ„ë¥¼ ì •í•˜ëŠ” ê¸°ì¤€ì´ ë­¡ë‹ˆê¹Œ?",
    ]

    # Q3: FLYREADY ìŠ¤íƒ€ì¼ ê¼¬ë¦¬ì§ˆë¬¸
    Q3_FLYREADY_TEMPLATES = [
        "ê·¸ë˜ì„œ ê²°ê³¼ëŠ” êµ¬ì²´ì ìœ¼ë¡œ ë­¡ë‹ˆê¹Œ?",
        "ë‹¤ë¥¸ ë°©ë²•ì€ ê³ ë ¤í•˜ì§€ ì•Šì•˜ìŠµë‹ˆê¹Œ?",
        "ê°™ì€ ìƒí™©ì´ ì™€ë„ ê°™ì€ ì„ íƒì„ í•˜ê² ìŠµë‹ˆê¹Œ?",
        "ê·¸ ê²½í—˜ì´ ìŠ¹ë¬´ì› ì—…ë¬´ì— ë„ì›€ì´ ë©ë‹ˆê¹Œ?",
        "ê·¸ë ‡ê²Œ ë§ì”€í•˜ì‹  ê·¼ê±°ê°€ ë­¡ë‹ˆê¹Œ?",
        "ì‹¤íŒ¨í•œ ë¶€ë¶„ì€ ì—†ì—ˆìŠµë‹ˆê¹Œ?",
    ]

    # FLYREADY í´ë°± ì‚¬ìš©
    q2 = Q2_FLYREADY_FALLBACK[(base + version) % len(Q2_FLYREADY_FALLBACK)]
    q3 = Q3_FLYREADY_TEMPLATES[(base + version * 3) % len(Q3_FLYREADY_TEMPLATES)]

    # í•­ê³µì‚¬ ìœ í˜•ì— ë§ëŠ” ê°€ì¹˜ ë°ì´í„° ì„ íƒ (FSC/LCC/HSC)
    if atype == "FSC":
        value_data = FSC_VALUE_DATA
    elif atype == "HSC":
        value_data = HSC_VALUE_DATA
    else:
        value_data = LCC_VALUE_DATA

    # í†µí•© LCC í•­ê³µì‚¬(ì§„ì—ì–´/ì—ì–´ë¶€ì‚°/ì—ì–´ì„œìš¸)ëŠ” ì¼ë¶€ ë²„ì „ì—ì„œ í†µí•© ê´€ë ¨ ì§ˆë¬¸ ì‚¬ìš©
    airline_key = _raw_airline_key(airline)
    use_integration_q = (airline_key in INTEGRATED_LCC_AIRLINES) and (version % 3 == 0)

    # FLYREADY í•­ê³µì‚¬ë³„ ì§ˆë¬¸ ì‚¬ìš© ì—¬ë¶€ (ë²„ì „ 2, 4, 6ì—ì„œ ì‚¬ìš©)
    flyready_airline_questions = get_airline_question_list(airline)
    use_flyready_airline_q = flyready_airline_questions and (version % 2 == 0) and not use_integration_q

    if use_integration_q:
        # í†µí•© LCC ì§ˆë¬¸ ì‚¬ìš©
        q4_idx = (base + version) % len(INTEGRATED_LCC_Q_TEMPLATES)
        q4_text = INTEGRATED_LCC_Q_TEMPLATES[q4_idx]
        kw1, kw2 = "í†µí•©", "ë³€í™”"
        desc = "í†µí•© LCC ì²´ì œì—ì„œì˜ ì ì‘ë ¥ í™•ì¸"
        airline_disp = _canonical_airline_name(airline)
    elif use_flyready_airline_q:
        # FLYREADY í•­ê³µì‚¬ë³„ ì§ˆë¬¸ ì‚¬ìš© (04_í•­ê³µì‚¬ë³„_í•µì‹¬ì§ˆë¬¸_660.json)
        q4_idx = (base + version * 11) % len(flyready_airline_questions)
        q4_text = flyready_airline_questions[q4_idx]
        kw = value_data.get("keywords", []) or VALUES_DEFAULT.get(atype, VALUES_DEFAULT["LCC"])
        kw1 = kw[(base + version) % len(kw)]
        kw2 = kw[(base + version * 3 + 1) % len(kw)]
        if kw2 == kw1 and len(kw) > 1:
            kw2 = kw[(base + version * 3 + 2) % len(kw)]
        desc = _auto_fix_particles_kor(_sanity_kor_endings(_strip_ellipsis_tokens(value_data.get("desc", ""))))
        airline_disp = _canonical_airline_name(airline)
    else:
        # ê¸°ì¡´ ì¸ì¬ìƒ ì§ˆë¬¸ ì‚¬ìš© (ë²„ì „ë³„ í†¤ ì ìš©)
        # HSCëŠ” LCC í…œí”Œë¦¿ ì‚¬ìš© (ì¥ê±°ë¦¬ íŠ¹í™”ì´ì§€ë§Œ ì €ë¹„ìš© êµ¬ì¡° ê¸°ë°˜)
        # í™€ìˆ˜ ë²„ì „(1,3,5)=ë‚ ì¹´ë¡œì›€, ì§ìˆ˜ ë²„ì „(2,4,6)=ë¶€ë“œëŸ¬ì›€
        is_soft_version = (version % 2 == 0)
        if atype == "FSC":
            value_tpls = VALUE_Q_TEMPLATES_FSC_SOFT if is_soft_version else VALUE_Q_TEMPLATES_FSC_SHARP
        else:
            value_tpls = VALUE_Q_TEMPLATES_LCC_SOFT if is_soft_version else VALUE_Q_TEMPLATES_LCC_SHARP
        q4_tpl_idx = _pick_det_idx(base, version, 4, len(value_tpls))
        kw = value_data.get("keywords", []) or []
        desc = _auto_fix_particles_kor(_sanity_kor_endings(_strip_ellipsis_tokens(value_data.get("desc", ""))))
        if not kw:
            kw = VALUES_DEFAULT.get(atype, VALUES_DEFAULT["LCC"])
        kw1 = kw[(base + version) % len(kw)]
        kw2 = kw[(base + version * 3 + 1) % len(kw)]
        if kw2 == kw1 and len(kw) > 1:
            kw2 = kw[(base + version * 3 + 2) % len(kw)]
        airline_disp = _canonical_airline_name(airline)
        q4_text = value_tpls[q4_tpl_idx].format(airline=airline_disp, kw1=kw1, kw2=kw2, desc=desc)
        q4_text = _normalize_airline_name_in_text(q4_text, airline)

    q4_text = _fix_particles_after_format(q4_text)  # ì¡°ì‚¬ ë³´ì • ì¶”ê°€
    q4 = _sanitize_question_strict(q4_text)

    q5 = _slot_q5_surprise(base, version, atype)

    anchor = _pick_anchor_by_rule(
        qtype_internal=qtype,
        action_sents=item.get("action_sents", []) or [],
        result_sents=item.get("result_sents", []) or [],
    )

    basis_summary = _trim_no_ellipsis(f"ë¬¸í•­ {item.get('index', 1)} ìœ í˜•({qtype}) / ê·¼ê±° í›„ë³´: {basis}", 180)
    intent2 = "ë¬¸í•­ ì§ˆë¬¸ê³¼ ë‹µë³€ì˜ ìŒì„ ê·¼ê±°ë¡œ, í‰ê°€ìê°€ ë³´ëŠ” í¬ì¸íŠ¸ë¥¼ ë‹¤ë¥¸ ê°ë„ì—ì„œ ê²€ì¦í•˜ê¸° ìœ„í•œ ì§ˆë¬¸ì…ë‹ˆë‹¤."
    intent3 = "2ë²ˆ ì§ˆë¬¸ê³¼ ë™ì¼ ë¬¸í•­ì˜ í‰ê°€ ì£¼ì œë¥¼ ìœ ì§€í•˜ë©´ì„œ, ë‹µë³€ì„ ê°€ì •í•˜ì§€ ì•Šê³  ë…ë¦½ì ìœ¼ë¡œ ê²€ì¦í•˜ê¸° ìœ„í•œ ì§ˆë¬¸ì…ë‹ˆë‹¤."

    return {
        "q1": {
            "type": "ê³µí†µ ì§ˆë¬¸",
            "question": q1,
            "basis": build_basis_text(summary=q1, intent="ê¸°ë³¸ ì—­ëŸ‰ê³¼ í˜„ì¥ ëŒ€ì‘ì„ í™•ì¸í•˜ê¸° ìœ„í•œ ê³µí†µ ì§ˆë¬¸ì…ë‹ˆë‹¤."),
            "anchor": "",
        },
        "q2": {
            "type": "ì§ë¬´ì—°ê²° ì§ˆë¬¸" if st.session_state.get("_is_job_connection_q2", False) else "ì‹¬ì¸µ(ìì†Œì„œ ê¸°ë°˜)",
            "question": q2,
            "basis": build_basis_text(summary=basis_summary, intent=intent2),
            "anchor": fmt_anchor_text(anchor),
        },
        "q3": {
            "type": "ì§ë¬´ì—°ê²° ê¼¬ë¦¬" if st.session_state.get("_is_job_connection_q2", False) else "ê¼¬ë¦¬ ì§ˆë¬¸",
            "question": q3,
            "basis": build_basis_text(summary=_trim_no_ellipsis(f"ë¬¸í•­ {item.get('index', 1)} í‰ê°€ ì£¼ì œ ìœ ì§€ / ìƒí™©: {situation}", 180), intent=intent3),
            "anchor": fmt_anchor_text(anchor),
        },
        "q4": {
            "type": "ì¸ì¬ìƒ/ê°€ì¹˜",
            "question": q4,
            "basis": build_basis_text(
                summary=f"[{airline_disp}] í•µì‹¬ê°€ì¹˜: {kw1}, {kw2}",
                intent=f"í•­ê³µì‚¬ ì¸ì¬ìƒê³¼ì˜ ì •í•©ì„± í™•ì¸. {desc}"
            ),
            "anchor": "",
            "value_meta": f"{kw1}|{kw2}",
        },
        "q5": {
            "type": "ëŒë°œ/í™•ì¥",
            "question": q5,
            "basis": build_basis_text(summary=q5, intent="ì••ë°• ìƒí™©ì—ì„œì˜ ì‚¬ê³  ì •ë¦¬ì™€ ì‹¤í–‰ ìš°ì„ ìˆœìœ„ë¥¼ í™•ì¸í•˜ê¸° ìœ„í•œ ì§ˆë¬¸ì…ë‹ˆë‹¤."),
            "anchor": "",
        },
    }


def generate_questions(essay: str, airline: str, version: int) -> Dict[str, Dict[str, str]]:
    atype = airline_profile(airline)

    essay_hash = hashlib.sha256(essay.encode("utf-8", errors="ignore")).hexdigest() if essay else ""
    cache = _get_or_build_item_analysis_cache(st.session_state.get("qa_sets", []), essay_hash)
    items = cache.get("items", []) or []

    # CLOVA ë‹¨ë… ëª¨ë“œ: OpenAI ì˜ì¡´ì„± ì™„ì „ ì œê±°
    qa_sets = st.session_state.get("qa_sets", [])
    pipeline_cache_key = f"_sharp_pipeline_{essay_hash}"

    essay_id = stable_int_hash(essay)
    base = stable_int_hash(f"{essay_id}|{airline}|{atype}|{len(items)}")

    if not items:
        logger.debug("[DEBUG generate_questions] items is empty, returning fallback")
        dummy_item = {
            "index": 1,
            "qtype": "ê²½í—˜ ìš”êµ¬í˜•",
            "situation": "í˜„ì¥ì—ì„œ ë³€ìˆ˜ê°€ ë°œìƒí•œ ìƒí™©",
            "basisA": {"text": "ìê¸°ì†Œê°œì„œ ë‹µë³€ ë‚´ìš©"},
            "basisB": {"text": "ìê¸°ì†Œê°œì„œ ë‹µë³€ ë‚´ìš©"},
            "action_sents": [],
            "result_sents": [],
        }
        return _fallback_questions_fixed_slots_item(base=base, version=version, airline=airline, atype=atype, item=dummy_item)

    pick_idx = _pick_item_index(base, version, len(items))
    item = items[pick_idx]

    logger.debug(f"[DEBUG ITEM] version={version}, items count={len(items)}, pick_idx={pick_idx} (CLOVA ë‹¨ë… ëª¨ë“œ)")

    qtype = item.get("qtype", "ê²½í—˜ ìš”êµ¬í˜•")
    situation = item.get("situation", "í˜„ì¥ì—ì„œ ë³€ìˆ˜ê°€ ë°œìƒí•œ ìƒí™©")
    raw_answer = item.get("answer", "")

    basisA = (item.get("basisA", {}) or {}).get("text", "")
    basisB = (item.get("basisB", {}) or {}).get("text", "")
    pick_ab = _pick_basis_ab(base, version)
    basis = basisA if pick_ab == 0 else basisB
    if not basis:
        basis = raw_answer[:80] if raw_answer else "ìê¸°ì†Œê°œì„œ ë‹µë³€ì—ì„œ ì–¸ê¸‰í•œ ë‚´ìš©"
    basis = _trim_no_ellipsis(_auto_fix_particles_kor(_sanity_kor_endings(_strip_ellipsis_tokens(basis))), 120)

    is_soft_version = (version % 2 == 0)

    q1_text = _slot_q1_common(base, version, atype)

    # ============================================
    # Q2: ìì†Œì„œ ê¸°ë°˜ ì‹¬ì¸µ ì§ˆë¬¸
    # FLYREADY 2íšŒ í˜¸ì¶œ ì—”ì§„ v2.0 ì‚¬ìš©
    # ============================================
    q2_text = ""
    q2_source_sentence = ""
    claim = ""  # claim ì´ˆê¸°í™”

    # ìƒˆ FLYREADY ì—”ì§„ ìºì‹œ í‚¤
    flyready_cache_key = f"_flyready_engine_{essay_hash}"

    # ë¬¸í•­2 ë°ì´í„° ì¶”ì¶œ
    item2_data = None
    if qa_sets and len(qa_sets) >= 2:
        item2_data = qa_sets[1]  # ì¸ë±ìŠ¤ 1 = ë¬¸í•­2
    elif qa_sets and len(qa_sets) == 1:
        item2_data = qa_sets[0]  # ë¬¸í•­ì´ 1ê°œë¿ì´ë©´ ê·¸ê²ƒ ì‚¬ìš©

    # FLYREADY ì—”ì§„ ì‹¤í–‰ (ìºì‹œ í™•ì¸)
    if flyready_cache_key not in st.session_state and item2_data:
        logger.debug("[DEBUG Q2] Running FLYREADY 2-call engine v2.0...")
        try:
            item2_question = item2_data.get("prompt", "")
            item2_answer = item2_data.get("answer", "")

            if item2_question and item2_answer and len(item2_answer) >= 50:
                # FLYREADY ì—”ì§„ í˜¸ì¶œ (2íšŒ í˜¸ì¶œ: ë¶„ì„ â†’ ì§ˆë¬¸ ìƒì„±)
                engine = FlyreadyClovaEngine(airline=airline)
                flyready_result = engine.analyze(item2_question, item2_answer, item_num=2)
                st.session_state[flyready_cache_key] = flyready_result
                logger.debug(f"[DEBUG Q2] FLYREADY Engine generated {len(flyready_result.get('questions', []))} questions")
            else:
                logger.debug("[DEBUG Q2] Item2 data insufficient, skipping FLYREADY engine")
                st.session_state[flyready_cache_key] = None
        except Exception as e:
            logger.debug(f"[DEBUG Q2] FLYREADY Engine error: {e}")
            import traceback
            traceback.print_exc()
            st.session_state[flyready_cache_key] = None

    # ============================================
    # Q2 ì§ˆë¬¸ í’€: ìì†Œì„œ í’ˆì§ˆì— ë”°ë¥¸ ë™ì  ë¶„ê¸°
    # í•µì‹¬ë¬¸ì¥ 4ê°œ ì´ìƒ: ëª¨ë“  ë²„ì „ ìì†Œì„œ ê¸°ë°˜
    # í•µì‹¬ë¬¸ì¥ 3ê°œ ì´í•˜: ë²„ì „1~3 ìì†Œì„œ, ë²„ì „4~6 ì§ë¬´ì—°ê²°
    # ============================================

    # ìì†Œì„œ ê¸°ë°˜ í´ë°± ì§ˆë¬¸ (ìì†Œì„œ ê°€ì • ì—†ì´, ì¼ë°˜ì  ê²€ì¦)
    # ì£¼ì˜: "~ë‹¤ê³ ìš”?" íŒ¨í„´ì€ ìì†Œì„œì— í•´ë‹¹ ë‚´ìš©ì´ ìˆì„ ë•Œë§Œ ì‚¬ìš©
    Q2_RESUME_FALLBACK = [
        {"question": "ìì†Œì„œì—ì„œ ê°€ì¥ ê°•ì¡°í•˜ê³  ì‹¶ì€ ê²½í—˜ì´ ë­¡ë‹ˆê¹Œ?", "intent": "í•µì‹¬ ê²½í—˜ í™•ì¸", "followup": "ì™œ ê·¸ ê²½í—˜ì…ë‹ˆê¹Œ?"},
        {"question": "ë³¸ì¸ì˜ ê°€ì¥ í° ê°•ì ì´ ë­¡ë‹ˆê¹Œ?", "intent": "ìê¸° ì¸ì‹", "followup": "ê·¸ ê°•ì ì„ ì¦ëª…í•  ì‚¬ë¡€ê°€ ìˆìŠµë‹ˆê¹Œ?"},
        {"question": "ì™œ ìŠ¹ë¬´ì›ì´ ë˜ê³  ì‹¶ìŠµë‹ˆê¹Œ?", "intent": "ë™ê¸° ê²€ì¦", "followup": "ë‹¤ë¥¸ ì§ì—…ì€ ì•ˆ ë©ë‹ˆê¹Œ?"},
        {"question": "ìŠ¹ë¬´ì›ì—ê²Œ ê°€ì¥ ì¤‘ìš”í•œ ì—­ëŸ‰ì´ ë­ë¼ê³  ìƒê°í•©ë‹ˆê¹Œ?", "intent": "ì§ë¬´ ì´í•´", "followup": "ë³¸ì¸ì€ ê·¸ ì—­ëŸ‰ì´ ìˆìŠµë‹ˆê¹Œ?"},
        {"question": "íŒ€ì—ì„œ ì£¼ë¡œ ì–´ë–¤ ì—­í• ì„ ë§¡ìŠµë‹ˆê¹Œ?", "intent": "íŒ€ ì—­í• ", "followup": "ê·¸ ì—­í• ì´ ì‹«ì„ ë•ŒëŠ” ì—†ìŠµë‹ˆê¹Œ?"},
        {"question": "ê°€ì¥ í˜ë“¤ì—ˆë˜ ê²½í—˜ì´ ë­¡ë‹ˆê¹Œ?", "intent": "ì—­ê²½ ê·¹ë³µ", "followup": "ì–´ë–»ê²Œ ê·¹ë³µí–ˆìŠµë‹ˆê¹Œ?"},
    ]

    # ì§ë¬´ì—°ê²° ì§ˆë¬¸ (ìŠ¹ë¬´ì› ì§ë¬´ íŠ¹í™”) - Q2ìš©
    # ë§íˆ¬ ë‹¤ì–‘í™”: ~í•˜ì‹œê² ìŠµë‹ˆê¹Œ?, ~í•´ë³¸ ì  ìˆìŠµë‹ˆê¹Œ?, ~ë¼ë©´ìš”?, ~ì€ ì–´ë–»ìŠµë‹ˆê¹Œ?
    Q2_JOB_CONNECTION = [
        {"question": "ë¹„í–‰ ì¤‘ ìŠ¹ê°ì´ ê°‘ìê¸° ì“°ëŸ¬ì§€ë©´ ì–´ë–»ê²Œ í•˜ì‹œê² ìŠµë‹ˆê¹Œ?", "intent": "ì‘ê¸‰ìƒí™© ëŒ€ì²˜", "followup": "ì˜ë£Œì§„ì´ ì—†ë‹¤ë©´ìš”?"},
        {"question": "ê¸°ë‚´ì—ì„œ ìŠ¹ê°ë¼ë¦¬ ë‹¤íˆ¬ë©´ ì–´ë–»ê²Œ ì¤‘ì¬í•˜ì‹œê² ìŠµë‹ˆê¹Œ?", "intent": "ê°ˆë“± ì¤‘ì¬", "followup": "í•œìª½ì´ ì•ˆ ë“¤ìœ¼ë©´ìš”?"},
        {"question": "ì¥ê±°ë¦¬ ë¹„í–‰ ì»¨ë””ì…˜ ê´€ë¦¬ëŠ” ì–´ë–»ê²Œ í•˜ì‹¤ ê²ë‹ˆê¹Œ?", "intent": "ìê¸°ê´€ë¦¬", "followup": "ì ì„ ëª» ìë©´ìš”?"},
        {"question": "ìƒì‚¬ê°€ ë¶€ë‹¹í•œ ì§€ì‹œë¥¼ í•˜ë©´ ì–´ë–»ê²Œ í•˜ì‹œê² ìŠµë‹ˆê¹Œ?", "intent": "ì¡°ì§ ì ì‘", "followup": "ê·¸ë˜ë„ í•˜ë¼ê³  í•˜ë©´ìš”?"},
        {"question": "ì„œë¹„ìŠ¤ ì¤‘ ì‹¤ìˆ˜ë¥¼ í•˜ë©´ ì–´ë–»ê²Œ ìˆ˜ìŠµí•˜ì‹œê² ìŠµë‹ˆê¹Œ?", "intent": "ìœ„ê¸° ëŒ€ì‘", "followup": "ìŠ¹ê°ì´ í™”ë¥¼ ë‚´ë©´ìš”?"},
        {"question": "íŒ€ì›ì´ ì¼ì„ ì•ˆ í•˜ë©´ ì–´ë–»ê²Œ í•˜ì‹œê² ìŠµë‹ˆê¹Œ?", "intent": "íŒ€ì›Œí¬", "followup": "ë§í•´ë„ ì•ˆ ê³ ì¹˜ë©´ìš”?"},
        {"question": "ì•ˆì „ê³¼ ì„œë¹„ìŠ¤ê°€ ì¶©ëŒí•˜ë©´ ë­˜ ìš°ì„ í•˜ì‹œê² ìŠµë‹ˆê¹Œ?", "intent": "ìš°ì„ ìˆœìœ„ íŒë‹¨", "followup": "ìŠ¹ê°ì´ ë¶ˆë§Œì´ë©´ìš”?"},
        {"question": "ê¸°ë‚´ì‹ì´ ë¶€ì¡±í•˜ë©´ ì–´ë–»ê²Œ í•˜ì‹œê² ìŠµë‹ˆê¹Œ?", "intent": "ìì› ë°°ë¶„", "followup": "VIPê°€ ìš”êµ¬í•˜ë©´ìš”?"},
        {"question": "ì§„ìƒ ìŠ¹ê°ì„ ì‘ëŒ€í•´ë³¸ ì  ìˆìŠµë‹ˆê¹Œ?", "intent": "ê³ ê° ëŒ€ì‘ ê²½í—˜", "followup": "ì–´ë–»ê²Œ í•´ê²°í–ˆìŠµë‹ˆê¹Œ?"},
        {"question": "ë¹„ìƒíƒˆì¶œ ì ˆì°¨ë¥¼ ì„¤ëª…í•´ë³´ì‹¤ ìˆ˜ ìˆìŠµë‹ˆê¹Œ?", "intent": "ì•ˆì „ ì§€ì‹", "followup": "ì‹¤ì œë¡œ ì—°ìŠµí•´ë´¤ìŠµë‹ˆê¹Œ?"},
        {"question": "ì™¸êµ­ì¸ ìŠ¹ê°ê³¼ ì†Œí†µì´ ì•ˆ ë˜ë©´ ì–´ë–»ê²Œ í•˜ì‹œê² ìŠµë‹ˆê¹Œ?", "intent": "ì–¸ì–´ ì¥ë²½ ëŒ€ì²˜", "followup": "í†µì—­ì´ ì—†ë‹¤ë©´ìš”?"},
        {"question": "ìŠ¹ê°ì´ ê¸°ë‚´ ê·œì •ì„ ì•ˆ ì§€í‚¤ë©´ ì–´ë–»ê²Œ í•˜ì‹œê² ìŠµë‹ˆê¹Œ?", "intent": "ê·œì • ì¤€ìˆ˜ ìœ ë„", "followup": "ê³„ì† ì•ˆ ì§€í‚¤ë©´ìš”?"},
    ]

    # ì§ë¬´ì—°ê²° ê¼¬ë¦¬ì§ˆë¬¸ (Q3ìš©) - ë‹¤ì–‘í•œ ë§íˆ¬
    Q3_JOB_CONNECTION = [
        "ì‹¤ì œë¡œ ê·¸ë ‡ê²Œ í•  ìˆ˜ ìˆìŠµë‹ˆê¹Œ?",
        "ì—°ìŠµí•´ë³¸ ì  ìˆìŠµë‹ˆê¹Œ?",
        "êµìœ¡ë°›ì€ ì  ìˆìŠµë‹ˆê¹Œ?",
        "ê·¸ ë°©ë²•ì´ ì•ˆ í†µí•˜ë©´ìš”?",
        "ë‹¤ë¥¸ ë°©ë²•ì€ ì—†ìŠµë‹ˆê¹Œ?",
        "ë¹„ìŠ·í•œ ê²½í—˜ì´ ìˆìŠµë‹ˆê¹Œ?",
        "ìì‹  ìˆìŠµë‹ˆê¹Œ?",
        "ì™œ ê·¸ë ‡ê²Œ ìƒê°í•©ë‹ˆê¹Œ?",
    ]

    # ============================================
    # FLYREADY v4.0: ë²„ì „ë³„ ì§ˆë¬¸ ì§ì ‘ ì„ íƒ (1íšŒ í˜¸ì¶œë¡œ 6ë²„ì „ ìƒì„±ë¨)
    # ============================================
    flyready_result = st.session_state.get(flyready_cache_key)
    use_job_connection = False
    q2_text = ""
    q2_source_sentence = ""
    followup = "ê·¸ë˜ì„œ ê²°ê³¼ê°€ ë­¡ë‹ˆê¹Œ?"

    if flyready_result and isinstance(flyready_result, dict):
        # v4.0: questionsëŠ” {"v1": "ì§ˆë¬¸1", "v2": "ì§ˆë¬¸2", ...} í˜•íƒœ
        questions_dict = flyready_result.get("questions", {})
        types_dict = flyready_result.get("types", {})
        slots_dict = flyready_result.get("slots", {})
        intents_dict = flyready_result.get("intents", {})

        # ì·¨ì•½ì  ì •ë³´ ì €ì¥ (ë””ë²„ê¹…/í‘œì‹œìš©)
        vulnerabilities = flyready_result.get("vulnerabilities", [])
        if vulnerabilities:
            st.session_state["_flyready_vulnerabilities"] = vulnerabilities

        # ë“±ê¸‰ ì •ë³´ ì €ì¥
        grade = flyready_result.get("grade", "MEDIUM")
        st.session_state["_flyready_grade"] = grade
        logger.debug(f"[DEBUG Q2 v4.0] ë“±ê¸‰: {grade}, ì·¨ì•½ì  ì ìˆ˜: {flyready_result.get('vulnerability_score', 0)}ì ")

        # ë²„ì „ì— ë§ëŠ” ì§ˆë¬¸ ì§ì ‘ ì„ íƒ
        v_key = f"v{version}"
        q2_text = questions_dict.get(v_key, "")
        q2_type = types_dict.get(v_key, "ìì†Œì„œ ê¸°ë°˜")
        q2_slot = slots_dict.get(v_key, "SLOT_ACTION")
        q2_source_sentence = intents_dict.get(v_key, "")

        # ì§ë¬´ì—°ê²° ì—¬ë¶€ íŒë‹¨
        use_job_connection = (q2_type == "ì§ë¬´ ì—°ê²°")

        logger.debug(f"[DEBUG Q2 v4.0] Version {version}: [{q2_type}] [{q2_slot}]")
        logger.debug(f"[DEBUG Q2 v4.0] ì§ˆë¬¸: {q2_text[:50]}..." if q2_text else "[DEBUG Q2 v4.0] ì§ˆë¬¸ ì—†ìŒ")

    # ì§ˆë¬¸ì´ ì—†ìœ¼ë©´ í´ë°±
    if not q2_text:
        logger.debug(f"[DEBUG Q2 v4.0] ì§ˆë¬¸ ì—†ìŒ - í´ë°± ì‚¬ìš©")
        fallback_idx = (base + version) % len(Q2_RESUME_FALLBACK)
        selected_fallback = Q2_RESUME_FALLBACK[fallback_idx]
        q2_text = selected_fallback.get("question", "")
        q2_source_sentence = selected_fallback.get("intent", "")
        followup = selected_fallback.get("followup", "ê·¸ë˜ì„œ ê²°ê³¼ê°€ ë­¡ë‹ˆê¹Œ?")

    # ì§ë¬´ì—°ê²° ì§ˆë¬¸ ì—¬ë¶€ ì €ì¥ (ë¼ë²¨ í‘œì‹œìš©)
    st.session_state["_is_job_connection_q2"] = use_job_connection

    logger.debug(f"[DEBUG Q2 v4.0] Final: {q2_text[:50]}... (ì§ë¬´ì—°ê²°: {use_job_connection})")

    # Q3ìš©ìœ¼ë¡œ ê¼¬ë¦¬ì§ˆë¬¸ ì €ì¥
    if use_job_connection:
        job_q3_idx = (base + version) % len(Q3_JOB_CONNECTION)
        followup = Q3_JOB_CONNECTION[job_q3_idx]

        st.session_state["_flyready_followup"] = followup
        logger.debug(f"[DEBUG Q3] Stored followup: {followup[:40]}...")

    # Q3: FLYREADY ìŠ¤íƒ€ì¼ ê¼¬ë¦¬ì§ˆë¬¸ (ë‚ ì¹´ë¡œìš´ ì–´ë¯¸ ìœ ì§€)
    Q3_FLYREADY_TEMPLATES = [
        "ê·¸ë˜ì„œ ê²°ê³¼ëŠ” êµ¬ì²´ì ìœ¼ë¡œ ë­¡ë‹ˆê¹Œ?",
        "ë‹¤ë¥¸ ë°©ë²•ì€ ê³ ë ¤í•˜ì§€ ì•Šì•˜ìŠµë‹ˆê¹Œ?",
        "ê°™ì€ ìƒí™©ì´ ì™€ë„ ê°™ì€ ì„ íƒì„ í•˜ê² ìŠµë‹ˆê¹Œ?",
        "ê·¸ ê²½í—˜ì´ ìŠ¹ë¬´ì› ì—…ë¬´ì— ë„ì›€ì´ ë©ë‹ˆê¹Œ?",
        "ê·¸ë ‡ê²Œ ë§ì”€í•˜ì‹  ê·¼ê±°ê°€ ë­¡ë‹ˆê¹Œ?",
        "ì‹¤íŒ¨í•œ ë¶€ë¶„ì€ ì—†ì—ˆìŠµë‹ˆê¹Œ?",
        "ë³¸ì¸ ì±…ì„ì€ ì–¼ë§ˆë‚˜ ë©ë‹ˆê¹Œ?",
        "ê·¸ íŒë‹¨ì´ í‹€ë ¸ìœ¼ë©´ìš”?",
    ]

    tone = "soft" if is_soft_version else "sharp"

    # Q3: FLYREADY ì—”ì§„ì˜ followup ìš°ì„  ì‚¬ìš©
    q3_text = ""

    # 1ìˆœìœ„: FLYREADY ì—”ì§„ì—ì„œ ì €ì¥ëœ ê¼¬ë¦¬ì§ˆë¬¸
    flyready_followup = st.session_state.get("_flyready_followup", "")
    if flyready_followup and len(flyready_followup) >= 5:
        q3_text = flyready_followup.strip()
        logger.debug(f"[DEBUG Q3] Using FLYREADY followup: {q3_text[:50]}...")

    # 2ìˆœìœ„: expected_answersì—ì„œ ê¼¬ë¦¬ì§ˆë¬¸
    if not q3_text:
        expected_answers = st.session_state.get("_q2_expected_answers", None)
        if expected_answers and isinstance(expected_answers, list) and expected_answers:
            ans_idx = (base + version) % len(expected_answers)
            selected_ans = expected_answers[ans_idx]

            if isinstance(selected_ans, dict):
                followup = selected_ans.get("followup", "")
                if followup and len(followup) >= 5:
                    q3_text = followup.strip()
                    logger.debug(f"[DEBUG Q3] Using expected_answers! Q3: {q3_text[:50]}...")

    # 3ìˆœìœ„: FLYREADY ìŠ¤íƒ€ì¼ ê¼¬ë¦¬ì§ˆë¬¸ í…œí”Œë¦¿ ì‚¬ìš©
    if not q3_text:
        q3_idx = (base + version * 3) % len(Q3_FLYREADY_TEMPLATES)
        q3_text = Q3_FLYREADY_TEMPLATES[q3_idx]
        logger.debug(f"[DEBUG Q3] Using FLYREADY fallback: {q3_text}")

    q5_text = _slot_q5_surprise(base, version, atype)

    # í•­ê³µì‚¬ ìœ í˜•ì— ë§ëŠ” ê°€ì¹˜ ë°ì´í„° ì„ íƒ (FSC/LCC/HSC)
    if atype == "FSC":
        value_data = FSC_VALUE_DATA
    elif atype == "HSC":
        value_data = HSC_VALUE_DATA
    else:
        value_data = LCC_VALUE_DATA
    # HSCëŠ” LCC í…œí”Œë¦¿ ì‚¬ìš© (ì¥ê±°ë¦¬ íŠ¹í™”ì´ì§€ë§Œ ì €ë¹„ìš© êµ¬ì¡° ê¸°ë°˜)
    # í™€ìˆ˜ ë²„ì „(1,3,5)=ë‚ ì¹´ë¡œì›€, ì§ìˆ˜ ë²„ì „(2,4,6)=ë¶€ë“œëŸ¬ì›€
    # (is_soft_versionì€ ìœ„ì—ì„œ ì´ë¯¸ ì •ì˜ë¨)
    if atype == "FSC":
        value_tpls = VALUE_Q_TEMPLATES_FSC_SOFT if is_soft_version else VALUE_Q_TEMPLATES_FSC_SHARP
    else:
        value_tpls = VALUE_Q_TEMPLATES_LCC_SOFT if is_soft_version else VALUE_Q_TEMPLATES_LCC_SHARP

    # ì§„ì—ì–´/ì—ì–´ë¶€ì‚°/ì—ì–´ì„œìš¸ í†µí•© LCC ì§ˆë¬¸ (version % 3 == 0 ì¼ ë•Œ)
    airline_key = _canonical_airline_name(airline)
    use_integration_q = (airline_key in INTEGRATED_LCC_AIRLINES) and (version % 3 == 0)

    if use_integration_q:
        q4_idx = (base + version) % len(INTEGRATED_LCC_Q_TEMPLATES)
        q4_text = INTEGRATED_LCC_Q_TEMPLATES[q4_idx]
    else:
        q4_tpl_idx = _pick_det_idx(base, version, 4, len(value_tpls))
        kw = value_data.get("keywords", []) or []
        desc = _auto_fix_particles_kor(_sanity_kor_endings(_strip_ellipsis_tokens(value_data.get("desc", ""))))
        if not kw:
            kw = VALUES_DEFAULT.get(atype, VALUES_DEFAULT["LCC"])
        kw1 = kw[(base + version) % len(kw)]
        kw2 = kw[(base + version * 3 + 1) % len(kw)]
        if kw2 == kw1 and len(kw) > 1:
            kw2 = kw[(base + version * 3 + 2) % len(kw)]
        airline_disp = airline_key
        q4_text = value_tpls[q4_tpl_idx].format(airline=airline_disp, kw1=kw1, kw2=kw2, desc=desc)
        q4_text = _normalize_airline_name_in_text(q4_text, airline)
        q4_text = _fix_particles_after_format(q4_text)  # ì¡°ì‚¬ ë³´ì • ì¶”ê°€
    q4_text = _sanitize_question_strict(q4_text)

    anchor = _pick_anchor_by_rule(
        qtype_internal=qtype,
        action_sents=item.get("action_sents", []) or [],
        result_sents=item.get("result_sents", []) or [],
    )
    anchor = fmt_anchor_text(anchor)

    # ìƒˆ ê³µê²© í¬ì¸íŠ¸ ê¸°ë°˜ ê·¼ê±° í…ìŠ¤íŠ¸ (deep_questions ì‚¬ìš© ì‹œ q2_source_sentence í™œìš©)
    attack_point = q2_source_sentence if q2_source_sentence else ""
    attack_info = _trim_no_ellipsis(f"ê³µê²© í¬ì¸íŠ¸: {attack_point}", 100) if attack_point else ""
    claim_info = _trim_no_ellipsis(f"ì£¼ì¥: {claim}", 80) if claim else ""
    basis_summary = _trim_no_ellipsis(f"ë¬¸í•­ {item.get('index', 1)} / {attack_info}", 180)
    intent2 = "ìì†Œì„œì˜ ì´ìƒì  í‘œí˜„ì´ë‚˜ ì·¨ì•½ ì§€ì ì„ ê³µê²©í•˜ì—¬ ì§€ì›ìì˜ ì‹ ë…ì„ ê²€ì¦í•˜ëŠ” ì§ˆë¬¸ì…ë‹ˆë‹¤."
    intent3 = "Q2ì˜ ë‹µë³€ì„ ì „ì œí•˜ì§€ ì•Šê³ , ì¬í˜„ì„±ê³¼ íŒë‹¨ ê¸°ì¤€ì„ ë…ë¦½ì ìœ¼ë¡œ ê²€ì¦í•˜ëŠ” ê¼¬ë¦¬ ì§ˆë¬¸ì…ë‹ˆë‹¤."

    out: Dict[str, Dict[str, str]] = {
        "q1": {
            "type": "ê³µí†µ ì§ˆë¬¸",
            "question": q1_text,
            "basis": build_basis_text(summary=q1_text, intent="ê¸°ë³¸ ì—­ëŸ‰ê³¼ í˜„ì¥ ëŒ€ì‘ì„ í™•ì¸í•˜ê¸° ìœ„í•œ ê³µí†µ ì§ˆë¬¸ì…ë‹ˆë‹¤."),
            "anchor": "",
        },
        "q2": {
            "type": "ì§ë¬´ì—°ê²° ì§ˆë¬¸" if st.session_state.get("_is_job_connection_q2", False) else "ì‹¬ì¸µ(ìì†Œì„œ ê¸°ë°˜)",
            "question": q2_text,
            "basis": build_basis_text(summary=basis_summary, intent=intent2),
            "anchor": anchor,
        },
        "q3": {
            "type": "ì§ë¬´ì—°ê²° ê¼¬ë¦¬" if st.session_state.get("_is_job_connection_q2", False) else "ê¼¬ë¦¬ ì§ˆë¬¸",
            "question": q3_text,
            "basis": build_basis_text(summary=_trim_no_ellipsis(f"ë¬¸í•­ {item.get('index', 1)} í‰ê°€ ì£¼ì œ ìœ ì§€ / ìƒí™©: {situation}", 180), intent=intent3),
            "anchor": anchor,
        },
        "q4": {
            "type": "ì¸ì¬ìƒ/ê°€ì¹˜",
            "question": q4_text,
            "basis": build_basis_text(
                summary=f"[{airline_disp}] í•µì‹¬ê°€ì¹˜: {kw1}, {kw2}",
                intent=f"í•­ê³µì‚¬ ì¸ì¬ìƒê³¼ì˜ ì •í•©ì„± í™•ì¸. {desc}"
            ),
            "anchor": "",
            "value_meta": f"{kw1}|{kw2}",
        },
        "q5": {
            "type": "ëŒë°œ/í™•ì¥",
            "question": q5_text,
            "basis": build_basis_text(summary=q5_text, intent="ì••ë°• ìƒí™©ì—ì„œì˜ ì‚¬ê³  ì •ë¦¬ì™€ ì‹¤í–‰ ìš°ì„ ìˆœìœ„ë¥¼ í™•ì¸í•˜ê¸° ìœ„í•œ ì§ˆë¬¸ì…ë‹ˆë‹¤."),
            "anchor": "",
        },
    }
    return out


def safe_generate_questions(essay: str, airline: str, version: int) -> Dict[str, Dict[str, str]]:
    start = time.monotonic()
    atype = airline_profile(airline)
    essay_id = stable_int_hash(essay)
    essay_hash = hashlib.sha256(essay.encode("utf-8", errors="ignore")).hexdigest() if essay else ""

    cache = _get_or_build_item_analysis_cache(st.session_state.get("qa_sets", []), essay_hash)
    items = cache.get("items", []) or []
    base = stable_int_hash(f"{essay_id}|{airline}|{atype}|{len(items)}")

    try:
        out = generate_questions(essay=essay, airline=airline, version=version)
        elapsed = time.monotonic() - start
        logger.debug(f"[DEBUG safe_generate] generate_questions completed. elapsed={elapsed:.2f}s, SOFT_TIMEOUT_SEC={SOFT_TIMEOUT_SEC}")

        if elapsed > SOFT_TIMEOUT_SEC:
            logger.debug(f"[DEBUG safe_generate] TIMEOUT! Triggering fallback")
            if items:
                pick_idx = _pick_item_index(base, version, len(items))
                return _fallback_questions_fixed_slots_item(base=base, version=version, airline=airline, atype=atype, item=items[pick_idx])
            dummy_item = {"index": 1, "qtype": "ê²½í—˜ ìš”êµ¬í˜•", "situation": "í˜„ì¥ì—ì„œ ë³€ìˆ˜ê°€ ë°œìƒí•œ ìƒí™©", "basisA": {"text": "ìê¸°ì†Œê°œì„œ ë‹µë³€ ë‚´ìš©"}, "basisB": {"text": "ìê¸°ì†Œê°œì„œ ë‹µë³€ ë‚´ìš©"}, "action_sents": [], "result_sents": []}
            return _fallback_questions_fixed_slots_item(base=base, version=version, airline=airline, atype=atype, item=dummy_item)

        for k in ["q1", "q2", "q3", "q4", "q5"]:
            if not out.get(k) or not out[k].get("question"):
                logger.debug(f"[DEBUG safe_generate] Missing question for {k}, triggering fallback")
                if items:
                    pick_idx = _pick_item_index(base, version, len(items))
                    return _fallback_questions_fixed_slots_item(base=base, version=version, airline=airline, atype=atype, item=items[pick_idx])
                dummy_item = {"index": 1, "qtype": "ê²½í—˜ ìš”êµ¬í˜•", "situation": "í˜„ì¥ì—ì„œ ë³€ìˆ˜ê°€ ë°œìƒí•œ ìƒí™©", "basisA": {"text": "ìê¸°ì†Œê°œì„œ ë‹µë³€ ë‚´ìš©"}, "basisB": {"text": "ìê¸°ì†Œê°œì„œ ë‹µë³€ ë‚´ìš©"}, "action_sents": [], "result_sents": []}
                return _fallback_questions_fixed_slots_item(base=base, version=version, airline=airline, atype=atype, item=dummy_item)
        logger.debug(f"[DEBUG safe_generate] Returning normal out. Q2: {out.get('q2', {}).get('question', '')[:50]}...")
        return out
    except Exception as e:
        logger.debug(f"[DEBUG safe_generate] EXCEPTION! {type(e).__name__}: {e}")
        if items:
            pick_idx = _pick_item_index(base, version, len(items))
            return _fallback_questions_fixed_slots_item(base=base, version=version, airline=airline, atype=atype, item=items[pick_idx])
        dummy_item = {"index": 1, "qtype": "ê²½í—˜ ìš”êµ¬í˜•", "situation": "í˜„ì¥ì—ì„œ ë³€ìˆ˜ê°€ ë°œìƒí•œ ìƒí™©", "basisA": {"text": "ìê¸°ì†Œê°œì„œ ë‹µë³€ ë‚´ìš©"}, "basisB": {"text": "ìê¸°ì†Œê°œì„œ ë‹µë³€ ë‚´ìš©"}, "action_sents": [], "result_sents": []}
        return _fallback_questions_fixed_slots_item(base=base, version=version, airline=airline, atype=atype, item=dummy_item)


# ----------------------------
# STEP 6: ì‚¬ì‹¤ ê¸°ë°˜ í”¼ë“œë°±
# ----------------------------

def overlap_keywords(answer: str, keywords: List[str]) -> List[str]:
    if not answer:
        return []
    hits = []
    for k in keywords:
        if k and (k in answer):
            hits.append(k)
    return hits


def answer_has_specifics(answer: str) -> bool:
    if not answer:
        return False
    if re.search(r"\d", answer):
        return True
    if re.search(r"(ì–¸ì œ|ì–´ë””ì„œ|ëˆ„êµ¬|ëª‡|ê¸°ê°„|íšŸìˆ˜|ë¹„ìœ¨|í¼ì„¼íŠ¸|ëª…|ê±´|ë§Œì›|ì›|ì‹œê°„|ë¶„)", answer):
        return True
    return False


def build_feedback_kor(qtype: str, anchor: str, basis: str, answer: str, risk_keywords: List[str], evidence: List[str], value_meta: str = "") -> str:
    if not answer.strip():
        return ""

    hits = overlap_keywords(answer, risk_keywords)
    specifics = answer_has_specifics(answer)

    grounded = False
    if anchor:
        a = anchor[:60]
        grounded = (a in answer) or (len(hits) > 0)

    evidence_pick = ""
    if evidence and hits:
        for s in evidence:
            if any(h in s for h in hits[:5]):
                evidence_pick = s
                break
    if not evidence_pick and evidence:
        evidence_pick = evidence[0]

    lines = []
    lines.append("í…ìŠ¤íŠ¸ ê·¼ê±° ê¸°ë°˜ ì ê²€:")

    if qtype == "ì¸ì¬ìƒ/ê°€ì¹˜" and value_meta:
        try:
            fit, conf = value_meta.split("|", 1)
        except Exception:
            fit, conf = "", ""
        if fit and conf:
            lines.append(f"- ê°€ì¹˜ ì–¸ê¸‰ ì—¬ë¶€(ì í•©/ì¶©ëŒ): {'ì í•©' if fit in answer else 'ì í•© ë¯¸ì–¸ê¸‰'} / {'ì¶©ëŒ' if conf in answer else 'ì¶©ëŒ ë¯¸ì–¸ê¸‰'}")
        else:
            lines.append("- ê°€ì¹˜ ì–¸ê¸‰ ì—¬ë¶€: í™•ì¸ ë¶ˆê°€(ê°’ ë©”íƒ€ ëˆ„ë½)")

        lines.append(f"- êµ¬ì²´ì„±(ìˆ«ì/ê¸°ê°„/ì¥ì†Œ/ëŒ€ìƒ): {'ìˆìŒ' if specifics else 'ë¶€ì¡±'}")
        lines.append("- ìš”êµ¬ì‚¬í•­ ì¶©ì¡± ì—¬ë¶€: 'ê°€ì¹˜ 2ê°œ(ì í•©/ì¶©ëŒ) + ìì†Œì„œ ê²½í—˜ ê·¼ê±°'ê°€ ë‹µë³€ì— í¬í•¨ë˜ì–´ì•¼ í•¨")
        if not specifics:
            lines.append("- ë³´ì™„: ê·¼ê±° ê²½í—˜ì„ í–‰ë™/ê²°ê³¼(ìˆ˜ì¹˜)ë¡œ ê³ ì •í•´ì„œ ë§í•  ê²ƒ")
        return "\n".join(lines)

    if hits:
        lines.append(f"- ìœ„í—˜ ì‹ í˜¸ í‚¤ì›Œë“œì™€ì˜ ì—°ê³„: {', '.join(hits[:8])}")
    else:
        lines.append("- ìœ„í—˜ ì‹ í˜¸ í‚¤ì›Œë“œì™€ì˜ ì—°ê³„: ê°ì§€ë˜ì§€ ì•ŠìŒ(ìì†Œì„œ í‘œí˜„ì„ ê·¸ëŒ€ë¡œ ê°€ì ¸ì™€ ê²€ì¦ ê°€ëŠ¥í•˜ê²Œ ë‹µë³€í•  ê²ƒ).")
    lines.append(f"- êµ¬ì²´ì„±(ìˆ«ì/ê¸°ê°„/ì¥ì†Œ/ëŒ€ìƒ): {'ìˆìŒ' if specifics else 'ë¶€ì¡±'}")
    if anchor:
        lines.append(f"- ì•µì»¤(ê·¼ê±° í‘œí˜„) ì—°ê³„: {'ìˆìŒ' if grounded else 'ì•½í•¨'}")
    if evidence_pick:
        lines.append(f"- ì°¸ê³  ê·¼ê±° ë¬¸ì¥: \"{evidence_pick[:180]}{'...' if len(evidence_pick) > 180 else ''}\"")
    lines.append("ê°œì„  í¬ì¸íŠ¸(ê²€ì¦ ê°€ëŠ¥í•˜ê²Œ):")
    lines.append("- ì£¼ì¥ 1ê°œë¥¼ ê³ ë¥´ê³ , ê·¸ ì£¼ì¥ì— ëŒ€í•œ ê·¼ê±°(ìˆ˜ì¹˜/ê¸°ê°„/í–‰ë™)ë¥¼ ë°”ë¡œ ì œì‹œ.")
    if not specifics:
        lines.append("- ìµœì†Œ 1ê°œ ì§€í‘œ(ê±´ìˆ˜/%, ì‹œê°„/ê¸°ê°„, ê³ ê°ì˜í–¥)ë¥¼ ë„£ì–´ ë‹µë³€ì„ í™•ì •ì ìœ¼ë¡œ ë§Œë“¤ê¸°.")
    return "\n".join(lines)


# ----------------------------
# ì»¤ì„œ CSS
# ----------------------------

_AIRPLANE_CURSOR_SVG = (
    "data:image/svg+xml;utf8,"
    "<svg xmlns='http://www.w3.org/2000/svg' width='32' height='32' viewBox='0 0 64 64'>"
    "<path d='M2 34l60-10-60-10 8 10 14 4v12l-14 4-8 10z' fill='black'/>"
    "</svg>"
)

st.markdown(
    f"""
    <meta name="google" content="notranslate">
    <meta name="robots" content="notranslate">
    <style>
      /* êµ¬ê¸€ ë²ˆì—­ ë°©ì§€ */
      html {{
        translate: no;
      }}
      .stApp, .stApp * {{
        cursor: url("{_AIRPLANE_CURSOR_SVG}") 4 4, auto !important;
      }}
      .stApp input, .stApp textarea, .stApp [contenteditable="true"] {{
        cursor: text !important;
      }}
      .stApp button, .stApp a, .stApp [role="button"] {{
        cursor: pointer !important;
      }}
      /* ì§ˆë¬¸ ìƒì„± ë²„íŠ¼ - ë¶€ë“œëŸ¬ìš´ íŒŒìŠ¤í…” ë¸”ë£¨ */
      button[kind="primary"] {{
        background-color: #A8D8EA !important;
        border-color: #A8D8EA !important;
        color: #2C3E50 !important;
      }}
      button[kind="primary"]:hover {{
        background-color: #8BC9DE !important;
        border-color: #8BC9DE !important;
      }}
    </style>
    """,
    unsafe_allow_html=True,
)

# ----------------------------
# UI / App
# ----------------------------

if "question_version" not in st.session_state:
    st.session_state.question_version = 1
if "last_essay_hash" not in st.session_state:
    st.session_state.last_essay_hash = ""
if "questions" not in st.session_state:
    st.session_state.questions = {}
if "answers" not in st.session_state:
    st.session_state.answers = {"q1": "", "q2": "", "q3": "", "q4": "", "q5": ""}
if "feedback" not in st.session_state:
    st.session_state.feedback = {"q1": "", "q2": "", "q3": "", "q4": "", "q5": ""}
if "report_text" not in st.session_state:
    st.session_state.report_text = ""
if "q3_generated" not in st.session_state:
    st.session_state.q3_generated = False
if "_regen_last_request_id" not in st.session_state:
    st.session_state._regen_last_request_id = ""
if "_regen_in_progress" not in st.session_state:
    st.session_state._regen_in_progress = False
if "qa_sets" not in st.session_state:
    st.session_state.qa_sets = [{"prompt": "", "answer": ""}]

st.title("ìŠ¹ë¬´ì› AI ë©´ì ‘ ì½”ì¹­")
st.caption("ìì†Œì„œ ê¸°ë°˜ ë©´ì ‘ ì§ˆë¬¸ ìƒì„± ë° ë‹µë³€ ì—°ìŠµ")
colL, colR = st.columns([1, 1])

with colL:
    st.subheader("STEP 1) ê¸°ë³¸ ì„¤ì •")
    airline_raw = st.selectbox(
        "ì§€ì› í•­ê³µì‚¬",
        AIRLINES,
        index=AIRLINES.index("ì—ì–´ë¡œì¼€ì´") if "ì—ì–´ë¡œì¼€ì´" in AIRLINES else 0,
        format_func=_canonical_airline_name,
    )
    airline = _canonical_airline_name(airline_raw)
    st.session_state.selected_airline = airline  # LLM í˜¸ì¶œ ì‹œ FSC/LCC êµ¬ë¶„ìš©
    atype = airline_profile(airline)
    st.info(f"í•­ê³µì‚¬ íƒ€ì…: {atype}")
    st.caption("ë©´ì ‘ ì–¸ì–´: í•œêµ­ì–´(ê³ ì •)")

    # í•­ê³µì‚¬ë³„ ì„ í˜¸ ì¸ì¬ìƒ í‘œì‹œ
    airline_key = _raw_airline_key(airline)
    pref_type = AIRLINE_PREFERRED_TYPE.get(airline_key, {})
    if pref_type:
        st.markdown(f"**2026 ì„ í˜¸ ì¸ì¬ìƒ:** {pref_type.get('nickname', '')}")
        st.caption(f"{pref_type.get('description', '')}")

    # ì˜ì–´ ë©´ì ‘ ìˆëŠ” í•­ê³µì‚¬ ì•ˆë‚´
    if airline_key in ENGLISH_INTERVIEW_AIRLINES:
        st.warning("ì´ í•­ê³µì‚¬ëŠ” **ì˜ì–´ ë©´ì ‘ ì „í˜•**ì´ ìˆìŠµë‹ˆë‹¤.")

    # ì´ë ¥ì„œ ì •ë³´ ì…ë ¥ (Basic/Pro ìš”ê¸ˆì œìš©)
    current_plan = st.session_state.get("current_plan", DEFAULT_PLAN)
    plan_config = PLAN_CONFIG.get(current_plan, PLAN_CONFIG["basic"])

    if plan_config.get("resume_questions", 0) > 0:
        with st.expander("ì´ë ¥ì„œ ì •ë³´ (ì„ íƒ)", expanded=False):
            st.caption("ê°œì¸ì •ë³´ ì—†ì´ í•„ìš”í•œ í•­ëª©ë§Œ ì„ íƒí•˜ì„¸ìš”.")

            # ì´ë ¥ì„œ ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
            if "resume_data" not in st.session_state:
                st.session_state.resume_data = {}

            resume_major = st.selectbox(
                "ì „ê³µ ê³„ì—´",
                options=RESUME_MAJOR_OPTIONS,
                index=0,
                key="resume_major"
            )
            resume_exp = st.selectbox(
                "ì´ ê²½ë ¥ ê¸°ê°„",
                options=RESUME_EXPERIENCE_OPTIONS,
                index=0,
                key="resume_exp"
            )
            resume_gap = st.selectbox(
                "ê²½ë ¥ ê³µë°±",
                options=RESUME_GAP_OPTIONS,
                index=0,
                key="resume_gap"
            )

            st.markdown("**í•´ë‹¹ í•­ëª© ì²´í¬**")
            has_short_career = st.checkbox("1ë…„ ë¯¸ë§Œ í‡´ì‚¬ ê²½ë ¥", key="has_short_career")
            has_overseas = st.checkbox("í•´ì™¸ ê²½í—˜ (ì–´í•™ì—°ìˆ˜/êµí™˜í•™ìƒ/ì›Œí™€)", key="has_overseas")
            has_service_exp = st.checkbox("ì„œë¹„ìŠ¤ì§ ê²½í—˜", key="has_service_exp")
            has_language_cert = st.checkbox("ì–´í•™ ìê²©ì¦ (í† ìµ/í† í”Œ/HSK ë“±)", key="has_language_cert")
            major_mismatch = st.checkbox("ì „ê³µ-ì§ë¬´ ì—°ê´€ì„± ë‚®ìŒ", key="major_mismatch")

            # ì„¸ì…˜ì— ì €ì¥
            st.session_state.resume_data = {
                "major": resume_major,
                "experience": resume_exp,
                "gap": resume_gap,
                "has_short_career": has_short_career,
                "has_overseas": has_overseas,
                "has_service_exp": has_service_exp,
                "has_language_cert": has_language_cert,
                "major_mismatch": major_mismatch,
            }

with colR:
    st.subheader("STEP 2) ìê¸°ì†Œê°œì„œ ì…ë ¥ (ìµœëŒ€ 5,000ì)")
    add_col, _sp = st.columns([1, 3])
    with add_col:
        if st.button("+ ë¬¸í•­ ì¶”ê°€", use_container_width=True):
            st.session_state.qa_sets.append({"prompt": "", "answer": ""})

    to_delete: Optional[int] = None
    for i, item in enumerate(st.session_state.qa_sets):
        top = st.columns([1, 1, 1, 1, 1])
        with top[0]:
            st.write(f"ë¬¸í•­ {i+1}")
        with top[4]:
            if st.button("ë¬¸í•­ ì‚­ì œ", key=f"del_{i}", use_container_width=True, disabled=(len(st.session_state.qa_sets) <= 1)):
                to_delete = i

        st.session_state.qa_sets[i]["prompt"] = st.text_input(
            "[ë¬¸í•­ ì§ˆë¬¸]",
            value=st.session_state.qa_sets[i].get("prompt", ""),
            key=f"prompt_{i}",
        )
        st.session_state.qa_sets[i]["answer"] = st.text_area(
            "[ë‚´ ë‹µë³€]",
            value=st.session_state.qa_sets[i].get("answer", ""),
            height=180,
            key=f"answer_{i}",
        )

    if to_delete is not None and len(st.session_state.qa_sets) > 1:
        st.session_state.qa_sets.pop(to_delete)
        st.rerun()

essay = "\n".join([normalize_ws(x.get("answer", "")) for x in st.session_state.qa_sets if normalize_ws(x.get("answer", ""))]).strip()

current_hash = hashlib.sha256(essay.encode("utf-8", errors="ignore")).hexdigest() if essay else ""
if st.session_state.last_essay_hash and current_hash and current_hash != st.session_state.last_essay_hash:
    st.session_state.question_version = 1
    st.session_state.questions = {}
    st.session_state.answers = {"q1": "", "q2": "", "q3": "", "q4": "", "q5": ""}
    st.session_state.feedback = {"q1": "", "q2": "", "q3": "", "q4": "", "q5": ""}
    st.session_state.report_text = ""
    st.session_state.q3_generated = False  # Q3 ë™ì  ìƒì„± í”Œë˜ê·¸ ë¦¬ì…‹
    st.session_state._regen_last_request_id = ""
    st.session_state._regen_in_progress = False
    st.session_state.ps_srcai_sentences = []
    st.session_state.ps_srcai_roles = []
    st.session_state.ps_srcai_selected_actions = []
    st.session_state.ps_srcai_selected_results = []
    st.session_state.ps_srcai_selected_questionables = []

st.session_state.last_essay_hash = current_hash
_srcai_apply_to_qa_sets(st.session_state.get("qa_sets", []))

st.divider()

# ë‚¨ì€ ì‚¬ìš©ëŸ‰ í‘œì‹œ

# ë¦¬ì…‹ + ì¬ë¶„ì„ ë²„íŠ¼ ìƒë‹¨ì— ë°°ì¹˜
reset_col, reanalyze_col, info_col = st.columns([1, 1, 2])
with reset_col:
    reset = st.button("ë¦¬ì…‹", use_container_width=True)
with reanalyze_col:
    reanalyze = st.button("ğŸ”„ ì¬ë¶„ì„", use_container_width=True, help="ìì†Œì„œë¥¼ ë‹¤ì‹œ ë¶„ì„í•˜ì—¬ ìƒˆë¡œìš´ ì§ˆë¬¸ì„ ìƒì„±í•©ë‹ˆë‹¤")
with info_col:
    st.caption("ğŸ‘‡ ì•„ë˜ 'STEP 4~5) ë©´ì ‘ ì§ˆë¬¸' ì˜†ì˜ **ì§ˆë¬¸ ìƒì„±** ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")

if reset:
    st.session_state.question_version = 1
    st.session_state.questions = {}
    st.session_state.answers = {"q1": "", "q2": "", "q3": "", "q4": "", "q5": ""}
    st.session_state.feedback = {"q1": "", "q2": "", "q3": "", "q4": "", "q5": ""}
    st.session_state.report_text = ""
    st.session_state.q3_generated = False  # Q3 ë™ì  ìƒì„± í”Œë˜ê·¸ ë¦¬ì…‹
    st.session_state._regen_last_request_id = ""
    st.session_state._regen_in_progress = False
    st.rerun()

if reanalyze:
    # ì‚¬ìš©ëŸ‰ ì²´í¬
    # ============================================
    # v4.0: ì¬ë¶„ì„ ì‹œ ìºì‹œ ì™„ì „ ì‚­ì œ + ìë™ ì§ˆë¬¸ ìƒì„±
    # ============================================
    print("[ì¬ë¶„ì„] ìºì‹œ ì‚­ì œ ì‹œì‘...")

    # 1. FLYREADY ìºì‹œ ì‚­ì œ (ëª¨ë“  ê´€ë ¨ í‚¤)
    keys_to_delete = [k for k in list(st.session_state.keys()) if k.startswith("_flyready") or k.startswith("_sharp")]
    for k in keys_to_delete:
        print(f"[ì¬ë¶„ì„] ì‚­ì œ: {k}")
        del st.session_state[k]

    # 2. ì§ˆë¬¸ ìºì‹œë„ ì‚­ì œ
    st.session_state.questions = {}
    st.session_state.q3_generated = False

    # 3. ìš”ì²­ ID ë¦¬ì…‹ (ìƒˆ ì§ˆë¬¸ ìƒì„± íŠ¸ë¦¬ê±°)
    st.session_state._regen_last_request_id = ""

    # 4. ìë™ìœ¼ë¡œ ì§ˆë¬¸ ìƒì„±
    if essay.strip() and st.session_state.get("qa_sets"):
        try:
            print("[ì¬ë¶„ì„] ìƒˆ ì§ˆë¬¸ ìƒì„± ì‹œì‘...")
            _get_or_build_item_analysis_cache(st.session_state.qa_sets, current_hash)
            generated_qs = safe_generate_questions(
                essay=essay,
                airline=airline,
                version=st.session_state.question_version,
            )
            st.session_state.questions = generated_qs
            print(f"[ì¬ë¶„ì„] ì™„ë£Œ! Q2: {generated_qs.get('q2', {}).get('question', '')[:50]}...")
        except Exception as e:
            print(f"[ì¬ë¶„ì„] ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()

    st.toast("ğŸ”„ ìì†Œì„œ ì¬ë¶„ì„ ì™„ë£Œ!")
    st.rerun()

st.subheader("STEP 3) ìê¸°ì†Œê°œì„œ ë¶„ì„")
if essay.strip():
    # ============================================
    # CLOVA ë‹¨ë… ëª¨ë“œ: OpenAI LLM ìƒíƒœ ì²´í¬ ì œê±°
    # ============================================

    # ìºì‹œ ë¹Œë“œ (CLOVA íŒŒì´í”„ë¼ì¸ì€ generate_questionsì—ì„œ ì‹¤í–‰ë¨)
    cache = _get_or_build_item_analysis_cache(st.session_state.qa_sets, current_hash)

    # CLOVA íŒŒì´í”„ë¼ì¸ ìƒíƒœ í™•ì¸
    pipeline_cache_key = f"_sharp_pipeline_{current_hash}"
    clova_pipeline_ready = pipeline_cache_key in st.session_state and st.session_state.get(pipeline_cache_key)

    # CLOVA ë‹¨ë… ëª¨ë“œ: ë¡œì»¬ ë¶„ì„ ê²°ê³¼ ì‚¬ìš©
    display_keywords = cache.get("agg_risk", []) or cache.get("agg_keywords", []) or []
    display_evidence = cache.get("agg_evidence", []) or []

    # ì¤‘ë³µ ì œê±°
    display_keywords = list(dict.fromkeys(display_keywords))[:10]
    display_evidence = list(dict.fromkeys(display_evidence))[:8]

    st.markdown("### ìì†Œì„œ ë¶„ì„ ê²°ê³¼")
    st.caption("ë©´ì ‘ê´€ì´ ì£¼ëª©í•  í‚¤ì›Œë“œì™€ ì§ˆë¬¸ì˜ ê·¼ê±°ê°€ ë  ë¬¸ì¥ì…ë‹ˆë‹¤.")

    c2, c3 = st.columns([1, 1])
    with c2:
        st.markdown("**í•µì‹¬ í‚¤ì›Œë“œ** (ë©´ì ‘ê´€ì´ ë¬¼ì–´ë³¼ í¬ì¸íŠ¸)")
        if display_keywords:
            # í‚¤ì›Œë“œë¥¼ íƒœê·¸ì²˜ëŸ¼ í‘œì‹œ
            keyword_html = " ".join([f"<span style='background-color:#e8f4f8;padding:4px 10px;border-radius:12px;margin:2px;display:inline-block;font-size:14px;'>{kw}</span>" for kw in display_keywords])
            st.markdown(keyword_html, unsafe_allow_html=True)
        else:
            st.write("-")
    with c3:
        st.markdown("**ê·¼ê±° ë¬¸ì¥** (ì§ˆë¬¸ì˜ ì¶œì²˜ê°€ ë˜ëŠ” ë¬¸ì¥)")
        if display_evidence:
            for i, s in enumerate(display_evidence, 1):
                # ë¬¸ì¥ì´ ë„ˆë¬´ ê¸¸ë©´ ìë¥´ê¸°
                display_s = s[:100] + "..." if len(s) > 100 else s
                st.markdown(f"<div style='background-color:#f9f9f9;padding:8px 12px;border-left:3px solid #4a90d9;margin:4px 0;font-size:13px;'>{i}. {display_s}</div>", unsafe_allow_html=True)
        else:
            st.write("-")

    # CLOVA ë‹¨ë… ëª¨ë“œ: ê°„ë‹¨í•œ ìƒíƒœ ì•ˆë‚´
    if essay.strip():
        if clova_pipeline_ready:
            st.success("âœ… ìì†Œì„œ ë¶„ì„ ì™„ë£Œ! 'ì§ˆë¬¸ ìƒì„±/ê°±ì‹ ' ë²„íŠ¼ì„ ëˆŒëŸ¬ ì§ˆë¬¸ì„ ìƒì„±í•˜ì„¸ìš”.")
        else:
            st.info("ğŸ‘† 'ì§ˆë¬¸ ìƒì„±/ê°±ì‹ ' ë²„íŠ¼ì„ ëˆŒëŸ¬ CLOVA ë¶„ì„ì„ ì‹œì‘í•˜ì„¸ìš”.")

    # ê°œë°œììš© ë””ë²„ê·¸ ì •ë³´ (ì ‘ì„ ìˆ˜ ìˆê²Œ)
    with st.expander("[DEV] CLOVA íŒŒì´í”„ë¼ì¸ ìƒíƒœ", expanded=False):
        st.caption(f"CLOVA ì¤€ë¹„={clova_pipeline_ready}")

st.divider()

# STEP 4~5 í—¤ë”ì™€ ì§ˆë¬¸ìƒì„± ë²„íŠ¼ì„ ë‚˜ë€íˆ ë°°ì¹˜
step_header_col, step_btn_col = st.columns([2.5, 1.5])
with step_header_col:
    st.subheader("STEP 4~5) ë©´ì ‘ ì§ˆë¬¸ 5ë¬¸í•­")
with step_btn_col:
    regen_step45 = st.button("ì§ˆë¬¸ ìƒì„±/ê°±ì‹ ", key="regen_step45", use_container_width=True, type="primary", disabled=(not essay.strip()))

# STEP 4~5 ë²„íŠ¼ í´ë¦­ ì‹œ ì „ì²´ ë¡œì§ ì‹¤í–‰
if regen_step45:
    # ì‚¬ìš©ëŸ‰ ì²´í¬
    current_ver = st.session_state.question_version
    st.session_state.question_version = (current_ver % 6) + 1
    request_id = hashlib.sha256(
        f"{current_hash}|{airline}|{st.session_state.question_version}".encode("utf-8", errors="ignore")
    ).hexdigest()

    should_run = (
        (not st.session_state._regen_in_progress)
        and (request_id != st.session_state._regen_last_request_id)
    )

    if should_run:
        st.session_state._regen_in_progress = True
        try:
            # CLOVA ë‹¨ë… ëª¨ë“œ: ìºì‹œ ë¹Œë“œ
            _get_or_build_item_analysis_cache(st.session_state.qa_sets, current_hash)
            generated_qs = safe_generate_questions(
                essay=essay,
                airline=airline,
                version=st.session_state.question_version,
            )
            st.session_state.questions = generated_qs
            logger.debug(f"[DEBUG SAVE] Saved Q2 to session_state: {generated_qs.get('q2', {}).get('question', '')[:50]}...")

            # ì´ë ¥ì„œ ê¸°ë°˜ ì§ˆë¬¸ ìƒì„± (Basic/Pro ìš”ê¸ˆì œ)
            resume_data = st.session_state.get("resume_data", {})
            num_resume_q = plan_config.get("resume_questions", 0)
            if num_resume_q > 0 and resume_data:
                resume_qs = generate_resume_questions(resume_data, airline, num_resume_q)
                if resume_qs:
                    st.session_state.resume_questions = resume_qs
                    st.session_state.resume_data = {}
                else:
                    st.session_state.resume_questions = []
            else:
                st.session_state.resume_questions = []

            st.session_state._regen_last_request_id = request_id
        finally:
            st.session_state._regen_in_progress = False

    st.session_state.answers = {"q1": "", "q2": "", "q3": "", "q4": "", "q5": "", "r1": "", "r2": ""}
    st.session_state.feedback = {"q1": "", "q2": "", "q3": "", "q4": "", "q5": "", "r1": "", "r2": ""}
    st.session_state.report_text = ""
    st.session_state.q3_generated = False
    st.rerun()

if st.session_state.questions:
    logger.debug(f"[DEBUG DISPLAY] Displaying Q2 from session_state: {st.session_state.questions.get('q2', {}).get('question', '')[:50]}...")
    st.caption(f"í˜„ì¬ ë²„ì „: {st.session_state.question_version} / ë²„íŠ¼ì„ ëˆŒëŸ¬ ë‹¤ë¥¸ ê°ë„ì˜ ì§ˆë¬¸ì„ ë°›ì•„ë³´ì„¸ìš”")
    for key in ["q1", "q2", "q3", "q4", "q5"]:
        qobj = st.session_state.questions.get(key, {})
        qtype = qobj.get("type", "")
        qtext = qobj.get("question", "")
        basis = qobj.get("basis", "")
        anchor = qobj.get("anchor", "")

        with st.expander(f"{key.upper()} Â· {qtype}", expanded=True):
            # Q3 íŠ¹ë³„ ì²˜ë¦¬: Q2 ë‹µë³€ ê¸°ë°˜ ë™ì  ìƒì„±
            if key == "q3":
                q2_answer = st.session_state.answers.get("q2", "").strip()
                q2_question = st.session_state.questions.get("q2", {}).get("question", "")

                # Q3ê°€ ë™ì  ìƒì„±ë˜ì§€ ì•Šì•˜ê³ , Q2 ë‹µë³€ì´ ì¶©ë¶„í•˜ë©´ ìƒì„± ë²„íŠ¼ í‘œì‹œ
                if not st.session_state.get("q3_generated", False):
                    if q2_answer and len(q2_answer) >= 20:
                        st.info("Q2 ë‹µë³€ì„ ê¸°ë°˜ìœ¼ë¡œ ë§ì¶¤í˜• ê¼¬ë¦¬ì§ˆë¬¸ì„ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                        if st.button("Q3 ê¼¬ë¦¬ì§ˆë¬¸ ìƒì„±", key="gen_q3_btn", type="primary"):
                            with st.spinner("Q3 ìƒì„± ì¤‘..."):
                                new_q3 = generate_q3_from_answer(q2_question, q2_answer)
                                if new_q3:
                                    st.session_state.questions["q3"]["question"] = new_q3
                                    st.session_state.questions["q3"]["basis"] = "Q2 ë‹µë³€ ê¸°ë°˜ AI ë™ì  ìƒì„±"
                                    st.session_state.q3_generated = True
                                    st.rerun()
                                else:
                                    st.error("Q3 ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ê¸°ì¡´ ì§ˆë¬¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                    elif q2_answer:
                        st.warning("Q2 ë‹µë³€ì„ ì¡°ê¸ˆ ë” ì…ë ¥í•˜ì„¸ìš”. (ìµœì†Œ 20ì)")
                    else:
                        st.warning("Q2 ë‹µë³€ì„ ë¨¼ì € ì…ë ¥í•˜ì„¸ìš”. ë‹µë³€ì„ ê¸°ë°˜ìœ¼ë¡œ ë§ì¶¤í˜• ê¼¬ë¦¬ì§ˆë¬¸ì„ ìƒì„±í•©ë‹ˆë‹¤.")

            st.markdown(f"**ì§ˆë¬¸**\n\n{qtext}")
            if basis:
                st.markdown(f"**ìƒì„± ê·¼ê±°**\n\n- {basis}")
            elif anchor:
                st.markdown(f"**ìƒì„± ê·¼ê±°**\n\n- ì´ ì§ˆë¬¸ì€ ìê¸°ì†Œê°œì„œì˜ \"{anchor}\" í‘œí˜„ì„ ê¸°ë°˜ìœ¼ë¡œ ìƒì„±ë¨")

            st.session_state.answers[key] = st.text_area(
                f"{key.upper()} ë‹µë³€",
                value=st.session_state.answers.get(key, ""),
                height=120,
                key=f"ans_{key}",
            )

    # ì´ë ¥ì„œ ê¸°ë°˜ ì§ˆë¬¸ í‘œì‹œ (Basic/Pro ìš”ê¸ˆì œ)
    resume_questions = st.session_state.get("resume_questions", [])
    if resume_questions:
        st.divider()
        st.subheader("ì´ë ¥ì„œ ê¸°ë°˜ ì¶”ê°€ ì§ˆë¬¸")
        st.caption("ì´ë ¥ì„œ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìƒì„±ëœ ì˜ˆìƒ ì§ˆë¬¸ì…ë‹ˆë‹¤.")

        for i, rq in enumerate(resume_questions, 1):
            rkey = f"r{i}"
            with st.expander(f"R{i} Â· ì´ë ¥ì„œ ê²€ì¦", expanded=True):
                st.markdown(f"**ì§ˆë¬¸**\n\n{rq}")
                st.markdown("**ìƒì„± ê·¼ê±°**\n\n- ì´ë ¥ì„œ ì •ë³´ ê¸°ë°˜ AI ìƒì„±")

                if rkey not in st.session_state.answers:
                    st.session_state.answers[rkey] = ""
                st.session_state.answers[rkey] = st.text_area(
                    f"R{i} ë‹µë³€",
                    value=st.session_state.answers.get(rkey, ""),
                    height=120,
                    key=f"ans_{rkey}",
                )

    st.divider()
    st.subheader("STEP 6~7) ì‚¬ì‹¤ ê¸°ë°˜ í”¼ë“œë°± & ë¦¬í¬íŠ¸")

    fb_btn, rep_btn = st.columns([1, 1])
    with fb_btn:
        do_feedback = st.button("í”¼ë“œë°± ìƒì„±", use_container_width=True)
    with rep_btn:
        do_report = st.button("ë¦¬í¬íŠ¸ ìƒì„±", use_container_width=True)

    cache = _get_or_build_item_analysis_cache(st.session_state.qa_sets, current_hash)
    evidence = cache.get("agg_evidence", []) or []
    risk_keywords = cache.get("agg_risk", []) or []
    if not risk_keywords:
        risk_keywords = cache.get("agg_keywords", []) or []
    items = split_essay_items(essay)

    if do_feedback:
        # Q2 ë‹µë³€ ë¯¸ë¦¬ ê°€ì ¸ì˜¤ê¸° (Q3 ë¶„ì„ì— í•„ìš”)
        q2_answer_for_q3 = st.session_state.answers.get("q2", "")

        # ì§„í–‰ë¥  í‘œì‹œ
        progress_bar = st.progress(0)
        status_text = st.empty()

        question_labels = {"q1": "Q1 ê³µí†µ", "q2": "Q2 ê²€ì¦", "q3": "Q3 ê¼¬ë¦¬", "q4": "Q4 ì¸ì¬ìƒ", "q5": "Q5 ëŒë°œ"}
        keys_to_process = ["q1", "q2", "q3", "q4", "q5"]

        for idx, key in enumerate(keys_to_process):
            status_text.text(f"ë¶„ì„ ì¤‘... {question_labels.get(key, key)}")
            progress_bar.progress((idx + 1) / len(keys_to_process))

            qobj = st.session_state.questions.get(key, {})
            qtype = qobj.get("type", "")
            qtext = qobj.get("question", "")
            anchor = qobj.get("anchor", "")
            basis = qobj.get("basis", "")
            ans = st.session_state.answers.get(key, "")

            # ë‹µë³€ì´ ì—†ìœ¼ë©´ ìŠ¤í‚µ
            if not ans or not ans.strip():
                st.session_state.feedback[key] = ""
                continue

            # ìƒˆ í”¼ë“œë°± ë¶„ì„ ì‹œìŠ¤í…œ ì‚¬ìš©
            # Set B (Q2, Q3)ì—ëŠ” ê³µê²© í¬ì¸íŠ¸ì™€ ìì†Œì„œ ì»¨í…ìŠ¤íŠ¸ ì „ë‹¬
            if key in ("q2", "q3"):
                # ê³µê²© í¬ì¸íŠ¸: anchor ë˜ëŠ” basisì—ì„œ ì¶”ì¶œ
                attack_point = anchor if anchor else basis
                # ìì†Œì„œ ì»¨í…ìŠ¤íŠ¸: ì „ì²´ ì—ì„¸ì´ ìš”ì•½
                essay_context = "\n".join(evidence[:5]) if evidence else ""
                # Q3ì˜ ê²½ìš° Q2 ë‹µë³€ë„ ì „ë‹¬
                q2_ans = q2_answer_for_q3 if key == "q3" else ""

                st.session_state.feedback[key] = analyze_answer(
                    question_key=key,
                    question_text=qtext,
                    user_answer=ans,
                    question_type=qtype,
                    attack_point=attack_point,
                    essay_context=essay_context,
                    q2_answer=q2_ans,
                    airline=airline,
                )
            else:
                # Set A (Q1, Q4, Q5)
                st.session_state.feedback[key] = analyze_answer(
                    question_key=key,
                    question_text=qtext,
                    user_answer=ans,
                    question_type=qtype,
                    airline=airline,
                )

        progress_bar.empty()
        status_text.empty()
        st.success("í”¼ë“œë°± ë¶„ì„ ì™„ë£Œ!")

    if any(v.strip() for v in st.session_state.feedback.values()):
        st.markdown("### í”¼ë“œë°± ë¶„ì„ ê²°ê³¼")
        question_labels = {"q1": "Q1 ê³µí†µì§ˆë¬¸", "q2": "Q2 ìì†Œì„œê²€ì¦", "q3": "Q3 ê¼¬ë¦¬ì§ˆë¬¸", "q4": "Q4 ì¸ì¬ìƒ", "q5": "Q5 ëŒë°œì§ˆë¬¸"}

        for key in ["q1", "q2", "q3", "q4", "q5"]:
            fb = st.session_state.feedback.get(key, "")
            if fb.strip():
                with st.expander(f"{question_labels.get(key, key.upper())} í”¼ë“œë°±", expanded=True):
                    # í”¼ë“œë°± ë‚´ìš©ì„ ì„¹ì…˜ë³„ë¡œ í•˜ì´ë¼ì´íŠ¸
                    for line in fb.split("\n"):
                        if line.startswith("[") and line.endswith("]"):
                            # ì„¹ì…˜ í—¤ë”
                            st.markdown(f"**{line}**")
                        elif line.strip().startswith("-") or line.strip().startswith("â€¢"):
                            st.markdown(line)
                        elif line.strip():
                            st.markdown(line)

    if do_report:
        lines = []
        lines.append("AI ë©´ì ‘ ì½”ì¹­ ê²°ê³¼ ë¦¬í¬íŠ¸ (MVP)")
        lines.append("=" * 40)
        lines.append(f"ì§€ì› í•­ê³µì‚¬: {airline} / íƒ€ì…: {atype}")
        lines.append("ë©´ì ‘ ì–¸ì–´: í•œêµ­ì–´")
        lines.append(f"ì§ˆë¬¸ ë²„ì „(ì„¸ì…˜ ì¹´ìš´í„°): {st.session_state.question_version}")
        lines.append("")
        lines.append("[ìê¸°ì†Œê°œì„œ ìš”ì•½]")
        lines.append("- ë¬¸í•­ ìˆ˜: " + str(len(items)))
        lines.append("- í•µì‹¬ í‚¤ì›Œë“œ: " + (", ".join(risk_keywords[:14]) if risk_keywords else "-"))
        lines.append("- ê·¼ê±° ë¬¸ì¥(ìƒìœ„):")
        for s in evidence[:6]:
            lines.append(f"  * {s}")
        prompts = [normalize_ws(x.get("prompt", "")) for x in st.session_state.qa_sets if normalize_ws(x.get("prompt", ""))]
        if prompts:
            lines.append("- ì‚¬ìš©ì ì…ë ¥ ë¬¸í•­ ì§ˆë¬¸:")
            for p in prompts[:20]:
                lines.append(f"  * {p}")

        lines.append("")
        lines.append("[ë©´ì ‘ ì§ˆë¬¸/ë‹µë³€/í”¼ë“œë°±]")
        for key in ["q1", "q2", "q3", "q4", "q5"]:
            qobj = st.session_state.questions.get(key, {})
            qtype = qobj.get("type", "")
            qtext = qobj.get("question", "")
            basis = qobj.get("basis", "")
            ans = st.session_state.answers.get(key, "").strip()
            fb = st.session_state.feedback.get(key, "").strip()

            lines.append("-" * 40)
            lines.append(f"{key.upper()} ({qtype})")
            lines.append(f"Q: {qtext}")
            if basis:
                lines.append(f"ê·¼ê±°: {basis}")
            lines.append("A: " + (ans if ans else "(ë¯¸ì…ë ¥)"))
            if fb:
                lines.append("Feedback:")
                for l in fb.splitlines():
                    lines.append(f"  {l}")
            else:
                lines.append("Feedback: (ë¯¸ìƒì„±)")
        lines.append("")
        st.session_state.report_text = "\n".join(lines)

    if st.session_state.report_text.strip():
        st.markdown("### ê²°ê³¼ ë¦¬í¬íŠ¸")
        st.code(st.session_state.report_text, language="text")
        st.download_button(
            label="TXT ë‹¤ìš´ë¡œë“œ",
            data=st.session_state.report_text.encode("utf-8"),
            file_name="interview_report.txt",
            mime="text/plain",
            use_container_width=True,
        )

else:
    st.warning("ìê¸°ì†Œê°œì„œë¥¼ ì…ë ¥í•œ ë’¤ 'ì§ˆë¬¸ ìƒì„±/ê°±ì‹  (STEP 4)'ì„ ëˆŒëŸ¬ ì§ˆë¬¸ì„ ìƒì„±í•˜ì„¸ìš”.")

st.markdown('</div>', unsafe_allow_html=True)
