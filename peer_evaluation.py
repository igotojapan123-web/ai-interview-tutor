# peer_evaluation.py
# FlyReady Lab ë©€í‹°í”Œë ˆì´ì–´ ëª¨ì˜ë©´ì ‘ ë™ë£Œ í‰ê°€ ì‹œìŠ¤í…œ

import json
import uuid
from pathlib import Path
from datetime import datetime, timedelta
from typing import Optional, List, Dict, Any
from collections import Counter, defaultdict

from logging_config import get_logger

# ë¡œê±° ì„¤ì •
logger = get_logger(__name__)

# ë°ì´í„° ë””ë ‰í† ë¦¬ ì„¤ì •
DATA_DIR = Path(__file__).parent / "data"
DATA_DIR.mkdir(exist_ok=True)

EVALUATIONS_FILE = DATA_DIR / "peer_evaluations.json"
REACTIONS_FILE = DATA_DIR / "peer_reactions.json"

# ========================================
# í‰ê°€ ê¸°ì¤€ ì •ì˜
# ========================================

EVALUATION_CRITERIA = {
    "content": {
        "name": "ë‹µë³€ ë‚´ìš©",
        "description": "ë‹µë³€ì˜ ë‚´ìš©ì  ì¸¡ë©´ì„ í‰ê°€í•©ë‹ˆë‹¤.",
        "subcriteria": {
            "specificity": {
                "name": "êµ¬ì²´ì„±",
                "description": "ë‹µë³€ì´ êµ¬ì²´ì ì¸ ì˜ˆì‹œì™€ ê²½í—˜ì„ í¬í•¨í•˜ëŠ”ì§€ í‰ê°€í•©ë‹ˆë‹¤.",
                "weight": 0.35
            },
            "relevance": {
                "name": "ê´€ë ¨ì„±",
                "description": "ë‹µë³€ì´ ì§ˆë¬¸ì˜ ì˜ë„ì— ë§ê²Œ ì‘ì„±ë˜ì—ˆëŠ”ì§€ í‰ê°€í•©ë‹ˆë‹¤.",
                "weight": 0.35
            },
            "logic": {
                "name": "ë…¼ë¦¬ì„±",
                "description": "ë‹µë³€ì˜ ë…¼ë¦¬ì  íë¦„ê³¼ ì¼ê´€ì„±ì„ í‰ê°€í•©ë‹ˆë‹¤.",
                "weight": 0.30
            }
        },
        "max_score": 25
    },
    "delivery": {
        "name": "ì „ë‹¬ë ¥",
        "description": "ë‹µë³€ì„ ì „ë‹¬í•˜ëŠ” ë°©ì‹ì„ í‰ê°€í•©ë‹ˆë‹¤.",
        "subcriteria": {
            "voice": {
                "name": "ëª©ì†Œë¦¬",
                "description": "ë°œìŒ, ì†ë„, í¬ê¸°ê°€ ì ì ˆí•œì§€ í‰ê°€í•©ë‹ˆë‹¤.",
                "weight": 0.35
            },
            "confidence": {
                "name": "ìì‹ ê°",
                "description": "ìì‹ ê° ìˆëŠ” íƒœë„ë¡œ ë‹µë³€í•˜ëŠ”ì§€ í‰ê°€í•©ë‹ˆë‹¤.",
                "weight": 0.35
            },
            "eye_contact": {
                "name": "ì‹œì„ ì²˜ë¦¬",
                "description": "ì‹œì„ ì´ ìì—°ìŠ¤ëŸ½ê³  ì ì ˆí•œì§€ í‰ê°€í•©ë‹ˆë‹¤.",
                "weight": 0.30
            }
        },
        "max_score": 25
    },
    "attitude": {
        "name": "íƒœë„",
        "description": "ë©´ì ‘ íƒœë„ë¥¼ í‰ê°€í•©ë‹ˆë‹¤.",
        "subcriteria": {
            "service_mind": {
                "name": "ì„œë¹„ìŠ¤ ë§ˆì¸ë“œ",
                "description": "ì„œë¹„ìŠ¤ ì •ì‹ ì´ ëŠê»´ì§€ëŠ”ì§€ í‰ê°€í•©ë‹ˆë‹¤.",
                "weight": 0.35
            },
            "enthusiasm": {
                "name": "ì—´ì •",
                "description": "ì—´ì •ê³¼ ì˜ì§€ê°€ ëŠê»´ì§€ëŠ”ì§€ í‰ê°€í•©ë‹ˆë‹¤.",
                "weight": 0.35
            },
            "authenticity": {
                "name": "ì§„ì •ì„±",
                "description": "ì§„ì†”í•˜ê³  ì§„ì •ì„± ìˆê²Œ ë‹µë³€í•˜ëŠ”ì§€ í‰ê°€í•©ë‹ˆë‹¤.",
                "weight": 0.30
            }
        },
        "max_score": 25
    },
    "structure": {
        "name": "êµ¬ì¡°",
        "description": "ë‹µë³€ì˜ êµ¬ì¡°ì  ì¸¡ë©´ì„ í‰ê°€í•©ë‹ˆë‹¤.",
        "subcriteria": {
            "opening": {
                "name": "ë‘ê´„ì‹",
                "description": "í•µì‹¬ ë©”ì‹œì§€ë¥¼ ë¨¼ì € ì „ë‹¬í•˜ëŠ”ì§€ í‰ê°€í•©ë‹ˆë‹¤.",
                "weight": 0.35
            },
            "star_method": {
                "name": "STAR ê¸°ë²•",
                "description": "ìƒí™©-ê³¼ì œ-í–‰ë™-ê²°ê³¼ êµ¬ì¡°ë¡œ ë‹µë³€í•˜ëŠ”ì§€ í‰ê°€í•©ë‹ˆë‹¤.",
                "weight": 0.35
            },
            "conclusion": {
                "name": "ê²°ë¡ ",
                "description": "ëª…í™•í•œ ê²°ë¡ ìœ¼ë¡œ ë§ˆë¬´ë¦¬í•˜ëŠ”ì§€ í‰ê°€í•©ë‹ˆë‹¤.",
                "weight": 0.30
            }
        },
        "max_score": 25
    }
}

# ì ìˆ˜ ê°€ì¤‘ì¹˜
SCORE_WEIGHTS = {
    "ai": 0.6,
    "peer": 0.4
}

# ë¹ ë¥¸ ë°˜ì‘ ì •ì˜
REACTIONS = {
    "like": {
        "emoji": "ğŸ‘",
        "name": "ì¢‹ì•„ìš”",
        "description": "ì¢‹ì€ ë‹µë³€ì´ì—ìš”"
    },
    "amazing": {
        "emoji": "ğŸ‘",
        "name": "ëŒ€ë‹¨í•´ìš”",
        "description": "ì •ë§ ì¸ìƒì ì¸ ë‹µë³€ì´ì—ìš”"
    },
    "confident": {
        "emoji": "ğŸ’ª",
        "name": "ìì‹ ê° ìˆì–´ìš”",
        "description": "ìì‹ ê°ì´ ëŠê»´ì ¸ìš”"
    },
    "creative": {
        "emoji": "ğŸ’¡",
        "name": "ì°½ì˜ì ì´ì—ìš”",
        "description": "ì°½ì˜ì ì¸ ë‹µë³€ì´ì—ìš”"
    },
    "empathy": {
        "emoji": "â¤ï¸",
        "name": "ê³µê°í•´ìš”",
        "description": "ê³µê°ì´ ê°€ëŠ” ë‹µë³€ì´ì—ìš”"
    }
}

# í‰ê°€ì ë°°ì§€
EVALUATOR_BADGES = {
    "helpful_evaluator": {
        "name": "ë„ì›€ì´ ë˜ëŠ” í‰ê°€ì",
        "description": "10íšŒ ì´ìƒ ë„ì›€ì´ ë˜ëŠ” í‰ê°€ë¥¼ ì‘ì„±í–ˆìŠµë‹ˆë‹¤.",
        "requirement": 10
    },
    "consistent_evaluator": {
        "name": "ì¼ê´€ëœ í‰ê°€ì",
        "description": "í‰ê°€ í¸ì°¨ê°€ ì ê³  ì¼ê´€ì„± ìˆëŠ” í‰ê°€ë¥¼ í•©ë‹ˆë‹¤.",
        "requirement": 0.8  # ì‹ ë¢°ë„ 80% ì´ìƒ
    },
    "active_evaluator": {
        "name": "ì ê·¹ì ì¸ í‰ê°€ì",
        "description": "50íšŒ ì´ìƒ ë™ë£Œ í‰ê°€ì— ì°¸ì—¬í–ˆìŠµë‹ˆë‹¤.",
        "requirement": 50
    },
    "master_evaluator": {
        "name": "ë§ˆìŠ¤í„° í‰ê°€ì",
        "description": "100íšŒ ì´ìƒ í‰ê°€í•˜ê³  ì‹ ë¢°ë„ 90% ì´ìƒì…ë‹ˆë‹¤.",
        "requirement": {"count": 100, "reliability": 0.9}
    }
}


# ========================================
# ë°ì´í„° ê´€ë¦¬
# ========================================

def _load_evaluations() -> List[Dict[str, Any]]:
    """í‰ê°€ ë°ì´í„° ë¡œë“œ"""
    try:
        if EVALUATIONS_FILE.exists():
            with open(EVALUATIONS_FILE, "r", encoding="utf-8") as f:
                return json.load(f)
    except (json.JSONDecodeError, FileNotFoundError) as e:
        logger.warning(f"í‰ê°€ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
    return []


def _save_evaluations(evaluations: List[Dict[str, Any]]) -> None:
    """í‰ê°€ ë°ì´í„° ì €ì¥"""
    with open(EVALUATIONS_FILE, "w", encoding="utf-8") as f:
        json.dump(evaluations, f, ensure_ascii=False, indent=2)


def _load_reactions() -> List[Dict[str, Any]]:
    """ë°˜ì‘ ë°ì´í„° ë¡œë“œ"""
    try:
        if REACTIONS_FILE.exists():
            with open(REACTIONS_FILE, "r", encoding="utf-8") as f:
                return json.load(f)
    except (json.JSONDecodeError, FileNotFoundError) as e:
        logger.warning(f"ë°˜ì‘ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
    return []


def _save_reactions(reactions: List[Dict[str, Any]]) -> None:
    """ë°˜ì‘ ë°ì´í„° ì €ì¥"""
    with open(REACTIONS_FILE, "w", encoding="utf-8") as f:
        json.dump(reactions, f, ensure_ascii=False, indent=2)


# ========================================
# í‰ê°€ ì œì¶œ ë° ê²€ì¦
# ========================================

def get_evaluation_template() -> Dict[str, Any]:
    """í‰ê°€ ì–‘ì‹ í…œí”Œë¦¿ ë°˜í™˜

    Returns:
        ë¹ˆ í‰ê°€ ì–‘ì‹ í…œí”Œë¦¿
    """
    template = {
        "scores": {},
        "feedback": "",
        "criteria_info": EVALUATION_CRITERIA
    }

    for category, info in EVALUATION_CRITERIA.items():
        template["scores"][category] = {
            "total": 0,
            "subcriteria": {}
        }
        for sub_key in info["subcriteria"]:
            template["scores"][category]["subcriteria"][sub_key] = 0

    logger.debug("í‰ê°€ í…œí”Œë¦¿ ìƒì„±")
    return template


def can_evaluate(evaluator_id: str, target_id: str, room_id: str) -> Dict[str, Any]:
    """í‰ê°€ ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸

    Args:
        evaluator_id: í‰ê°€ì ID
        target_id: í‰ê°€ ëŒ€ìƒ ID
        room_id: ë°© ID

    Returns:
        í‰ê°€ ê°€ëŠ¥ ì—¬ë¶€ ë° ì´ìœ 
    """
    # ìê¸° ìì‹  í‰ê°€ ë¶ˆê°€
    if evaluator_id == target_id:
        logger.debug(f"ìê¸° í‰ê°€ ì‹œë„: {evaluator_id}")
        return {
            "can_evaluate": False,
            "reason": "ìê¸° ìì‹ ì€ í‰ê°€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        }

    # ê°™ì€ ë°© ì°¸ê°€ ì—¬ë¶€ í™•ì¸ (room_managerì—ì„œ í™•ì¸)
    try:
        from room_manager import get_room
        room = get_room(room_id)

        if not room:
            return {
                "can_evaluate": False,
                "reason": "ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ë°©ì…ë‹ˆë‹¤."
            }

        participant_ids = [p["user_id"] for p in room.get("participants", [])]

        if evaluator_id not in participant_ids:
            return {
                "can_evaluate": False,
                "reason": "í•´ë‹¹ ë°©ì— ì°¸ê°€í•˜ì§€ ì•Šì€ ì‚¬ìš©ìì…ë‹ˆë‹¤."
            }

        if target_id not in participant_ids:
            return {
                "can_evaluate": False,
                "reason": "í‰ê°€ ëŒ€ìƒì´ í•´ë‹¹ ë°©ì— ì°¸ê°€í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            }

    except ImportError:
        logger.warning("room_manager ëª¨ë“ˆì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    # ì´ë¯¸ í‰ê°€í–ˆëŠ”ì§€ í™•ì¸
    evaluations = _load_evaluations()
    already_evaluated = any(
        e["evaluator_id"] == evaluator_id and
        e["target_id"] == target_id and
        e["room_id"] == room_id
        for e in evaluations
    )

    if already_evaluated:
        return {
            "can_evaluate": False,
            "reason": "ì´ë¯¸ ì´ ì°¸ê°€ìë¥¼ í‰ê°€í–ˆìŠµë‹ˆë‹¤."
        }

    return {
        "can_evaluate": True,
        "reason": "í‰ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
    }


def submit_peer_evaluation(
    evaluator_id: str,
    target_id: str,
    room_id: str,
    question_idx: int,
    scores: Dict[str, Any],
    feedback: str
) -> Dict[str, Any]:
    """ë™ë£Œ í‰ê°€ ì œì¶œ

    Args:
        evaluator_id: í‰ê°€ì ID
        target_id: í‰ê°€ ëŒ€ìƒ ID
        room_id: ë°© ID
        question_idx: ì§ˆë¬¸ ì¸ë±ìŠ¤
        scores: í‰ê°€ ì ìˆ˜ ë”•ì…”ë„ˆë¦¬
        feedback: í”¼ë“œë°± í…ìŠ¤íŠ¸

    Returns:
        ì œì¶œëœ í‰ê°€ ì •ë³´
    """
    # í‰ê°€ ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
    check = can_evaluate(evaluator_id, target_id, room_id)
    if not check["can_evaluate"]:
        logger.warning(f"í‰ê°€ ë¶ˆê°€: {check['reason']}")
        raise ValueError(check["reason"])

    # ì ìˆ˜ ê²€ì¦
    total_score = 0
    validated_scores = {}

    for category, info in EVALUATION_CRITERIA.items():
        if category not in scores:
            raise ValueError(f"'{info['name']}' ì¹´í…Œê³ ë¦¬ ì ìˆ˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.")

        category_score = scores[category]
        max_score = info["max_score"]

        # ì´ì  ë˜ëŠ” í•˜ìœ„ í•­ëª© ì ìˆ˜ í™•ì¸
        if isinstance(category_score, dict) and "subcriteria" in category_score:
            # í•˜ìœ„ í•­ëª©ë³„ ì ìˆ˜
            sub_total = 0
            validated_scores[category] = {"subcriteria": {}}

            for sub_key, sub_info in info["subcriteria"].items():
                sub_score = category_score["subcriteria"].get(sub_key, 0)
                sub_max = max_score * sub_info["weight"]

                if not 0 <= sub_score <= sub_max:
                    raise ValueError(
                        f"'{sub_info['name']}' ì ìˆ˜ëŠ” 0~{sub_max:.1f} ì‚¬ì´ì—¬ì•¼ í•©ë‹ˆë‹¤."
                    )

                validated_scores[category]["subcriteria"][sub_key] = sub_score
                sub_total += sub_score

            validated_scores[category]["total"] = sub_total
            total_score += sub_total
        else:
            # ì¹´í…Œê³ ë¦¬ ì´ì ë§Œ
            cat_score = category_score if isinstance(category_score, (int, float)) else category_score.get("total", 0)

            if not 0 <= cat_score <= max_score:
                raise ValueError(
                    f"'{info['name']}' ì ìˆ˜ëŠ” 0~{max_score} ì‚¬ì´ì—¬ì•¼ í•©ë‹ˆë‹¤."
                )

            validated_scores[category] = {"total": cat_score, "subcriteria": {}}
            total_score += cat_score

    # í‰ê°€ ID ìƒì„±
    evaluation_id = str(uuid.uuid4())[:8]

    evaluation = {
        "evaluation_id": evaluation_id,
        "evaluator_id": evaluator_id,
        "target_id": target_id,
        "room_id": room_id,
        "question_idx": question_idx,
        "scores": validated_scores,
        "total_score": round(total_score, 1),
        "feedback": feedback,
        "created_at": datetime.now().isoformat(),
        "flagged": False,
        "flag_reason": None
    }

    evaluations = _load_evaluations()
    evaluations.append(evaluation)
    _save_evaluations(evaluations)

    logger.info(f"ë™ë£Œ í‰ê°€ ì œì¶œ: {evaluator_id} -> {target_id} (ì ìˆ˜: {total_score})")

    return evaluation


# ========================================
# í‰ê°€ ì¡°íšŒ
# ========================================

def get_evaluations_received(
    user_id: str,
    room_id: Optional[str] = None
) -> List[Dict[str, Any]]:
    """ë°›ì€ í‰ê°€ ì¡°íšŒ

    Args:
        user_id: ì‚¬ìš©ì ID
        room_id: ë°© ID (ì„ íƒ, íŠ¹ì • ë°©ë§Œ ì¡°íšŒ)

    Returns:
        ë°›ì€ í‰ê°€ ëª©ë¡
    """
    evaluations = _load_evaluations()

    received = [e for e in evaluations if e["target_id"] == user_id]

    if room_id:
        received = [e for e in received if e["room_id"] == room_id]

    # ìµœì‹ ìˆœ ì •ë ¬
    received.sort(key=lambda x: x["created_at"], reverse=True)

    logger.debug(f"ë°›ì€ í‰ê°€ ì¡°íšŒ: {user_id}, {len(received)}ê°œ")
    return received


def get_evaluations_given(
    user_id: str,
    room_id: Optional[str] = None
) -> List[Dict[str, Any]]:
    """ì‘ì„±í•œ í‰ê°€ ì¡°íšŒ

    Args:
        user_id: ì‚¬ìš©ì ID
        room_id: ë°© ID (ì„ íƒ, íŠ¹ì • ë°©ë§Œ ì¡°íšŒ)

    Returns:
        ì‘ì„±í•œ í‰ê°€ ëª©ë¡
    """
    evaluations = _load_evaluations()

    given = [e for e in evaluations if e["evaluator_id"] == user_id]

    if room_id:
        given = [e for e in given if e["room_id"] == room_id]

    # ìµœì‹ ìˆœ ì •ë ¬
    given.sort(key=lambda x: x["created_at"], reverse=True)

    logger.debug(f"ì‘ì„±í•œ í‰ê°€ ì¡°íšŒ: {user_id}, {len(given)}ê°œ")
    return given


def get_peer_feedback_summary(user_id: str) -> Dict[str, Any]:
    """ë™ë£Œ í”¼ë“œë°± ìš”ì•½

    Args:
        user_id: ì‚¬ìš©ì ID

    Returns:
        í”¼ë“œë°± ìš”ì•½ ì •ë³´
    """
    evaluations = get_evaluations_received(user_id)

    if not evaluations:
        return {
            "user_id": user_id,
            "total_evaluations": 0,
            "message": "ì•„ì§ ë°›ì€ í‰ê°€ê°€ ì—†ìŠµë‹ˆë‹¤."
        }

    # ì¹´í…Œê³ ë¦¬ë³„ í‰ê·  ì ìˆ˜
    category_scores = defaultdict(list)
    total_scores = []
    all_feedbacks = []

    for e in evaluations:
        total_scores.append(e["total_score"])
        all_feedbacks.append(e.get("feedback", ""))

        for category, score_data in e["scores"].items():
            category_scores[category].append(score_data["total"])

    category_averages = {}
    for category, scores in category_scores.items():
        avg = sum(scores) / len(scores)
        category_averages[category] = {
            "name": EVALUATION_CRITERIA[category]["name"],
            "average": round(avg, 1),
            "max_score": EVALUATION_CRITERIA[category]["max_score"],
            "evaluation_count": len(scores)
        }

    # ê°€ì¥ ë†’ì€/ë‚®ì€ ì¹´í…Œê³ ë¦¬
    sorted_categories = sorted(
        category_averages.items(),
        key=lambda x: x[1]["average"] / x[1]["max_score"],
        reverse=True
    )

    return {
        "user_id": user_id,
        "total_evaluations": len(evaluations),
        "overall_average": round(sum(total_scores) / len(total_scores), 1),
        "max_possible_score": 100,
        "category_averages": category_averages,
        "strongest_category": sorted_categories[0][0] if sorted_categories else None,
        "weakest_category": sorted_categories[-1][0] if sorted_categories else None,
        "recent_feedbacks": [f for f in all_feedbacks if f][:5]
    }


# ========================================
# ì ìˆ˜ ì§‘ê³„
# ========================================

def calculate_peer_score(user_id: str, room_id: str) -> Dict[str, Any]:
    """ë™ë£Œ í‰ê°€ í‰ê·  ì ìˆ˜ ê³„ì‚°

    Args:
        user_id: ì‚¬ìš©ì ID
        room_id: ë°© ID

    Returns:
        í‰ê·  ì ìˆ˜ ì •ë³´
    """
    evaluations = get_evaluations_received(user_id, room_id)

    if not evaluations:
        return {
            "user_id": user_id,
            "room_id": room_id,
            "peer_score": 0,
            "evaluation_count": 0,
            "message": "ë™ë£Œ í‰ê°€ê°€ ì—†ìŠµë‹ˆë‹¤."
        }

    # ì‹ ë¢°ë„ ì¡°ì •ëœ ì ìˆ˜ ê³„ì‚°
    adjusted_scores = []

    for e in evaluations:
        # í”Œë˜ê·¸ëœ í‰ê°€ëŠ” ì œì™¸
        if e.get("flagged"):
            continue

        # í‰ê°€ì ì‹ ë¢°ë„ ê°€ì¤‘ì¹˜ ì ìš©
        evaluator_reliability = calculate_evaluator_reliability(e["evaluator_id"])
        reliability = evaluator_reliability.get("reliability", 1.0)

        adjusted_score = e["total_score"] * reliability
        adjusted_scores.append(adjusted_score)

    if not adjusted_scores:
        return {
            "user_id": user_id,
            "room_id": room_id,
            "peer_score": 0,
            "evaluation_count": 0,
            "message": "ìœ íš¨í•œ í‰ê°€ê°€ ì—†ìŠµë‹ˆë‹¤."
        }

    average_score = sum(adjusted_scores) / len(adjusted_scores)

    return {
        "user_id": user_id,
        "room_id": room_id,
        "peer_score": round(average_score, 1),
        "raw_average": round(sum(e["total_score"] for e in evaluations if not e.get("flagged")) / len([e for e in evaluations if not e.get("flagged")]), 1),
        "evaluation_count": len(adjusted_scores),
        "total_evaluations": len(evaluations)
    }


def get_combined_score(
    user_id: str,
    room_id: str,
    ai_score: float
) -> Dict[str, Any]:
    """AIì™€ ë™ë£Œ í‰ê°€ ê²°í•© ì ìˆ˜ ê³„ì‚°

    Args:
        user_id: ì‚¬ìš©ì ID
        room_id: ë°© ID
        ai_score: AI í‰ê°€ ì ìˆ˜ (0-100)

    Returns:
        ê²°í•© ì ìˆ˜ ì •ë³´
    """
    peer_result = calculate_peer_score(user_id, room_id)
    peer_score = peer_result.get("peer_score", 0)

    # ê°€ì¤‘ì¹˜ ì ìš©
    ai_weight = SCORE_WEIGHTS["ai"]
    peer_weight = SCORE_WEIGHTS["peer"]

    # ë™ë£Œ í‰ê°€ê°€ ì—†ìœ¼ë©´ AI ì ìˆ˜ë§Œ ì‚¬ìš©
    if peer_result.get("evaluation_count", 0) == 0:
        combined = ai_score
        peer_weight_used = 0
        ai_weight_used = 1.0
    else:
        combined = (ai_score * ai_weight) + (peer_score * peer_weight)
        ai_weight_used = ai_weight
        peer_weight_used = peer_weight

    return {
        "user_id": user_id,
        "room_id": room_id,
        "combined_score": round(combined, 1),
        "ai_score": round(ai_score, 1),
        "ai_weight": ai_weight_used,
        "peer_score": round(peer_score, 1),
        "peer_weight": peer_weight_used,
        "peer_evaluation_count": peer_result.get("evaluation_count", 0),
        "score_breakdown": {
            "ai_contribution": round(ai_score * ai_weight_used, 1),
            "peer_contribution": round(peer_score * peer_weight_used, 1)
        }
    }


# ========================================
# í”¼ë“œë°± ë¶„ì„
# ========================================

def analyze_feedback_patterns(user_id: str) -> Dict[str, Any]:
    """ë°›ì€ í”¼ë“œë°± íŒ¨í„´ ë¶„ì„

    Args:
        user_id: ì‚¬ìš©ì ID

    Returns:
        í”¼ë“œë°± íŒ¨í„´ ë¶„ì„ ê²°ê³¼
    """
    evaluations = get_evaluations_received(user_id)

    if len(evaluations) < 3:
        return {
            "user_id": user_id,
            "message": "ì¶©ë¶„í•œ í”¼ë“œë°±ì´ ëª¨ì´ë©´ íŒ¨í„´ì„ ë¶„ì„í•´ ë“œë¦½ë‹ˆë‹¤.",
            "evaluations_needed": 3 - len(evaluations)
        }

    # ì¹´í…Œê³ ë¦¬ë³„ ì ìˆ˜ ì¶”ì´
    category_trends = defaultdict(list)
    for e in sorted(evaluations, key=lambda x: x["created_at"]):
        for category, score_data in e["scores"].items():
            category_trends[category].append({
                "score": score_data["total"],
                "date": e["created_at"][:10]
            })

    # ê°œì„ /í•˜ë½ ì¶”ì„¸ ë¶„ì„
    trends = {}
    for category, scores in category_trends.items():
        if len(scores) >= 3:
            recent = sum(s["score"] for s in scores[-3:]) / 3
            older = sum(s["score"] for s in scores[:3]) / 3

            if recent > older * 1.1:
                trend = "improving"
                trend_label = "ê°œì„  ì¤‘"
            elif recent < older * 0.9:
                trend = "declining"
                trend_label = "í•˜ë½ ì¤‘"
            else:
                trend = "stable"
                trend_label = "ìœ ì§€ ì¤‘"

            trends[category] = {
                "trend": trend,
                "trend_label": trend_label,
                "recent_average": round(recent, 1),
                "older_average": round(older, 1)
            }

    # ì¼ê´€ë˜ê²Œ ë†’ì€/ë‚®ì€ ì¹´í…Œê³ ë¦¬
    consistent_high = []
    consistent_low = []

    for category, info in EVALUATION_CRITERIA.items():
        scores = [e["scores"][category]["total"] for e in evaluations]
        avg = sum(scores) / len(scores)
        ratio = avg / info["max_score"]

        if ratio >= 0.8:
            consistent_high.append({
                "category": category,
                "name": info["name"],
                "average_ratio": round(ratio, 2)
            })
        elif ratio <= 0.5:
            consistent_low.append({
                "category": category,
                "name": info["name"],
                "average_ratio": round(ratio, 2)
            })

    return {
        "user_id": user_id,
        "total_evaluations": len(evaluations),
        "category_trends": trends,
        "consistent_strengths": consistent_high,
        "consistent_weaknesses": consistent_low,
        "analysis_date": datetime.now().isoformat()
    }


def get_improvement_suggestions_from_peers(user_id: str) -> Dict[str, Any]:
    """ë™ë£Œ í”¼ë“œë°±ì—ì„œ ê°œì„  ì œì•ˆ ì¶”ì¶œ

    Args:
        user_id: ì‚¬ìš©ì ID

    Returns:
        ê°œì„  ì œì•ˆ ëª©ë¡
    """
    evaluations = get_evaluations_received(user_id)

    if not evaluations:
        return {
            "user_id": user_id,
            "suggestions": [],
            "message": "ì•„ì§ ë°›ì€ í‰ê°€ê°€ ì—†ìŠµë‹ˆë‹¤."
        }

    # í”¼ë“œë°± í…ìŠ¤íŠ¸ ìˆ˜ì§‘
    feedbacks = [e.get("feedback", "") for e in evaluations if e.get("feedback")]

    # ë‚®ì€ ì ìˆ˜ ì¹´í…Œê³ ë¦¬ ì‹ë³„
    category_scores = defaultdict(list)
    for e in evaluations:
        for category, score_data in e["scores"].items():
            max_score = EVALUATION_CRITERIA[category]["max_score"]
            ratio = score_data["total"] / max_score
            category_scores[category].append(ratio)

    weak_categories = []
    for category, ratios in category_scores.items():
        avg_ratio = sum(ratios) / len(ratios)
        if avg_ratio < 0.6:
            weak_categories.append({
                "category": category,
                "name": EVALUATION_CRITERIA[category]["name"],
                "average_ratio": round(avg_ratio, 2),
                "suggestion": _generate_improvement_suggestion(category, avg_ratio)
            })

    # ì •ë ¬ (ê°€ì¥ ê°œì„ ì´ í•„ìš”í•œ ìˆœ)
    weak_categories.sort(key=lambda x: x["average_ratio"])

    return {
        "user_id": user_id,
        "suggestions": weak_categories[:3],  # ìƒìœ„ 3ê°œ
        "feedback_excerpts": feedbacks[:5],
        "total_evaluations": len(evaluations)
    }


def _generate_improvement_suggestion(category: str, ratio: float) -> str:
    """ì¹´í…Œê³ ë¦¬ë³„ ê°œì„  ì œì•ˆ ìƒì„±"""
    suggestions = {
        "content": {
            "low": "ë‹µë³€ì— êµ¬ì²´ì ì¸ ê²½í—˜ê³¼ ì˜ˆì‹œë¥¼ ë” ì¶”ê°€í•´ ë³´ì„¸ìš”. STAR ê¸°ë²•ì„ í™œìš©í•˜ë©´ ì¢‹ìŠµë‹ˆë‹¤.",
            "medium": "ë‹µë³€ì˜ ê´€ë ¨ì„±ì„ ë†’ì´ê¸° ìœ„í•´ ì§ˆë¬¸ì˜ í•µì‹¬ì„ íŒŒì•…í•˜ê³  ê·¸ì— ë§ëŠ” ë‚´ìš©ì„ ì¤€ë¹„í•´ ë³´ì„¸ìš”."
        },
        "delivery": {
            "low": "ë°œìŒê³¼ ë§í•˜ëŠ” ì†ë„ë¥¼ ì—°ìŠµí•´ ë³´ì„¸ìš”. ë…¹ìŒí•´ì„œ ë“¤ì–´ë³´ë©´ ë„ì›€ì´ ë©ë‹ˆë‹¤.",
            "medium": "ìì‹ ê° ìˆê²Œ ë§í•˜ë˜, ì ì ˆí•œ ëˆˆ ë§ì¶¤ì„ ìœ ì§€í•´ ë³´ì„¸ìš”."
        },
        "attitude": {
            "low": "ì„œë¹„ìŠ¤ ë§ˆì¸ë“œë¥¼ ë³´ì—¬ì£¼ëŠ” ê²½í—˜ê³¼ ê°€ì¹˜ê´€ì„ ë‹µë³€ì— ë…¹ì—¬ë³´ì„¸ìš”.",
            "medium": "ì—´ì •ê³¼ ì§„ì •ì„±ì´ ëŠê»´ì§€ë„ë¡ ë³¸ì¸ë§Œì˜ ì´ì•¼ê¸°ë¥¼ ë‹´ì•„ë³´ì„¸ìš”."
        },
        "structure": {
            "low": "ë‘ê´„ì‹ìœ¼ë¡œ í•µì‹¬ ë©”ì‹œì§€ë¥¼ ë¨¼ì € ë§í•˜ê³ , ëª…í™•í•œ ê²°ë¡ ìœ¼ë¡œ ë§ˆë¬´ë¦¬í•´ ë³´ì„¸ìš”.",
            "medium": "STAR ê¸°ë²•(ìƒí™©-ê³¼ì œ-í–‰ë™-ê²°ê³¼)ì„ ì ìš©í•´ ë‹µë³€ì„ êµ¬ì¡°í™”í•´ ë³´ì„¸ìš”."
        }
    }

    level = "low" if ratio < 0.4 else "medium"
    return suggestions.get(category, {}).get(level, "ê¾¸ì¤€íˆ ì—°ìŠµí•˜ë©´ ì¢‹ì•„ì§ˆ ê±°ì˜ˆìš”!")


def get_common_strengths(user_id: str) -> Dict[str, Any]:
    """ë™ë£Œë“¤ì´ ê³µí†µìœ¼ë¡œ ì¹­ì°¬í•œ ì  ì¶”ì¶œ

    Args:
        user_id: ì‚¬ìš©ì ID

    Returns:
        ê³µí†µ ê°•ì  ëª©ë¡
    """
    evaluations = get_evaluations_received(user_id)

    if not evaluations:
        return {
            "user_id": user_id,
            "strengths": [],
            "message": "ì•„ì§ ë°›ì€ í‰ê°€ê°€ ì—†ìŠµë‹ˆë‹¤."
        }

    # ë†’ì€ ì ìˆ˜ë¥¼ ë°›ì€ ì¹´í…Œê³ ë¦¬/í•˜ìœ„í•­ëª© ìˆ˜ì§‘
    high_scores = defaultdict(int)

    for e in evaluations:
        for category, score_data in e["scores"].items():
            max_score = EVALUATION_CRITERIA[category]["max_score"]
            ratio = score_data["total"] / max_score

            if ratio >= 0.8:
                high_scores[category] += 1

            # í•˜ìœ„ í•­ëª©ë„ í™•ì¸
            for sub_key, sub_score in score_data.get("subcriteria", {}).items():
                sub_info = EVALUATION_CRITERIA[category]["subcriteria"].get(sub_key, {})
                sub_max = max_score * sub_info.get("weight", 0.33)

                if sub_max > 0 and sub_score / sub_max >= 0.8:
                    high_scores[f"{category}.{sub_key}"] += 1

    # ê°€ì¥ ë§ì´ ì¹­ì°¬ë°›ì€ ìˆœ ì •ë ¬
    sorted_strengths = sorted(high_scores.items(), key=lambda x: x[1], reverse=True)

    strengths = []
    for key, count in sorted_strengths[:5]:
        if "." in key:
            cat, sub = key.split(".")
            name = EVALUATION_CRITERIA[cat]["subcriteria"][sub]["name"]
            parent_name = EVALUATION_CRITERIA[cat]["name"]
            strengths.append({
                "category": cat,
                "subcategory": sub,
                "name": f"{parent_name} - {name}",
                "praise_count": count,
                "ratio": round(count / len(evaluations), 2)
            })
        else:
            name = EVALUATION_CRITERIA[key]["name"]
            strengths.append({
                "category": key,
                "name": name,
                "praise_count": count,
                "ratio": round(count / len(evaluations), 2)
            })

    return {
        "user_id": user_id,
        "strengths": strengths,
        "total_evaluations": len(evaluations)
    }


def get_common_weaknesses(user_id: str) -> Dict[str, Any]:
    """ë™ë£Œë“¤ì´ ê³µí†µìœ¼ë¡œ ì§€ì í•œ ì  ì¶”ì¶œ

    Args:
        user_id: ì‚¬ìš©ì ID

    Returns:
        ê³µí†µ ì•½ì  ëª©ë¡
    """
    evaluations = get_evaluations_received(user_id)

    if not evaluations:
        return {
            "user_id": user_id,
            "weaknesses": [],
            "message": "ì•„ì§ ë°›ì€ í‰ê°€ê°€ ì—†ìŠµë‹ˆë‹¤."
        }

    # ë‚®ì€ ì ìˆ˜ë¥¼ ë°›ì€ ì¹´í…Œê³ ë¦¬/í•˜ìœ„í•­ëª© ìˆ˜ì§‘
    low_scores = defaultdict(int)

    for e in evaluations:
        for category, score_data in e["scores"].items():
            max_score = EVALUATION_CRITERIA[category]["max_score"]
            ratio = score_data["total"] / max_score

            if ratio <= 0.5:
                low_scores[category] += 1

            # í•˜ìœ„ í•­ëª©ë„ í™•ì¸
            for sub_key, sub_score in score_data.get("subcriteria", {}).items():
                sub_info = EVALUATION_CRITERIA[category]["subcriteria"].get(sub_key, {})
                sub_max = max_score * sub_info.get("weight", 0.33)

                if sub_max > 0 and sub_score / sub_max <= 0.5:
                    low_scores[f"{category}.{sub_key}"] += 1

    # ê°€ì¥ ë§ì´ ì§€ì ë°›ì€ ìˆœ ì •ë ¬
    sorted_weaknesses = sorted(low_scores.items(), key=lambda x: x[1], reverse=True)

    weaknesses = []
    for key, count in sorted_weaknesses[:5]:
        if "." in key:
            cat, sub = key.split(".")
            name = EVALUATION_CRITERIA[cat]["subcriteria"][sub]["name"]
            parent_name = EVALUATION_CRITERIA[cat]["name"]
            suggestion = _generate_improvement_suggestion(cat, 0.5)
            weaknesses.append({
                "category": cat,
                "subcategory": sub,
                "name": f"{parent_name} - {name}",
                "criticism_count": count,
                "ratio": round(count / len(evaluations), 2),
                "suggestion": suggestion
            })
        else:
            name = EVALUATION_CRITERIA[key]["name"]
            suggestion = _generate_improvement_suggestion(key, 0.5)
            weaknesses.append({
                "category": key,
                "name": name,
                "criticism_count": count,
                "ratio": round(count / len(evaluations), 2),
                "suggestion": suggestion
            })

    return {
        "user_id": user_id,
        "weaknesses": weaknesses,
        "total_evaluations": len(evaluations)
    }


# ========================================
# í‰ê°€ í’ˆì§ˆ ê´€ë¦¬
# ========================================

def flag_unfair_evaluation(evaluation_id: str, reason: str) -> Dict[str, Any]:
    """ë¶ˆê³µì • í‰ê°€ ì‹ ê³ 

    Args:
        evaluation_id: í‰ê°€ ID
        reason: ì‹ ê³  ì‚¬ìœ 

    Returns:
        ì‹ ê³  ê²°ê³¼
    """
    evaluations = _load_evaluations()

    for e in evaluations:
        if e["evaluation_id"] == evaluation_id:
            e["flagged"] = True
            e["flag_reason"] = reason
            e["flagged_at"] = datetime.now().isoformat()

            _save_evaluations(evaluations)

            logger.info(f"í‰ê°€ ì‹ ê³ ë¨: {evaluation_id}, ì‚¬ìœ : {reason}")

            return {
                "evaluation_id": evaluation_id,
                "flagged": True,
                "reason": reason,
                "message": "ì‹ ê³ ê°€ ì ‘ìˆ˜ë˜ì—ˆìŠµë‹ˆë‹¤. ê²€í†  í›„ ì¡°ì¹˜í•˜ê² ìŠµë‹ˆë‹¤."
            }

    raise ValueError("í•´ë‹¹ í‰ê°€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")


def calculate_evaluator_reliability(user_id: str) -> Dict[str, Any]:
    """í‰ê°€ì ì‹ ë¢°ë„ ê³„ì‚°

    Args:
        user_id: í‰ê°€ì ID

    Returns:
        ì‹ ë¢°ë„ ì •ë³´
    """
    evaluations = _load_evaluations()

    # ì´ ì‚¬ìš©ìê°€ ì‘ì„±í•œ í‰ê°€ë“¤
    given = [e for e in evaluations if e["evaluator_id"] == user_id]

    if len(given) < 5:
        return {
            "user_id": user_id,
            "reliability": 1.0,  # ê¸°ë³¸ ì‹ ë¢°ë„
            "evaluation_count": len(given),
            "message": "í‰ê°€ ì´ë ¥ì´ ë¶€ì¡±í•˜ì—¬ ê¸°ë³¸ ì‹ ë¢°ë„ê°€ ì ìš©ë©ë‹ˆë‹¤."
        }

    # ì‹ ë¢°ë„ ìš”ì†Œë“¤
    factors = []

    # 1. ì‹ ê³ ëœ í‰ê°€ ë¹„ìœ¨ (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)
    flagged_count = sum(1 for e in given if e.get("flagged"))
    flag_ratio = flagged_count / len(given)
    flag_score = 1 - (flag_ratio * 2)  # ì‹ ê³  ë¹„ìœ¨ì— íŒ¨ë„í‹°
    factors.append(max(0, flag_score))

    # 2. í‰ê°€ ì ìˆ˜ ë¶„í¬ (ë„ˆë¬´ ê·¹ë‹¨ì ì´ë©´ ê°ì )
    scores = [e["total_score"] for e in given]
    avg_score = sum(scores) / len(scores)

    # í‘œì¤€í¸ì°¨ ê³„ì‚°
    variance = sum((s - avg_score) ** 2 for s in scores) / len(scores)
    std_dev = variance ** 0.5

    # ì ì ˆí•œ ë¶„í¬ (í‘œì¤€í¸ì°¨ 10-20 ì •ë„ê°€ ì´ìƒì )
    if 10 <= std_dev <= 20:
        distribution_score = 1.0
    elif std_dev < 5 or std_dev > 30:
        distribution_score = 0.7
    else:
        distribution_score = 0.85
    factors.append(distribution_score)

    # 3. í‰ê°€ ì™„ì„±ë„ (í”¼ë“œë°± ì‘ì„± ë¹„ìœ¨)
    feedback_count = sum(1 for e in given if e.get("feedback"))
    feedback_ratio = feedback_count / len(given)
    factors.append(feedback_ratio)

    # 4. í™œë™ëŸ‰ ë³´ë„ˆìŠ¤
    activity_bonus = min(1.0, len(given) / 50)  # 50ê°œ ì´ìƒì´ë©´ 1.0
    factors.append(0.5 + activity_bonus * 0.5)

    # ì¢…í•© ì‹ ë¢°ë„ (0~1)
    reliability = sum(factors) / len(factors)
    reliability = round(max(0.3, min(1.0, reliability)), 2)

    return {
        "user_id": user_id,
        "reliability": reliability,
        "evaluation_count": len(given),
        "flagged_evaluations": flagged_count,
        "feedback_rate": round(feedback_ratio, 2),
        "score_std_dev": round(std_dev, 1),
        "reliability_level": _get_reliability_level(reliability)
    }


def _get_reliability_level(reliability: float) -> str:
    """ì‹ ë¢°ë„ ë ˆë²¨ ë¬¸ìì—´ ë°˜í™˜"""
    if reliability >= 0.9:
        return "ë§¤ìš° ë†’ìŒ"
    elif reliability >= 0.75:
        return "ë†’ìŒ"
    elif reliability >= 0.5:
        return "ë³´í†µ"
    else:
        return "ë‚®ìŒ"


def adjust_for_bias(evaluations: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """í‰ê°€ í¸í–¥ ì¡°ì •

    Args:
        evaluations: í‰ê°€ ëª©ë¡

    Returns:
        í¸í–¥ ì¡°ì •ëœ í‰ê°€ ëª©ë¡
    """
    if len(evaluations) < 3:
        return evaluations

    # í‰ê°€ìë³„ í‰ê·  ì ìˆ˜ ê³„ì‚° (ê´€ëŒ€/ì—„ê²© í¸í–¥ íŒŒì•…)
    evaluator_averages = defaultdict(list)
    for e in evaluations:
        evaluator_averages[e["evaluator_id"]].append(e["total_score"])

    # ì „ì²´ í‰ê· 
    all_scores = [e["total_score"] for e in evaluations]
    global_average = sum(all_scores) / len(all_scores)

    # ì¡°ì •ëœ í‰ê°€ ëª©ë¡ ìƒì„±
    adjusted = []
    for e in evaluations:
        evaluator_id = e["evaluator_id"]
        evaluator_scores = evaluator_averages[evaluator_id]

        if len(evaluator_scores) >= 3:
            evaluator_avg = sum(evaluator_scores) / len(evaluator_scores)

            # í¸í–¥ ë³´ì •
            bias = evaluator_avg - global_average
            adjusted_score = e["total_score"] - (bias * 0.5)  # 50% ë³´ì •
            adjusted_score = max(0, min(100, adjusted_score))
        else:
            adjusted_score = e["total_score"]

        adjusted_eval = e.copy()
        adjusted_eval["adjusted_score"] = round(adjusted_score, 1)
        adjusted_eval["original_score"] = e["total_score"]
        adjusted.append(adjusted_eval)

    logger.debug(f"í¸í–¥ ì¡°ì • ì™„ë£Œ: {len(adjusted)}ê°œ í‰ê°€")
    return adjusted


# ========================================
# ë¦¬ë”ë³´ë“œ
# ========================================

def get_peer_review_leaderboard(period: str = "weekly") -> List[Dict[str, Any]]:
    """ë™ë£Œ í‰ê°€ ê¸°ì¤€ ë¦¬ë”ë³´ë“œ

    Args:
        period: ê¸°ê°„ (weekly, monthly, all)

    Returns:
        ë¦¬ë”ë³´ë“œ ëª©ë¡
    """
    evaluations = _load_evaluations()

    # ê¸°ê°„ í•„í„°
    now = datetime.now()
    if period == "weekly":
        cutoff = now - timedelta(days=7)
    elif period == "monthly":
        cutoff = now - timedelta(days=30)
    else:
        cutoff = datetime.min

    filtered = [
        e for e in evaluations
        if datetime.fromisoformat(e["created_at"]) >= cutoff
        and not e.get("flagged")
    ]

    # ì‚¬ìš©ìë³„ í‰ê·  ì ìˆ˜ ê³„ì‚°
    user_scores = defaultdict(list)
    for e in filtered:
        user_scores[e["target_id"]].append(e["total_score"])

    # ë¦¬ë”ë³´ë“œ ìƒì„± (ìµœì†Œ 2ê°œ í‰ê°€ í•„ìš”)
    leaderboard = []
    for user_id, scores in user_scores.items():
        if len(scores) >= 2:
            avg_score = sum(scores) / len(scores)
            leaderboard.append({
                "user_id": user_id,
                "average_score": round(avg_score, 1),
                "evaluation_count": len(scores),
                "highest_score": max(scores),
                "lowest_score": min(scores)
            })

    # í‰ê·  ì ìˆ˜ ìˆœ ì •ë ¬
    leaderboard.sort(key=lambda x: x["average_score"], reverse=True)

    # ìˆœìœ„ ë¶€ì—¬
    for i, entry in enumerate(leaderboard):
        entry["rank"] = i + 1

    logger.debug(f"ë™ë£Œ í‰ê°€ ë¦¬ë”ë³´ë“œ ì¡°íšŒ: {period}, {len(leaderboard)}ëª…")

    return {
        "period": period,
        "period_label": {"weekly": "ì£¼ê°„", "monthly": "ì›”ê°„", "all": "ì „ì²´"}[period],
        "leaderboard": leaderboard[:20],  # ìƒìœ„ 20ëª…
        "generated_at": now.isoformat()
    }


def get_best_evaluator_leaderboard() -> Dict[str, Any]:
    """ìš°ìˆ˜ í‰ê°€ì ë¦¬ë”ë³´ë“œ

    Returns:
        í‰ê°€ì ë¦¬ë”ë³´ë“œ
    """
    evaluations = _load_evaluations()

    # í‰ê°€ìë³„ í†µê³„
    evaluator_stats = defaultdict(lambda: {
        "total_evaluations": 0,
        "feedback_count": 0,
        "flagged_count": 0
    })

    for e in evaluations:
        evaluator_id = e["evaluator_id"]
        evaluator_stats[evaluator_id]["total_evaluations"] += 1

        if e.get("feedback"):
            evaluator_stats[evaluator_id]["feedback_count"] += 1

        if e.get("flagged"):
            evaluator_stats[evaluator_id]["flagged_count"] += 1

    # ì ìˆ˜ ê³„ì‚° ë° ë¦¬ë”ë³´ë“œ ìƒì„±
    leaderboard = []
    for user_id, stats in evaluator_stats.items():
        if stats["total_evaluations"] < 5:
            continue

        reliability = calculate_evaluator_reliability(user_id)["reliability"]
        feedback_rate = stats["feedback_count"] / stats["total_evaluations"]

        # ì¢…í•© ì ìˆ˜ (í™œë™ëŸ‰ + ì‹ ë¢°ë„ + í”¼ë“œë°± ë¹„ìœ¨)
        activity_score = min(stats["total_evaluations"], 100) / 100 * 30
        reliability_score = reliability * 40
        feedback_score = feedback_rate * 30

        total_score = activity_score + reliability_score + feedback_score

        leaderboard.append({
            "user_id": user_id,
            "total_score": round(total_score, 1),
            "total_evaluations": stats["total_evaluations"],
            "reliability": reliability,
            "feedback_rate": round(feedback_rate, 2),
            "badges": _get_evaluator_badges(user_id, stats, reliability)
        })

    # ì ìˆ˜ ìˆœ ì •ë ¬
    leaderboard.sort(key=lambda x: x["total_score"], reverse=True)

    # ìˆœìœ„ ë¶€ì—¬
    for i, entry in enumerate(leaderboard):
        entry["rank"] = i + 1

    return {
        "leaderboard": leaderboard[:20],
        "generated_at": datetime.now().isoformat()
    }


def _get_evaluator_badges(
    user_id: str,
    stats: Dict[str, Any],
    reliability: float
) -> List[Dict[str, str]]:
    """í‰ê°€ì ë°°ì§€ ëª©ë¡ ë°˜í™˜"""
    badges = []

    if stats["total_evaluations"] >= EVALUATOR_BADGES["active_evaluator"]["requirement"]:
        badges.append({
            "id": "active_evaluator",
            "name": EVALUATOR_BADGES["active_evaluator"]["name"]
        })

    if reliability >= EVALUATOR_BADGES["consistent_evaluator"]["requirement"]:
        badges.append({
            "id": "consistent_evaluator",
            "name": EVALUATOR_BADGES["consistent_evaluator"]["name"]
        })

    master_req = EVALUATOR_BADGES["master_evaluator"]["requirement"]
    if (stats["total_evaluations"] >= master_req["count"] and
        reliability >= master_req["reliability"]):
        badges.append({
            "id": "master_evaluator",
            "name": EVALUATOR_BADGES["master_evaluator"]["name"]
        })

    return badges


def award_evaluator_badge(user_id: str) -> Dict[str, Any]:
    """í‰ê°€ì ë°°ì§€ ìˆ˜ì—¬

    Args:
        user_id: í‰ê°€ì ID

    Returns:
        ìˆ˜ì—¬ëœ ë°°ì§€ ì •ë³´
    """
    evaluations = _load_evaluations()

    # ì‘ì„±í•œ í‰ê°€ ìˆ˜
    given = [e for e in evaluations if e["evaluator_id"] == user_id]
    total_evaluations = len(given)
    feedback_count = sum(1 for e in given if e.get("feedback"))

    # ì‹ ë¢°ë„
    reliability = calculate_evaluator_reliability(user_id)["reliability"]

    stats = {
        "total_evaluations": total_evaluations,
        "feedback_count": feedback_count,
        "flagged_count": sum(1 for e in given if e.get("flagged"))
    }

    earned_badges = _get_evaluator_badges(user_id, stats, reliability)

    # ìƒˆë¡œ íšë“í•œ ë°°ì§€ í™•ì¸ (ì´ì „ ë°°ì§€ì™€ ë¹„êµ í•„ìš”, ì—¬ê¸°ì„œëŠ” ê°„ë‹¨íˆ ë°˜í™˜)
    return {
        "user_id": user_id,
        "earned_badges": earned_badges,
        "stats": {
            "total_evaluations": total_evaluations,
            "reliability": reliability,
            "feedback_rate": round(feedback_count / total_evaluations, 2) if total_evaluations > 0 else 0
        }
    }


# ========================================
# ë¹ ë¥¸ ë°˜ì‘ ì‹œìŠ¤í…œ
# ========================================

def add_reaction(
    user_id: str,
    target_id: str,
    room_id: str,
    reaction: str
) -> Dict[str, Any]:
    """ë¹ ë¥¸ ë°˜ì‘ ì¶”ê°€

    Args:
        user_id: ë°˜ì‘í•˜ëŠ” ì‚¬ìš©ì ID
        target_id: ëŒ€ìƒ ì‚¬ìš©ì ID
        room_id: ë°© ID
        reaction: ë°˜ì‘ ì¢…ë¥˜ (like, amazing, confident, creative, empathy)

    Returns:
        ì¶”ê°€ëœ ë°˜ì‘ ì •ë³´
    """
    if reaction not in REACTIONS:
        raise ValueError(f"ìœ íš¨í•˜ì§€ ì•Šì€ ë°˜ì‘ì…ë‹ˆë‹¤. ê°€ëŠ¥í•œ ë°˜ì‘: {list(REACTIONS.keys())}")

    if user_id == target_id:
        raise ValueError("ìê¸° ìì‹ ì—ê²ŒëŠ” ë°˜ì‘í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    reaction_data = {
        "reaction_id": str(uuid.uuid4())[:8],
        "user_id": user_id,
        "target_id": target_id,
        "room_id": room_id,
        "reaction": reaction,
        "emoji": REACTIONS[reaction]["emoji"],
        "name": REACTIONS[reaction]["name"],
        "created_at": datetime.now().isoformat()
    }

    reactions = _load_reactions()
    reactions.append(reaction_data)
    _save_reactions(reactions)

    logger.debug(f"ë°˜ì‘ ì¶”ê°€: {user_id} -> {target_id} ({reaction})")

    return reaction_data


def get_reactions_received(
    user_id: str,
    room_id: Optional[str] = None
) -> List[Dict[str, Any]]:
    """ë°›ì€ ë°˜ì‘ ì¡°íšŒ

    Args:
        user_id: ì‚¬ìš©ì ID
        room_id: ë°© ID (ì„ íƒ, íŠ¹ì • ë°©ë§Œ ì¡°íšŒ)

    Returns:
        ë°›ì€ ë°˜ì‘ ëª©ë¡
    """
    reactions = _load_reactions()

    received = [r for r in reactions if r["target_id"] == user_id]

    if room_id:
        received = [r for r in received if r["room_id"] == room_id]

    # ìµœì‹ ìˆœ ì •ë ¬
    received.sort(key=lambda x: x["created_at"], reverse=True)

    return received


def get_reaction_summary(user_id: str) -> Dict[str, Any]:
    """ë°˜ì‘ ìš”ì•½

    Args:
        user_id: ì‚¬ìš©ì ID

    Returns:
        ë°˜ì‘ ìš”ì•½ ì •ë³´
    """
    reactions = get_reactions_received(user_id)

    if not reactions:
        return {
            "user_id": user_id,
            "total_reactions": 0,
            "message": "ì•„ì§ ë°›ì€ ë°˜ì‘ì´ ì—†ìŠµë‹ˆë‹¤."
        }

    # ë°˜ì‘ë³„ ì¹´ìš´íŠ¸
    reaction_counts = Counter(r["reaction"] for r in reactions)

    # ì •ë ¬ëœ ë°˜ì‘ ëª©ë¡
    sorted_reactions = []
    for reaction_key, count in reaction_counts.most_common():
        reaction_info = REACTIONS[reaction_key]
        sorted_reactions.append({
            "reaction": reaction_key,
            "emoji": reaction_info["emoji"],
            "name": reaction_info["name"],
            "count": count
        })

    # ê°€ì¥ ë§ì´ ë°›ì€ ë°˜ì‘
    most_common = sorted_reactions[0] if sorted_reactions else None

    return {
        "user_id": user_id,
        "total_reactions": len(reactions),
        "reaction_breakdown": sorted_reactions,
        "most_received": most_common,
        "unique_reactors": len(set(r["user_id"] for r in reactions))
    }


# ========================================
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# ========================================

def get_evaluation_criteria() -> Dict[str, Any]:
    """í‰ê°€ ê¸°ì¤€ ì •ë³´ ë°˜í™˜"""
    return EVALUATION_CRITERIA


def get_available_reactions() -> Dict[str, Any]:
    """ì‚¬ìš© ê°€ëŠ¥í•œ ë°˜ì‘ ëª©ë¡ ë°˜í™˜"""
    return REACTIONS


def get_score_weights() -> Dict[str, float]:
    """ì ìˆ˜ ê°€ì¤‘ì¹˜ ë°˜í™˜"""
    return SCORE_WEIGHTS
